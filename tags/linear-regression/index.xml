<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear regression on 爱豆吧！</title>
    <link>https://idouba.com/tags/linear-regression/</link>
    <description>Recent content in Linear regression on 爱豆吧！</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>浙ICP备18050493号-1 浙公网安备 33010802006262号</copyright>
    <lastBuildDate>Wed, 02 Oct 2024 15:32:08 +0000</lastBuildDate><atom:link href="https://idouba.com/tags/linear-regression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>云原生工程师初识深度学习系列（一）：从线性回归入门神经网络</title>
      <link>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</link>
      <pubDate>Wed, 02 Oct 2024 15:32:08 +0000</pubDate>
      
      <guid>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</guid>
      <description>
        
          
            背景 最近团队的业务除了面向通用计算外，越来越多的要处理面向AI场景的软硬件资源的供给、分发、调度等。虽然还是在熟悉的云原生领域，折腾的还是哪些对象哪些事儿，适配到一种新场景。但为了避免新瓶装老酒，能有机会做的更扎实，做出价值，对这个要服务领域内的一些东西也想花点时间和精力稍微了解下。
国庆长假环太湖一圈回来，假期最后这两天豆哥被要求上课，正好难得集中时间可以稍微看些东西。暂时没有精力系统地构建，先入个门。作为一个云原生领域的从业者，目标是知道容器里跑的是什么，怎么跑的。
学新东西时习惯用自己的文字，尽可能简单易懂地总结记录贯通下，做不到严谨、全面、深入、专业。开始前定个小目标只要做到基本的通、透、够用即可。
说干就干，先从深度学习基础技术神经网络开始。Google一把，内容可真叫个多。确实最近身边不管曾经做什么的，摇身一变都能与AI扯上关系。这么多信息对我们这些局外人非常不友好。很多年前自学数学等相关基础课程时，习惯从稍微了解点的东西入手，有点脸熟的东西看着不怵。重拾十来年前尚老师Data Mining那门课程的部分内容，看看老的概念和新的技术能产生哪些联系。
切入点线性回归 线性回归可能是一个比较适当的切入点，模型简单好理解。线性回归时通过一组数据点来拟合线性模型，找出一个或者多个特征变量和目标结果之间的关系，有了这个关系就可以带入条件预测结果。一个非常经典的线性回归例子就是二手房价预测。
影响房价的因素很多，记得当时书上形式化表达是用了一个向量乘法y = wx + b。x向量由（x1,x2,x3,x4）组成，表示若干个属性。这里简单示意下假设只有两个因素x1、x2，分别表示屋子的房间数和面积，也不用向量乘了，就直接写成 y = w1 * x1 + w2 * x2 + b，y就房子价格。其中w1、w2和b称为线性回归模型的参数，w1、w2称为权重weight，b称为偏差bias。
可以看到，作为一种最简单的回归模型，线性回归使用这种线性回归方程对一个或者多个自变量和因变量之间的关系进行建模。有了这个假设的模型，就可以根据已有的二手房成交记录求解出模型上的参数w1、w2和b，这就是老听说的模型训练。完成模型训练求解出线性回归模型的参数，就可以把房间数、面积x1、x2带入表达式，得到房子的预测价格，这就是一个推理过程。
这个一个简单例子把模型的的表达、训练和推理过程最简单地顺一遍。省略了太多的信息和步骤，迭代着加上去应该就是关注的神经网络的关键内容。
首先是模型的表达，通过最基础的数学知识，这个线性回归的输入、输出和运算过程可以大致画成这样。
即使完全不了解神经网络，基于最朴素的概念理解，瞅着这个图上这些点的关系好像也已经和神经网络有点神似了。
神经网络的概念 神经网络这个典型术语标准定义很多，总结下简单理解神经网络就是一种模拟人脑处理信息的方式。从数据中获取关联，在输入和输出中建立关系，特别是复杂的输入和输出之间。类似我们人脑中神经元构成一个复杂、高度互联的网络，互相发送电信号处理信息。神经网络由人工神经元组成，在这些神经元上运行算法，求解各种复杂的模型，所以我们说的神经网络完整点描述其实是人工神经网络。
只是从外形简单比较，前面线性回归那个图看上去像一种单层或者单个神经元组成的神经网络，大致可以认为是神经网络的一个简单特例。
神经网络的结构 经典的神经网络包括输入层、输出层和隐藏层。
**输入层：**接受外部输入的数据，将数据输入给神经网络，简单处理后发给下一层。在预测房价这个线性模型中，输入层就是影响房价的两个属性。在另外一个典型应用图片分类中输入层就是像素，如100*100像素的图片就又10000个输入。 **隐藏层：**神经网络中大量的隐藏层从上一层，如输入层或者上一个隐藏层获取输入，进行数据处理，然后传递给下一层。神经网络的关键处理都集中在隐藏层，越复杂的模型、表达能力越强的模型，隐藏层的层数越多，隐藏层上的节点也越多。 **输出层：**输出神经网络对数据的最终处理结果。因为模型固定，输出层的节点数一般也是固定的，如上一个房价预测线性回归的图中，就只有一个输出。如果是分类的模型，一般有几个分类，就对应输出层有几个节点。 不考虑内部复杂实现，只看这个外部结构，从我们程序员的语言看，一个神经网络就像我们编程的一个方法。输入层对应这个方法定义的入参，输出层对应方法定义的返回值，隐藏层可以简单类比我们的方法实现逻辑。模型训练好后，去做推理时就是传入参数调用这个方法，得到返回值的过程。
从数学的视角可能更准确一些，一个神经网络对应一个映射函数，输入层对应函数的自变量x1、x2，输出层对应函数的因变量y。训练过程就是找到函数表达式中的各个参数。有了这个求解的函数表达式，其他任意参数x1、x2带进去也能得到相应基本正确的y。
作为映射函数，只要有一组输入就能映射到一组输出。除了这个根据房子大小、房间数映射出房价外，其他更强大的神经网络可以拟合更复杂的函数。对于大多数深度学习的应用，虽然我们没有办法像这个预测房价的例子这么直观地写出一个具体表达式，但还是可以理解存在这样一个函数映射，或者说通过训练有办法逼近一个理想的函数映射。
记得从哪里看到黄教主说过“AI深度学习，也是一种解决难以指定的问题的算法和一种开发软件的新方法。想象我们有一个任意维度的通用函数逼近器”。如果设计的神经网络足够深、参数足够多，足够复杂就可以逼近任意复杂的函数映射。不只是预测房价这个简单的线性回归，也不只是当年学习的Han Jiawei的Data Mining课本上的分类、聚类、啤酒尿布频繁项这些业务固定的应用。
为了便于理解把以上简单类比总结成下表。
神经网络 程序视角 数学视角 模型 代码方法 映射函数 输入层 入参定义 自变量 隐藏层 方法体实现 函数表达式 输出层 返回值定义 因变量 训练 构造实现并UT验证修正 求解函数参数 推理 实际方法调用 带入新的自变量求解因变量 样板特征 Feature UT测试用例输入 已知的符合表达式的自变量取值 样板标签 Label UT用例预期输出 已知的符合表达式的因变量取值 以上两个临时起意的类别，前一个更像神经网络的物理存在。不管多复杂的神经网络，最终都是一个方法调用。实际应用中通过Restful接口或者其他应用协议调到推理服务上，获得一个输出，返回给调用方使用。而数学的这个类比更像神经网络的逻辑定义，模型本身的定义和模型训练、推理过程。
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
