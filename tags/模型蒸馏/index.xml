<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>模型蒸馏 on 爱豆吧！</title>
    <link>https://idouba.com/tags/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/</link>
    <description>Recent content in 模型蒸馏 on 爱豆吧！</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>浙ICP备18050493号-1 浙公网安备 33010802006262号</copyright>
    <lastBuildDate>Fri, 07 Feb 2025 15:32:08 +0000</lastBuildDate><atom:link href="https://idouba.com/tags/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>云原生工程师入坑AI深度学习系列（二）：给昌子解释DeepSeek的模型蒸馏</title>
      <link>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-make-changzi-understand-model-distillation-of-deepseek/</link>
      <pubDate>Fri, 07 Feb 2025 15:32:08 +0000</pubDate>
      
      <guid>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-make-changzi-understand-model-distillation-of-deepseek/</guid>
      <description>
        
          
            背景 一篇技术入门文章，尝试向非领域技术人员介绍DeepSeek的模型蒸馏。先求易懂，再求准确。起因是昨天，正月初七，昌子突然微信问我“DeepSeek咋用”。回复“你不好好焊钢轨，打听这玩意儿干啥”。小哥说我就想闹明白我们中国这个技术是不是像有些短视频里说的抄了老美的。不用问，这小哥是过年在家闲的刷到OpenAI可能起诉DeepSeek的新闻了。虽然给他说Altman在咱们大年初六好已经承认不打算起诉了，但是说服昌子小哥还是费了不少口舌。为了说明没有抄，模型蒸馏的话题就绕不过。在沟通中临时起意提到小哥咱们很多年前的一段趣事，居然说明白了。觉得很有意思，就顺手归档下。
模型蒸馏概念初体验 首先，DeepSeek关于模型蒸馏的定义是这样的：
模型蒸馏通过将复杂模型（教师模型）的知识转移到简单模型（学生模型）中，实现模型压缩、加速推理并保持性能，适用于多种资源受限的场景。
是不是不太好理解，用咱们共同的一段经历做个不算太恰当的类比，可能更好理解了。还记得咱们上中专那会儿，正是十五六岁贪玩的年龄。那学期好像是安排去永济电机厂还是洛阳重机厂实习，我们疯玩了几个月，你总带哥去厂子的花园里捞鱼。临期末有一门考试好像是《公差与配合》，你和老七担心考不过，总磨着我给你俩补课。
其实只要自己老老实实地把课本过一遍，课后题做几遍，保证都妥妥考得过，但你俩还是选择了个山寨老师补课。对应到模型训练过程，你俩作为学生模型没有选择从头看书、自己做题这种完全从头开始训练的方式，而是模仿学习一种你们信任的教师模型来获取知识。
但是你俩这个过程还是学习的过程，并没有直接抄哥这个山寨老师的。你们得自己学过去，然后期末考试时候才能考出来。正好对应模型训练和推理的过程。我们详细捋下咱们当时怎么干的。
一般你和老七会拿一道题目过来，让我给出我的答案。当然每道题，你们都是自己做过的。没记错的话，你们开始那些答案都是凭着感觉蒙的。然后我们会讨论题目，如最小最大实体尺寸对应于孔和轴分别是他们的最大极限尺寸还是最小极限尺寸。你们相信哥的答案一般是对的答案，在讨论过程中调整、形成自己的思考方式。看上去是哥在纠正你们，实际是你们在自己思考，接近这个山寨老师对这段知识点的处理。
反复经历这个过程，然后你俩就越来越自信。最后，你们就出师了。可以独立判断，得出答案。尤其是老七，练习的可溜了。以至于后面他再磨我的时候，我非常坚定地说：“你都学到这个程度了，如果再考不过，我请你哥俩吃饭”。结果，结果你还记得吗？这么多年来每次见面喝酒你光记得有次学校附近那个叫双虎的小饭馆我们花生米、土豆丝、老白干搞得可爽了，背景还记得不？让哥这段讲完给你串起来，先不岔开，哥再尝试给你讲点机械专业以外的新鲜东西。
在这个过程里，对应模型蒸馏的术语。哥这个山寨老师的叫教师模型，你俩那叫学生模型。一般教师模型复杂，学生模型简单。对应到咱们的实践，哥当时可是老老实实地把课本学过一遍，虽然跟你们玩，但是老师上课讲的都认真地听了的。所以教师模型里不只是塞了那些题，还塞了更多。而你俩呢，课本基本上是不看的，在你们那里课本都是些不一定必要的信息，你们只想学习满足期末考试需要的知识。即模型蒸馏过程去掉了多余部分，让你们的学生模型更简洁、更高效地提供能力。
对应的一般教师模型参数多，学生模型参数少。记得开始拿到一个题目时，哥给你们讲的时候啰啰嗦嗦说一堆课本里的背景原理，然后推出结果，而后来我们练的根据简单的题目特征，就能找到对的答案。当时你经常说的一句话是“哥们记不清，简单点，好理解”。差不多对应到模型蒸馏，就是简化无用参数。但这个过程中，提高效率的同时，模型能力并没有下降很多。
通过这种有针对性的练习，学习老师模型总结过的知识，因此也更高效。你们用更简单的模型，通过更低的成本，但更高的效率达到了很高的模型质量。到考试前那阵子，特别是你，不止比我墨迹白天推导要快，有些时候也更准。
那段时间，咱们仨人一直鬼混一起，也就是一直在一起学。你和老七顶多就是过会儿拿书过来问我这个题我的答案是啥，为啥这样想的，然后你的答案是啥，为啥不一样。从外面行为看，我们整个状态都是学习的状态，认真学习的状态，你俩也就每学期这时候有个学习的状态。从内在看，你俩是自己脑子在思考，在各自脑子里构造各自的模型。对于同一个题目，准确说是一块知识点，我这个山寨老师咋想的，你们咋想的，能得到尽量一致的答案。我没有把我的模型给你们抄，也没有途径啊，抄不着。顶多从这过程获取了一些重要知识，帮助你在考试时能同样方式去思考，选到一个对的答案。
我讲的都一样，你俩各自独立地在思考学习，各自在自己脑子里形成对《公差与配合》独立的思考。你和老七学到的也不一样，最终期末考试时在实际的测试数据集上验证，结果大不相同。结果留个彩蛋在后面。同样的教师模型，同样的训练过程，两个不同的学生模型，学到的效果不同。这也反证了，如果是抄，你俩应该抄的是一样的才对。同样的模型蒸馏，也是学，不是抄。只是学到了行为和能力，抄不到内部结构和实现。
模型蒸馏流程 上面通过简单类比，我们理解个概念：模型蒸馏是什么。下面简单了解下流程：模型蒸馏怎么做的。
DeepSeek输出的模型蒸馏流程 向DeepSeek提问“模型蒸馏原理”，得到如下答案。你只作为参照就好。内容可能太干，后面结合我们的具体案例东拉西扯下就大致理解了。
0. 流程图 1[输入数据 X] 2 │ 3 ├─────► [教师模型（大型复杂模型，如BERT/ResNet）] ────► [软标签（Soft Targets，带温度参数 T）] 4 │ │ 5 │ ▼ 6 │ [概率分布（Softmax with Temperature T）] 7 │ │ 8 │ ▼ 9 └─────► [学生模型（小型轻量模型，如MobileNet/TinyBERT）] ◄──┐ 10 │ │ 11 ▼ │ 12 [概率分布（Softmax with Temperature T）] │ 13 │ │ 14 ▼ │ 15 [损失函数：软目标损失（KL散度） + 硬目标损失（交叉熵）] │ 16 │ │ 17 ▼ │ 18 [参数更新（反向传播）] ──────────────┘ 1.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
