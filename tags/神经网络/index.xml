<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>神经网络 on 爱豆吧！</title>
    <link>https://idouba.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
    <description>Recent content in 神经网络 on 爱豆吧！</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>浙ICP备18050493号-1 浙公网安备 33010802006262号</copyright>
    <lastBuildDate>Wed, 02 Oct 2024 15:32:08 +0000</lastBuildDate><atom:link href="https://idouba.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>云原生工程师初识深度学习系列（一）：从线性回归入门神经网络</title>
      <link>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</link>
      <pubDate>Wed, 02 Oct 2024 15:32:08 +0000</pubDate>
      
      <guid>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</guid>
      <description>
        
          
            背景 最近团队的业务除了面向通用计算外，越来越多的需求要处理面向AI场景的软硬件资源的供给、分发、调度等。虽然还是在熟悉的云原生领域，折腾的还是哪些对象哪些事儿，适配到一种新场景。但为了避免新瓶装老酒，能有机会做的更扎实，做出价值，对这个要服务领域内的一些东西也想花点时间和精力稍微了解下。
国庆长假环太湖一圈回来，假期最后这两天豆哥被要求上课，正好有难得的集中时间可以稍微看些东西。能对这部分有个比较完整的入个门，暂时没有精力系统地构建。作为一个云原生领域的从业者，目标是知道容器里跑的是什么，怎么跑的。
习惯稍微正式学点东西不只是把别人的东西快速过一遍，而是愿意用自己的文字，尽可能简单易懂地总结记录贯通下，做不到严谨、全面、深入、专业，开始前定个小目标只要做到基本的通、透、够用即可。
说干就干，先从深度学习基础技术神经网络开始。Google一把，内容可真叫个多。确实现在身边不管曾经做什么的，摇身一变都能与AI扯上关系。这么多信息对我们这些局外人非常不友好。很多年前自学的数学等相关基础课程时，习惯从稍微了解点的东西入手，有点脸熟的东西看着不怵。于是决定重拾十来年前尚老师Data Mining那门课程的部分内容，看看哪些老概念和当前这些新的技术能产生哪些联系。
切入点线性回归 线性回归可能是一个比较适当的切入点，模型简单好理解。线性回归时通过一组数据点来拟合线性模型，找出一个或者多个特征变量和目标结果之间的关系，有了这个关系就可以带入条件预测结果。一个非常经典的线性回归例子就是二手房价预测。
影响房价的因素很多，记得当时书上形式化表达是用了一个向量乘法y = wx + b。x向量是（x1,x2,x3,x4）组成，表示若干个属性。这里简单示意下假设只有两个因素x1、x2，分别表示屋子的房间数和面积，也不用向量乘了，就直接写成 y = w1 * x1 + w2 * x2 + b，y就房子价格。其中w1、w2和b称为线性回归模型的参数，w1、w2称为权重weight，b称为偏差bias。
可以看到，作为一种最简单的回归模型，线性回归使用这种线性回归方程对一个或者多个自变量和因变量之间的关系进行建模。有了这个假设的模型，就可以根据已有的二手房成交记录求解出模型上的参数w1、w2和b，这就是老听说的模型训练。完成模型训练求解出线性回归模型的参数，就可以把其他的房子信息房间数、面积x1、x2带入表达式，得到这个房子的预测价格，这就是一个推理过程。
这样通过一个简单例子把模型的的表达、训练和推理过程顺了一遍。其中省略了太多的信息和步骤，迭代着加上去应该就是关注的神经网络的关键内容。
首先是模型的表达，通过最基础的数学知识，这个线性回归的输入、输出和运算过程可以大致画出这样。
即使完全不了解神经网络，基于最朴素的概念理解，瞅着这个图上这些点的关系好像也已经和神经网络有点神似了。
认识神经网络 神经网络这个典型术语对象标准定义很多，总结下简单理解神经网络就是一种模拟人脑处理信息的方式。从数据中获取关联，在输入和输出中建立关系，特别是复杂的输入和输出之间。类似我们人脑中神经元构成一个复杂、高度互联的网络，互相发送电信号处理信息。神经网络由人工神经元组成，在这些神经元上运行算法，求解各种复杂的模型，所以我们说的神经网络说完整点描述其实是人工神经网络。
只是从外形简单比较，前面线性回归那个图看上去像一种单层或者单个神经元组成的神经网络，大致可以认为是神经网络的简单特例。
神经网络结构 经典的神经网络包括输入层、输出层和隐藏层。
**输入层：**接受外部输入的数据，将数据输入给神经网络，简单处理后发给下一层。在预测房价这个线性模型中，输入层就是影响房价的两个属性。在经常说的图片识别分类的应用中输入层就是像素，如100*100像素的图片就又10000个输入。 **隐藏层：**神经网络中大量的隐藏层从上一层，如输入层或者上一个隐藏层获取输入，进行数据处理，然后传递给下一层。神经网络的关键处理都集中在隐藏层，越复杂的模型、表达能力越强的模型，隐藏层层数越多，隐藏层上的节点也越多。 **输出层：**输出神经网络对数据的最终处理结果。因为模型固定，输出层的节点数一般也是固定的，如上一个图表示，房价预测这个线性回归，作为简单特例的神经网络只有一个输出。如果是分类的模型，有几个分类，就对应输出层有几个节点。 从我们程序员的语言看，一个神经网络可以简单看成我们编程的一个方法。输入层对应这个方法定义的入参，输出层对应方法定义的返回值，隐藏层可以见到类别我们的方法实现逻辑。模型训练好后，去做推理时就是传入入参调用这个方法，得到返回值的过程。
从数学的视角看，一个神经网络就好像一个映射函数，函数的自变量x1、x2对应输入层，输出层对应函数的因变量y。训练过程就是找到函数表达式中的各个参数。有了这个求解的函数表达式，其他任意的x1、x2带进去也能得到对应基本正确的y。
作为映射函数，只要有一组输入就能映射到一组输出。除了前面根据房子大小、房间数映射出房价外，其他更强大的神经网络可以拟合更复杂的函数。对于大多数深度学习的应用，虽然我们没有办法向这个预测房价的例子这么直观地写出一个具体表达式来，但还是理解存在这样一个函数映射，或者说通过训练有办法逼近一个理想的函数映射。
记得从哪里看到黄教主说过“AI深度学习，也是一种解决难以指定的问题的算法和一种开发软件的新方法。想象我们有一个任意维度的通用函数逼近器”。如果设计的神经网络足够深、参数足够多，足够复杂就可以逼近任意复杂的函数映射。不只是预测房价这个简单的线性回归，也不只是当年机器学习课本上的分类、聚类、啤酒尿布频繁项这些业务固定的应用。
为了便于理解把这种不太严谨的类比总结成这个表。
神经网络 程序视角 数学视角 模型 代码方法 映射函数 输入层 入参定义 自变量 隐藏层 方法体实现 函数表达式 输出层 返回值定义 因变量 训练 构造实现并UT验证修正 求解函数参数 推理 实际方法调用 带入新的自变量求解因变量 样板特征 Feature UT测试输入 已知的函数参数 样板标签 Label UT预期输出 对应的函数取值 以上两个临时起意的比喻，前一个更像神经网络的物理存在，不管多复杂的神经网络，最终都是一个方法调用。Restful或者其他协议调到推理服务上，获得一个输出。而数学的这个类比更像神经网络的逻辑定义，模型的定义和训练、推理过程等。
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
