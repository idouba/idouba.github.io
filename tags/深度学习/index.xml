<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on 爱豆吧！</title>
    <link>https://idouba.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on 爱豆吧！</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>浙ICP备18050493号-1 浙公网安备 33010802006262号</copyright>
    <lastBuildDate>Fri, 07 Feb 2025 15:32:08 +0000</lastBuildDate><atom:link href="https://idouba.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>云原生工程师入坑AI深度学习系列（二）：给昌子解释DeepSeek的模型蒸馏</title>
      <link>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-make-changzi-understand-model-distillation-of-deepseek/</link>
      <pubDate>Fri, 07 Feb 2025 15:32:08 +0000</pubDate>
      
      <guid>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-make-changzi-understand-model-distillation-of-deepseek/</guid>
      <description>
        
          
            背景 一篇技术入门文章，尝试向非领域技术人员介绍DeepSeek的模型蒸馏。先求易懂，再求准确。起因是昨天，正月初七，昌子突然微信问我“DeepSeek咋用”。回复“你不好好焊钢轨，打听这玩意儿干啥”。小哥说我就想闹明白我们中国这个技术是不是像有些短视频里说的抄了老美的。不用问，这小哥是过年在家闲的刷到OpenAI可能起诉DeepSeek的新闻了。虽然给他说Altman在咱们大年初六好已经承认不打算起诉了，但是说服昌子小哥还是费了不少口舌。为了说明没有抄，模型蒸馏的话题就绕不过。在沟通中临时起意提到小哥咱们很多年前的一段趣事，居然说明白了。觉得很有意思，就顺手归档下。
模型蒸馏概念初体验 首先，DeepSeek关于模型蒸馏的定义是这样的：
模型蒸馏通过将复杂模型（教师模型）的知识转移到简单模型（学生模型）中，实现模型压缩、加速推理并保持性能，适用于多种资源受限的场景。
是不是不太好理解，用咱们共同的一段经历做个不算太恰当的类比，可能更好理解了。还记得咱们上中专那会儿，正是十五六岁贪玩的年龄。那学期好像是安排去永济电机厂还是洛阳重机厂实习，我们疯玩了几个月，你总带哥去厂子的花园里捞鱼。临期末有一门考试好像是《公差与配合》，你和老七担心考不过，总磨着我给你俩补课。
其实只要自己老老实实地把课本过一遍，课后题做几遍，保证都妥妥考得过，但你俩还是选择了个山寨老师补课。对应到模型训练过程，你俩作为学生模型没有选择从头看书、自己做题这种完全从头开始训练的方式，而是模仿学习一种你们信任的教师模型来获取知识。
但是你俩这个过程还是学习的过程，并没有直接抄哥这个山寨老师的。你们得自己学过去，然后期末考试时候才能考出来。正好对应模型训练和推理的过程。我们详细捋下咱们当时怎么干的。
一般你和老七会拿一道题目过来，让我给出我的答案。当然每道题，你们都是自己做过的。没记错的话，你们开始那些答案都是凭着感觉蒙的。然后我们会讨论题目，如最小最大实体尺寸对应于孔和轴分别是他们的最大极限尺寸还是最小极限尺寸。你们相信哥的答案一般是对的答案，在讨论过程中调整、形成自己的思考方式。看上去是哥在纠正你们，实际是你们在自己思考，接近这个山寨老师对这段知识点的处理。
反复经历这个过程，然后你俩就越来越自信。最后，你们就出师了。可以独立判断，得出答案。尤其是老七，练习的可溜了。以至于后面他再磨我的时候，我非常坚定地说：“你都学到这个程度了，如果再考不过，我请你哥俩吃饭”。结果，结果你还记得吗？这么多年来每次见面喝酒你光记得有次学校附近那个叫双虎的小饭馆我们花生米、土豆丝、老白干搞得可爽了，背景还记得不？让哥这段讲完给你串起来，先不岔开，哥再尝试给你讲点机械专业以外的新鲜东西。
在这个过程里，对应模型蒸馏的术语。哥这个山寨老师的叫教师模型，你俩那叫学生模型。一般教师模型复杂，学生模型简单。对应到咱们的实践，哥当时可是老老实实地把课本学过一遍，虽然跟你们玩，但是老师上课讲的都认真地听了的。所以教师模型里不只是塞了那些题，还塞了更多。而你俩呢，课本基本上是不看的，在你们那里课本都是些不一定必要的信息，你们只想学习满足期末考试需要的知识。即模型蒸馏过程去掉了多余部分，让你们的学生模型更简洁、更高效地提供能力。
对应的一般教师模型参数多，学生模型参数少。记得开始拿到一个题目时，哥给你们讲的时候啰啰嗦嗦说一堆课本里的背景原理，然后推出结果，而后来我们练的根据简单的题目特征，就能找到对的答案。当时你经常说的一句话是“哥们记不清，简单点，好理解”。差不多对应到模型蒸馏，就是简化无用参数。但这个过程中，提高效率的同时，模型能力并没有下降很多。
通过这种有针对性的练习，学习老师模型总结过的知识，因此也更高效。你们用更简单的模型，通过更低的成本，但更高的效率达到了很高的模型质量。到考试前那阵子，特别是你，不止比我墨迹白天推导要快，有些时候也更准。
那段时间，咱们仨人一直鬼混一起，也就是一直在一起学。你和老七顶多就是过会儿拿书过来问我这个题我的答案是啥，为啥这样想的，然后你的答案是啥，为啥不一样。从外面行为看，我们整个状态都是学习的状态，认真学习的状态，你俩也就每学期这时候有个学习的状态。从内在看，你俩是自己脑子在思考，在各自脑子里构造各自的模型。对于同一个题目，准确说是一块知识点，我这个山寨老师咋想的，你们咋想的，能得到尽量一致的答案。我没有把我的模型给你们抄，也没有途径啊，抄不着。顶多从这过程获取了一些重要知识，帮助你在考试时能同样方式去思考，选到一个对的答案。
我讲的都一样，你俩各自独立地在思考学习，各自在自己脑子里形成对《公差与配合》独立的思考。你和老七学到的也不一样，最终期末考试时在实际的测试数据集上验证，结果大不相同。结果留个彩蛋在后面。同样的教师模型，同样的训练过程，两个不同的学生模型，学到的效果不同。这也反证了，如果是抄，你俩应该抄的是一样的才对。同样的模型蒸馏，也是学，不是抄。只是学到了行为和能力，抄不到内部结构和实现。
模型蒸馏流程 上面通过简单类比，我们理解个概念：模型蒸馏是什么。下面简单了解下流程：模型蒸馏怎么做的。
DeepSeek输出的模型蒸馏流程 向DeepSeek提问“模型蒸馏原理”，得到如下答案。你只作为参照就好。内容可能太干，后面结合我们的具体案例东拉西扯下就大致理解了。
0. 流程图 1[输入数据 X] 2 │ 3 ├─────► [教师模型（大型复杂模型，如BERT/ResNet）] ────► [软标签（Soft Targets，带温度参数 T）] 4 │ │ 5 │ ▼ 6 │ [概率分布（Softmax with Temperature T）] 7 │ │ 8 │ ▼ 9 └─────► [学生模型（小型轻量模型，如MobileNet/TinyBERT）] ◄──┐ 10 │ │ 11 ▼ │ 12 [概率分布（Softmax with Temperature T）] │ 13 │ │ 14 ▼ │ 15 [损失函数：软目标损失（KL散度） + 硬目标损失（交叉熵）] │ 16 │ │ 17 ▼ │ 18 [参数更新（反向传播）] ──────────────┘ 1.
          
          
        
      </description>
    </item>
    
    <item>
      <title>云原生工程师入坑AI深度学习系列（一）：从线性回归入门神经网络</title>
      <link>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</link>
      <pubDate>Sun, 06 Oct 2024 15:32:08 +0000</pubDate>
      
      <guid>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</guid>
      <description>
        
          
            背景 最近团队的业务除了面向通用计算外，越来越多的要处理面向AI场景的软硬件资源的供给、分发、调度等。虽然还是在熟悉的云原生领域，折腾的还是哪些对象哪些事儿，适配到一种新场景。但为了避免新瓶装老酒，能有机会做的更扎实，做出价值，对这个要服务领域内的一些东西也想花点时间和精力稍微了解下。
国庆长假环太湖一圈回来，假期最后这两天豆哥被要求上课，正好难得集中时间可以稍微看些东西。暂时没有精力系统地构建，先入个门。作为一个云原生领域的从业者，目标是知道容器里跑的是什么，怎么跑的。
学新东西时习惯用自己的文字，尽可能简单易懂地总结记录贯通下，做不到严谨、全面、深入、专业。开始前定个小目标只要做到基本的通、透、够用即可。
说干就干，先从深度学习基础技术神经网络开始。Google一把，内容可真叫个多。确实最近身边不管曾经做什么的，摇身一变都能与AI扯上关系。这么多信息对我们这些局外人非常不友好。很多年前自学数学等相关基础课程时，习惯从稍微了解点的东西入手，有点脸熟的东西看着不怵。重拾十来年前尚老师Data Mining那门课程的部分内容，看看老的概念和新的技术能产生哪些联系。
切入点线性回归 线性回归可能是一个比较适当的切入点，模型简单好理解。线性回归时通过一组数据点来拟合线性模型，找出一个或者多个特征变量和目标结果之间的关系，有了这个关系就可以带入条件预测结果。一个非常经典的线性回归例子就是二手房价预测。
影响房价的因素很多，记得当时书上形式化表达是用了一个向量乘法y = wx + b。x向量由（x1,x2,x3,x4）组成，表示若干个属性。这里简单示意下假设只有两个因素x1、x2，分别表示屋子的房间数和面积，也不用向量乘了，就直接写成 y = w1 * x1 + w2 * x2 + b，y就房子价格。其中w1、w2和b称为线性回归模型的参数，w1、w2称为权重weight，b称为偏差bias。
可以看到，作为一种最简单的回归模型，线性回归使用这种线性回归方程对一个或者多个自变量和因变量之间的关系进行建模。有了这个假设的模型，就可以根据已有的二手房成交记录求解出模型上的参数w1、w2和b，这就是老听说的模型训练。完成模型训练求解出线性回归模型的参数，就可以把房间数、面积x1、x2带入表达式，得到房子的预测价格，这就是一个推理过程。
这个一个简单例子把模型的的表达、训练和推理过程最简单地顺一遍。省略了太多的信息和步骤，迭代着加上去应该就是关注的神经网络的关键内容。
首先是模型的表达，通过最基础的数学知识，这个线性回归的输入、输出和运算过程可以大致画成这样。
即使完全不了解神经网络，基于最朴素的概念理解，瞅着这个图上这些点的关系好像也已经和神经网络有点神似了。
神经网络的概念 神经网络这个典型术语标准定义很多，总结下简单理解神经网络就是一种模拟人脑处理信息的方式。从数据中获取关联，在输入和输出中建立关系，特别是复杂的输入和输出之间。类似我们人脑中神经元构成一个复杂、高度互联的网络，互相发送电信号处理信息。神经网络由人工神经元组成，在这些神经元上运行算法，求解各种复杂的模型，所以我们说的神经网络完整点描述其实是人工神经网络。
只是从外形简单比较，前面线性回归那个图看上去像一种单层或者单个神经元组成的神经网络，大致可以认为是神经网络的一个简单特例。
神经网络的结构 经典的神经网络包括输入层、输出层和隐藏层。
**输入层：**接受外部输入的数据，将数据输入给神经网络，简单处理后发给下一层。在预测房价这个线性模型中，输入层就是影响房价的两个属性。在另外一个典型应用图片分类中输入层就是像素，如100*100像素的图片就又10000个输入。 **隐藏层：**神经网络中大量的隐藏层从上一层，如输入层或者上一个隐藏层获取输入，进行数据处理，然后传递给下一层。神经网络的关键处理都集中在隐藏层，越复杂的模型、表达能力越强的模型，隐藏层的层数越多，隐藏层上的节点也越多。 **输出层：**输出神经网络对数据的最终处理结果。因为模型固定，输出层的节点数一般也是固定的，如上一个房价预测线性回归的图中，就只有一个输出。如果是分类的模型，一般有几个分类，就对应输出层有几个节点。 不考虑内部复杂实现，只看这个外部结构，从我们程序员的语言看，一个神经网络就像我们编程的一个方法。输入层对应这个方法定义的入参，输出层对应方法定义的返回值，隐藏层可以简单类比我们的方法实现逻辑。模型训练好后，去做推理时就是传入参数调用这个方法，得到返回值的过程。
从数学的视角可能更准确一些，一个神经网络对应一个映射函数，输入层对应函数的自变量x1、x2，输出层对应函数的因变量y。训练过程就是找到函数表达式中的各个参数。有了这个求解的函数表达式，其他任意参数x1、x2带进去也能得到相应基本正确的y。
作为映射函数，只要有一组输入就能映射到一组输出。除了这个根据房子大小、房间数映射出房价外，其他更强大的神经网络可以拟合更复杂的函数。对于大多数深度学习的应用，虽然我们没有办法像这个预测房价的例子这么直观地写出一个具体表达式，但还是可以理解存在这样一个函数映射，或者说通过训练有办法逼近一个理想的函数映射。
记得从哪里看到黄教主说过“AI深度学习，也是一种解决难以指定的问题的算法和一种开发软件的新方法。想象我们有一个任意维度的通用函数逼近器”。如果设计的神经网络足够深、参数足够多，足够复杂就可以逼近任意复杂的函数映射。不只是预测房价这个简单的线性回归，也不只是当年学习的Han Jiawei的Data Mining课本上的分类、聚类、啤酒尿布频繁项这些业务固定的应用。
为了便于理解把以上简单类比总结成下表。
神经网络 程序视角 数学视角 模型 代码方法 映射函数 输入层 入参定义 自变量 隐藏层 方法体实现 函数表达式 输出层 返回值定义 因变量 训练 构造实现并UT验证修正 求解函数参数 推理 实际方法调用 带入新的自变量求解因变量 样板特征 Feature UT测试用例输入 已知的符合表达式的自变量取值 样板标签 Label UT用例预期输出 已知的符合表达式的因变量取值 以上两个临时起意的类别，前一个更像神经网络的物理存在。不管多复杂的神经网络，最终都是一个方法调用。实际应用中通过Restful接口或者其他应用协议调到推理服务上，获得一个输出，返回给调用方使用。而数学的这个类比更像神经网络的逻辑定义，模型本身的定义和模型训练、推理过程。
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
