[{"body":"","link":"https://idouba.com/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://idouba.com/posts/","section":"posts","tags":null,"title":"Posts"},{"body":"","link":"https://idouba.com/tags/","section":"tags","tags":null,"title":"Tags"},{"body":" 今天是元旦，2025年的第一天。四点多到机场，赶回家的第一班飞机。\n可能不是这个机场2025年起飞的第一架飞机，却是昨晚十点接到母亲电话能选到最快的一条回家路。当时还在远程会议里，合上笔记本，从沙发上费了些劲才站起来。担心坐下又起不来，靠在墙上打开手机买票，手抖得操作不了。十点后当晚的高铁飞机都已没有班次，只能选到第二天最早六点的一班飞机。选日期时发现，居然已经是明年了。\n2024年最后一天的最后几个小时，父亲的一生定格在这个时刻。经过一年半与伤痛的斗争，父亲终于不用再那么辛苦了。\n已经进入腊月，我们还期望着今年能一起在家里团团圆圆地过个年。去年春节一家人是在医院过的。白天母亲和妹妹守护，晚上我和弟弟值班。大年三十，外面传来过年的鞭炮声，我们能听到的是病床上一直辛苦急促的咳痰声，每一声就有个儿子扑上来赶紧清理。病房里的夜似乎格外漫长，不知道几点是几点。只记得有一刻突然一阵密集的鞭炮声，病房窗户外的烟花把天都照亮了，一直顾不上看时间也知道是零点，过年了。\n去年过年好像就在昨天，转眼又到年底。从一年半前的那个傍晚意外发生，跟打仗一样经历了一个个紧张的日日夜夜。反复的被坏消息、更坏的消息、稍好的消息、说不清好坏的消息轰炸着，担心、恐慌、欣慰的情绪交织起伏，但不变的是希望。本来计划着回家休养几个月，过完年再约个医院申请手术。过去的几个月里因为身体状态不达标，我们被西安最好两个医院拒绝了三次，两次都住院进去术前检查被劝退了。\n坐在登机口的椅子上，有点犯困，早上起的早，昨晚没睡着。这是一个负一层的登机口，光线稍微暗一些，脑子里不自觉地回放起这一年半里一个个带画面的时刻。\n得到消息是出差深圳的那个上午，正在会议室里开会。走出来接到电话，再走进会议室瘫坐在一个椅子上。啥声音都听不到了，就看见刚才一起开会的同事嘴在动，看着最近写的材料像陌生的图片一样投射在屏幕上。从深圳飞回来赶到抢救的医院已经是凌晨，母亲在ICU门口的长椅上躺着睡着了，我就在紧挨着的另外个椅子上躺下了。从此半个月的日夜就守在这个门口。\n见到父亲是第二天，躺在急救室的转运床上，准备送到楼下放射科做CT检查。人完全昏迷中，闭着眼，纱布裹着头，身上插着数不清的导流、输液各种管。现在回想起来和当时一样身体里生理的疼，强迫跳过这个画面不去想。\n再闪现出的是高速上鸣着笛飞驰的救护车。在当地医院抢救保住了性命，基本稳定后转移到咸阳的医院。救护车里握着父亲的手，盯着心电检测仪。上车前随车医生宣读的转移风险吓到了母亲，我和弟弟虽坚持转院，但也很紧张。脑海中的画面中除了救护车里跳动的检测仪屏幕，印象深刻的特写镜头是父亲松塌皱巴的皮肤包裹着的的小腿，人已经瘦得不成样了。\n办好入院后和弟弟傍晚驱车回家取些做饭的东西，准备在医院附近长期安顿下来。这是父亲受伤后第一次回家，也是整个一年多救治过程里唯一一次回家。推开大门看到家的样子是长这么大见过最糟的样子。核桃树的落叶从后院被吹到了前院，厚厚的一层。后院门口石棉瓦大片小片散落一地，搅在满地的核桃叶里，还原着这那个下午发生的事情。厨房门口父母剥好的一大袋当季鲜核桃仁上飞的爬的虫子到处是，那是父母给我们准备的还没有完全弄完。厨房案板上一些菜已经烂的看不出样子，边上是父亲给自己买的还没来及下锅的面条。\n换了个医院，我们也换到了另外个ICU门口值守。每天医生叫家属沟通病情、治疗签字，或者去楼下买些抢救室需要的护理用品，每天都差不多。特别有印象的一个画面是一个晚上在ICU门口地板上躺着接了一个电话会议，从九点多开到十二点。对着耳机话筒一直压低声音说话特别累嗓子，只是担心吵到和激怒方圆十几平方米内地板上躺满的陪护家属。当时讲的啥不记得，但气氛和感受印象深刻。晚上和凌晨的医院，是很多人不知道也最好不要去经历的另外一个世界。\n经常半夜在睡觉地板的不远处，转运床的滚轮快速碾过，伴随着一阵急促的脚步声，振得地板都跟着紧张起来。跟着一起紧张的还有地板上躺着的这群人，就像动物世界里看到胆小的小动物群被外界惊扰到一样。那些来的久的人可能会被打扰少一些，说不定累了都不会被吵醒，毕竟之前的某个时刻也是这样把自己的亲人送进那个写着ICU的两扇门里面。一般跟着转运床跑的家属都会控制情绪压低声音说话，不打扰这一群地上睡着但不一定睡着的可怜人，因为一会儿他们就要加入进来。\n不同于这些带着恐惧但却瞄着希望送进去的家属，那些在晚上或凌晨被通知接出去的才是最绝望的时刻。同样急促的转运床滚轮声音和脚步声里，随时可能加入突然爆发的声嘶力竭的哭声，完全来不及照顾过去几天几十天里挨着睡在同一块地板上的战友们。这时躺在地板上的所有人都会被这强烈的代入感里包裹的恐惧与绝望所支配。对应动物世界里的小动物的画面，那是遭遇到了灭顶之灾的恐惧。哭的人昨天还是一个屋檐下一起交流病情拉家常的战友，听下来昨天他亲人的病情比自家的还要轻一点的。等天亮后，一堆人像往常一样围上抢救室里出来的大夫打听自家亲人的病情，也会三三两两地议论昨晚几几床的出去了。\n从这个世界出来的人，都会体会到，那些喊叫生活上的所谓各种忙碌、压力、失意、担忧、焦虑，都是无病呻吟。“无病呻吟”，写下这四个字第一次发现是个多么具象化的词。\n父亲在这里住的时间较长，弟弟经常帮别的家抬病人做检查等各种热心的公共事情，俨然已经是家属队伍的领导者。他跟我交流这个片区其他病人时都不用病床号，而是说那个换灯泡摔了的，那个打核桃摔了的，那个被快递车撞了的，那个是脑梗的，各有各的故事。\n在抢救室门口就是日复一日的等待。医生许可转移到普票病房就标志着抢救完成。哪家接到通知就跟自家孩子升学了一样，自己松口气，同时得到其他羡慕的家属的祝贺。父亲花了很长时间才升学成功，但后来的一个小状况又让他留级回到了抢救室。每次想到留级那次父亲刚愈合的伤口上不得不再割开，总下意识地去摸脖子，感觉割疼在自己脖子上一样。不愿意继续想这些，强迫想些稍微不这么沉重的的时刻。\n最欣慰的时刻是第八天例行CT检查的时候，父亲躺在那里，能慢慢睁开眼看四周。看着我微微地笑，虽然有些僵硬，但我确认他认出我了。似乎能回忆起自己闯了祸，有些委屈有些难过还有些害怕地看着儿子。附身趴在父亲耳边，我说“爸，不用害怕，我们都在你身边。您放心，我们一定治好您”。父亲的眼里含着泪，不能说话，后来一年半直到最后也没能说话。但我相信那次他听懂了我说的，父子进行了一次完整的沟通。\n最兴奋的时刻是接到弟弟的一个电话。父亲在抢救的较长一段时间尽管我们通过眼神的交流判定父亲有意识，但是医生基于病理原则并不认可。直到第二十天，接到弟弟的电话，早上医生谈话说，父亲可以按照指令数字二伸出二根指头。那一刻在电话两头，兄弟二人都差不多喜极而泣。真的救过来了，我们父亲有救了。只是我们当时都没有意识到后续的状况比我们预期的要复杂得多，严峻得多。\n在抢救室度过了四十多天，终于盼来了转移到普通病房。这个阶段最长、最辛苦，回忆的画面里这个阶段的镜头也最多。但当打算记录下来的时候不得不全部略去，不愿过多提及父亲病情的细节和病房里的细节，不愿提及父亲长时间遭遇的伤痛和不便。\n这个阶段母亲和弟弟白天晚上每时每刻守护着，各种贴身护理，超大强度的辛劳在超长时间线上，伴随着希望一天一天向前。我对这种强度的感知只来自几十分之一的参与。那时基本三四周回来一次，周末两天给母亲和弟弟分担一点。其实也就是白天给弟弟打下手，晚上替换弟弟值个班。每次在周日晚九点多回杭时，飞机还没有起飞就睡着了，降落滑行了才被邻座人叫醒。这只是母亲和弟弟零头的强度和劳动量，这种状态一直持续到后面父亲出院在家，再到去年过年前进入康复医院，又在康复医院的八个月，和出院回家的四个月。后来弟弟也要回单位工作了，基本就母亲一个人照顾。\n前年冬天出院住在西安是父亲身体和精神最好的时候。十一月回来那次，弟弟妹妹在客厅照顾父亲，我在厨房做饭。父亲被弟弟搀扶着走过厨房门口时，还给我竖了个大拇指。我给弟弟妹妹和外甥女做了顿土豆炖牛腩，同时给父亲做饭，高压锅炖烂的牛肉再用破壁机打成汁通过鼻饲的胃管注射到胃里。\n想起去年五一前出差西安，每天晚上躺在父亲的病房里很安心。那五天白天一天在单位会议室，晚上骑车六公里回到父亲康复的国医病房，睡在父亲身边的陪护床上。白天单位很忙很紧张，晚上在父亲病房也要起来挺多次处理痰和小便。但那个星期不累，父亲状况平稳，出差参与那个项目也很有成效。\n广播提示登机，思绪被打断，停下来不想了感觉有点累。从摆渡车往外看，天还是黑的。\n上了飞机居然迷瞪睡着了，直到被飞机广播叫醒，提醒准备降落。天已经亮了，能再次看见脚下的秦岭。这一年每次周六早班飞机回西安，都会特别多看几眼秦岭，从南往北回家的飞机飞十几分钟也飞不出这座中国最厚重的父亲山。我曾多次和现在一样看着窗外，看着脚下的秦岭，看着那望不到边的一道道山。想着那个最宽最长最雄伟的山脊是父亲，依偎在他边上，从他身上长出来的像是两个儿子，两个新的父亲。\n飞过秦岭很快就要降落到咸阳机场，没有注意到这架飞机刚才有没有在我们村西头的上空绕一圈。很多飞机可能是起飞或降落时调整方向，都会在那里掉个头。\n突然想到还没有坐过飞机前，这个机场附近曾经来过。那是一个寒假，父亲带着我和弟弟骑自行车跟随村里人来机场附近村子卖红糖。早上一大早哥俩骑在父亲车子后面，跟着同村人一起沿着公路从西向东骑行。那时候小孩也只有大自行车骑，骑过来磨得屁股疼。当时不理解为啥要骑这么远，现在想应该是机场附近村子比其他地方的要富裕些，冬季红糖等消费也稍多一些。只记得这里的飞机很大飞得很低，中午饿了吃着奶奶带给我们的锅盔很香。\n带着两个儿子来到二十多公里外的陌生村庄，父亲手一指你这边你这边就分配好工作，自己骑上车就走开了。我那时候可能初中，弟弟估计还小学。没有现在的手机或电话手表的联系方式，没有地图关注儿子们的位置，父亲就这样放手让我们自己去找寻自己的路。就像后来对儿子们的工作生活一样，父母亲完全支持我们个人的决定。我中专毕业十九岁开始修了几年铁路，然后就把当时装在干部档案袋里的饭碗扔掉，回西安自学些乱七八糟的东西又考研换了行业。弟弟也是中学物理老师干了没多久，就在深圳创业折腾。但是那天在一个陌生的村口分发了儿子后，父亲下午又神奇地都回收了回来，傍晚安全地领着俩儿子骑行回家。在我和弟弟妹妹的人生中，父亲一直是这样坚实、安全又神奇的依靠。\n还在想当时骑了多久到家，网约车司机电话打过来，出发前约好接机的车已经到了。这次终于可以导航到咱们村，过去的一年半时间里目的地大多是咸阳和西安的不同医院。每次我都会不由自主地尝试把目的地切换为咱们村，并比较从机场到医院和到家的距离。最终发现比起机场东边西安城里的医院，到机场西边咱们村更近一些。千里归来有家不能回，要跑到反方向的医院，那一刻莫名的伤感叠加对父亲状况的担忧，总是会鼻子酸酸的，要控制下情绪才能跟司机交流地址。直到在医院里见到父亲，见到围在父亲身边的一家人才回过神，那里那时才是家。\n网约车载着出了机场，看导航西安绕城高速连着的茂陵出口都修到咱们村边上，三十分钟就到家了。司机请求是否可以不走高速，时间就差五六分钟，接机里的十四块高速费他就省了，点头同意后就坐在车上打盹。车子在机场周边走了一段后走上一条笔直由东向西的公路，这条路，可能就是当年那个傍晚父亲领着我和弟弟骑行回家的路。\n车子走了半个小时，离家越来越近，车窗外也越发熟悉了。\n小十字路口往北一点应该就是镇上的初中，每天两个来回从田间的小路三四里走过来上学。父亲每学期结束的时候都会溜达到学校来，在墙上的排名榜找自己儿子的名字是不是出现在第一行。虽然他没念过多少书，但也用这种最粗糙的方式关注和管理着儿子的学习。还记得上学前班之前父亲给家里带回来一本非课本的小学数学书，我居然翻过好几遍。很难想像八十年代的偏远农村一个年轻父亲怎么做到的，也不知道他是县城里新华书店买来的，还是收破烂时候收来的。父亲跟村里小学的老师关系都保持得不错，一二年级时每次考试完，他都会把卷子里做错的抄回来，让重新做。仍然记得有个线段上表示数字，还是数字加减的题，他抄回来的图歪歪扭扭的更难看懂，做不出来还被说了。没有打印版的八十年代年，这种原始的错题本可能对实质的学习改进有限，更多的是对态度和习惯的管理，或者准确说是警示。\n车子右转往北直行就是我们村了。麦地里冬小麦长得很扎实，明年夏天应该又是个丰收季节。很多年都没参与过收割了，听说近些年都是收割机来到地头收完，人一点不需要参与。小时候全靠人，号称三夏大忙，真的是忙。记得有一年父亲带着母亲我和弟弟四个人把家北边的一块地麦子收完，赶时间去收南边一块地，从家门口经过父亲都没让我们进去歇会儿。每年三夏麦收季节，父亲经常白天晚上连轴转，不同名词术语的活很多很细，好像干不完。在我很小还参与不了农活的时候，记得父亲光着膀子干完活回家，经常简单洗完脸，蹲下来到我能够着的高度，让我用毛巾帮他擦脊背。那时候太小手上没劲，沾了水的毛巾好像很沉，两只手在父亲那晒得发黑的背上使劲推，才能把裹在汗水里的麦皮、灰尘刮下来，里面还夹杂有父亲背上被毒太阳晒得脱落的皮。\n来不及看太多，车子马上就到家了。想起刚才走过的路，看到的景，原来都是父亲领我们见世界最初的样子。我们从这里走出去，跑得很远。父亲一辈子就在这周围打转，包括那个傍晚。\n......\n几天里没怎么睡觉，很忙碌，但不困。做了多少事，见了多少人，说了多少话，掉了多少泪，一点都想不起来。偶尔想起来一点，感觉像做梦一样，不真实，不愿意相信。忘了从哪里看过，有个现象，人的记忆会自然过滤那些不愿意接受的瞬间。好像是这样的，这几天的记忆就像真空一样。只记得一段和豆哥的对话。豆哥说“爸爸，你刚才介绍爷爷的时候，虽然我听不懂全部陕西话，但是我听着听着眼泪就止不住地往外流”。他说的是一天晚上仪式里的读的一篇祭文，是前一天凌晨三点到四点我守着父亲的时候写的。安静的凌晨，注视着那个黑纱镶边的相框里的照片，那是父亲驾驶证照片放大的，回想着父亲这一生，边戳手机编辑文字边抹泪。写下四五百字追忆父亲，读的时候站在我身边的儿子也感染到了。父亲带他的时候豆哥还很小，但还是能记得爷爷带他做的有意思事情，如骑车带他从江边的坡上冲下来，带他越着台阶爬楼梯，带他去隔壁小区逗小鱼玩，至今对爷爷给他变的魔术还深信不疑。和豆哥讨论了人的生死，下一次我们国家的人口普查就统计不到你爷爷了，但是只要我们记得爷爷，他就永远活在我们心里，就永远活着。末了补充了下，写作文只要讲真实的事情，讲出真情实感，即使用最平实的词汇，也能打动人，也能写出好作文。\n......\n今天是元月五日，完成了送别父亲的仪式。刚才屋里门口几百多位来送别父亲的亲戚街坊在家里吃完中饭，陆陆续续都回了，家里也慢慢回归了安静。母亲心疼我们，让在炕上好好睡一觉。这几天白天晚上陪护父亲，困了就在前屋父母亲的房间大炕轮流睡一会，有点像去年过年在病房的那些夜。只是这次父亲平静地躺在那里，儿女只用在边上安静地守着。父母亲的大炕比病房里的陪护床要舒服很多，只是和当时一样，我和弟弟都来不及洗澡洗脚，炕上弥漫着不知道谁的脚臭味。\n父母楼下这个房间很大，那头放着电视、音响、台式电脑，各路电线杂乱地接在各级插板上，估计只有父亲当时第一次接线时能理清楚。沙发、茶几应该都是父亲这些年新换代的，比家里之前的布沙发要干净很多。炕上的水暖电热毯也比之前生炉子方便很多，水暖的睡着很舒服。房间里的每一个大小东西都是父亲置办和持续维护的。但是看到这些印象最深的是很多年前一个尴尬的名场面。太久了细节记不清了，一家人在房间里看电视，边看还边讨论电视内容。祖母忙着去厨房做饭就走出去了，接着母亲也出去帮忙。房间里就剩下我们兄妹三人和父亲在屋里接着看电视，特别安静地看电视。没多久当时很小的妹妹溜了，然后是弟弟还知道找个理由也跑了，作为老大的我坚持了好一会儿最终也当了逃兵，留下父亲一个人坐在沙发上无聊地盯着电视，等着叫吃饭。\n这是只是一个缩影，父亲因为对我们从小严厉，规则冰冷，不自觉地就形成了这样一个距离。即使父亲后来年龄慢慢大起来，脾气好起来，特别和孙子辈的非常亲近甚至宠溺，但儿子们形成的敬畏的距离很多年里也就只近了一点。现在回想起来父亲的教育方式确实简单粗暴，但却有道理，也有些效果。记得很早时候父亲要求的底线原则是不许撒谎，不许拿别人东西。原话是如果被他知道拿了别人东西，哪只手拿的就把那只手剁了。跟长辈打招呼必须看着人说话，吃饭要先给长辈端饭。陕西人平时就吃面，碗和筷子一起，给长辈端饭时筷子放在碗上要对齐。我和弟弟很小的时候就被父亲要求跟着家人干各种农活，弟弟那时候还小，在玉米地里闷得经常喊叫头晕，但也从来没能逃掉过。假期里还让跟着村里人骑车去二十多公里去机场附近卖红糖，尽管也就是参与下，前一天晚上才教怎么认识秤上的星，当然哥俩也就体验了那一次。\n印象最深的在我上学前班时父亲给我装订作业本的事。买一张大白纸对折再对折切割成32开大小，用订书机订起来，本子顶头还会用个纸条把切口包起来再装订，并且在包起来的页缝里会用笔标记总页数。父亲要求一个本子写完要拿回家检查，正反面都写满了，同时交上来的本子他会数页数，和他装订时写在页缝的数字对比。虽然没有说少一页会有啥惩罚，但我们确实严格遵守父亲的这个要求。记得有个夏天带的饮用水水瓶倒了，水洒在本子上。一页湿透了，不得不撕下来，放学回家第一时间向父亲报告了。\n虽然制定了很细的规则要求我们在各方面养成勤俭的生活习惯，但从记事起，父亲通过的他的勤劳和经营给我们物质上最大的安全感。在那个很多人还在听收音机匣子的偏远农村，父亲在村里很早就买了电视机。在小学前就会让看新闻联播，和新闻联播前陕西台的动画片。小学低年级时记得手里有零钱可以买自己喜欢的书，记得一次买了一本百科常识之类的书。去外地念书时，身边大部分同学都是家里每月寄生活费过来，父亲在带我报完名后递给我一个校对面邮政储蓄的存折，里面存着我一学期都没有花完的生活费。父亲给那个存折设的密码，是我直到现在一直在用的一组数字。父亲的这种养育方式一定程度上影响了我们兄妹生活和事业上的一些观念，在金钱和事业成就感方面可能更看重后者，老大老二先后把稳定的挣钱机会扔掉了，转而去追求所谓的个人价值。尽管开始几年父亲支持或者不干预，最近些年父亲也许是老了，有时会念叨老大如果一直在铁路上会多好，老二如果好好教书会多好。\n父亲的很多规矩非常简单粗暴，甚至在现在看也有点暴力，但确实帮助我们养成了基本的习惯。顺便说下，父亲规则中的暴力从来没有实施过。与之相反，有时里面不小心居然读出了其中隐藏的爱，特别是为人父之后。父亲的爱，和母亲的不一样，和我一直习惯和怀念的祖母无处不在的爱更不一样。读到的大多数作品里，母爱各种抒情，父爱只是一个个事件的简单描述，买橘子穿铁道，一家和一家不一样。和父亲独处的记忆本来就不是很多，也有几个这样的时刻。\n学前班的时候，跟着几个小朋友起哄喊一个高年级大块头哥哥的名字和外号，被人家教训了，好像也就是语言上吓唬了这几个小屁孩。当时父亲正好和几个人从学校附近经过，去地里干活。了解到后，没有走学校大门直接从墙上翻了过来，把那个哥哥摁在地上要求承诺以后不再以大欺小。后来说到这个事情邻居街坊会开玩笑说父亲的鲁莽，特别是放到现在的学校里，家长这种不文明行为基本不可想象。但好些年后当豆哥小学一年级和同学的意外冲突被他的父亲错误地担心为校园霸凌，引发了失态过激的反应时，我才理解了当年那个年轻的父亲顾不上绕学校正门直接翻墙保护儿子的冲动。\n十五岁那年去外省上学，父亲带我坐了一夜火车送到学校。报完名把领的被褥抱到宿舍，父亲让我站在边上，他把贴着我名字的下铺床擦了又擦，才把褥子床单铺好，又拎着两个被子角把被子装在被套里，叠好放在床头。然后让我跟他一起坐在床上，等其他舍友陆续到来。平时在家都是祖母和母亲铺床叠被收拾，那是我第一次见父亲做类似的事情，仿佛比母亲还要细心周到。过了会儿上铺来自沧州的舍友父母给儿子整理床铺，父亲虚伪地给人家说“孩子大了，这种事让他们自己干就好”。他只知道心疼他的笨儿子第一次离开家单独住宿，却不知道直到快一个月他的儿子才学会了铺床，洗袜子，学会了在宿舍的单人床上睡觉不蹬被子。前几天趁着冬日难得的好天气把豆哥房间的被褥都换下来在阳台上晾晒，晒好后好再装回去。那个傍晚抱着豆哥心爱的海洋舰船的被褥，闻着散发着太阳的香味，我想到了我的父亲，想到了宿舍的那个上午。\n另外一个记忆是一个夏天，和父母亲一起来拉刚收割的麦子。不记得啥动静惊扰我跑到地头大水渠前。渠南岸一群人拉着一辆架子车从西向东走，渠北岸我隔岸追着车子跑，边跑边哭。因为我看到车子上拉着我的父亲，好像有血。后来知道父亲那天开着拖拉机在狭小的渠沿上倒车时翻进了注满水的水渠里，压断了腿。当时太小了，再细节就记不清了。就记得父亲冲我拜拜手，严厉地说“不要哭，回去，快回去。别怕，爸没事”。\n躺在床上一直想事情，睡不着，就起来看家里哪些东西要收拾下，过两天就都要离开家了。从房间里走出来，眼睛看到家里的所有东西都能感受到父亲。院子里的餐桌、板凳，院子角上父亲扩建大门时换下来的门墩，还有院子里母亲正在洗我们这几天换下来衣服的洗衣机。家里的一块砖一片瓦，甚至窗台上的一颗钉子，一段铁丝，都是父亲带到这个家里的。在近些年父亲也一直在给家里更新换代，保证儿女们回家更舒适。楼上楼下门窗、地板、电器全部重新装修了，厨房换上了管道煤气、油烟机。父亲用自己的勤劳支持着这个家庭的温饱，养大了我们。父亲创造了这个家，创造了这个家里我们习惯的一切。但站在院子里我看到有些东西不是父亲准备的，而是我们不得不为父亲准备的。\n院子里挨着墙铁管焊接的两个用于行走练习的辅助器械，是父亲出院回家后，参照国医的康复设备在家里搭建的。回家的几个月里，每天母亲都会按照康复计划扶着父亲在这里走六个以上来回，再练习十几个下蹲。每天中午视频时母亲会让看下走的咋样，有时走的好一点，有时走不动。\n父亲的护理床搬到了后院，边上放着吸痰器、雾化器、加湿器，是按照病房的日常护理要求置备的。母亲每天晚上给父亲泡完脚后，搀着躺到在床上，切口消毒换药，配药雾化。母亲之前是村里的赤脚医生和妇联主任，除了护理之外有些医学上的辅助也会视情况实施。上次回家看着母亲忙完这些安顿下来都十二点了，躺下后半夜每隔一阵子吸痰、小便等护理，基本上睡不了整觉。\n房间里仓库一样堆着没有用完的护理垫、湿巾，都是妹妹定期成箱地买到家里。尽管母亲非常节省地使用着，用过的估计早就能堆满一个房间了。屋里地上的蛋白粉还有几罐没有拆开，母亲每次在通过胃管注射各种打碎的肉汁时添加一些。母亲细心地根据注入后的表现控制添加蛋白粉的数量，而不是按我们的建议一味地多加点。每天几顿饭都要破壁机打成汁状注入，高强度下破壁机一个劳损了，买了另外一个顶上来。母亲一个人却一直比那破壁机的刀头还刚硬地坚持着，陪着父亲一起坚持着。。\n收拾了会儿，有点累了，坐在后院门口的马扎板凳上。冬日的阳光从南面厢房的屋檐下照射进来，很暖和。母亲每天都会把父亲用轮椅推到这里晒太阳。和每天中午视频的时候父亲的状态差不多，坐在这里晒着太阳有点打盹了，比刚才趴在炕上还困。午后的阳光像电影院的光束一样投射在脚下的地上，像电影一样清晰地投射出关于父亲的记忆。那是我小时候用毛巾擦过父亲脊背上往下滚落的汗水；是我见过父亲扛过比他体重还要重很多的粮食麻袋；是我那年夏天隔岸追过那辆架子车上载着的父亲和他压骨折的腿。父亲用流血流汗支撑着这个家，养大了我们，也教会了我们父亲的样子。\n这时母亲让弟弟在院子里拉一根晾衣服的电线，我站起来从父亲的工具箱里找了一把钳子递给站在凳子上的弟弟。弟弟站在那里就像他的父亲一样。不知道什么时候开始，弟弟开始接班父亲支撑家里这些体力活、技术活，我也习惯性地给弟弟打下手。为了照顾父亲，弟弟十几个小时不休息开车回家，随着他那辆近十年的粤B车一起回来的还有他一直坚持的事业。这个的冬日午后的阳光下，在院子里学着父亲干活的兄弟两人心里都感受到了父亲。父亲，在二里外咱家地里，那个两小时前我们才送去歇息的地方，也想您的儿女了吧。按照咱们这里的习俗，两个儿子晚上一会儿就来看望您，在寒冷的冬夜里生把火，和您说说话。\n......\n元月八号，送别父亲的仪式后按照村里风俗陪了父亲三个晚上。准备返杭时接到通知到深圳出差，早上在家里吃完母亲做的早饭，弟弟开车送我到机场。在咸阳机场过了安检往登机口的路上又见到了秦岭，停下来认真地拍下来。\n早上弟弟开着他粤B的车送我到机场，这边下飞机又打到另一辆粤B的车，感觉挺奇妙的。另外今天从家里出发来深圳出差，感觉挺不同的，一点不累。在这辆车行驶的近一个小时行程中，继续快速在手机里编辑这个文档。在刚才的飞机上和车上快速把脑子里涌出的东西记录下来，记录2025年初这不一样的第一周，怀念我的父亲。下车后马上要切换到另外一个工作模式，担心没有时间想父亲了。\n下车住在公司附近的同一家酒店，继续两周前的同一个项目。熟悉的环境，熟悉的事情，就像没有离开过这里一样。父亲也像没有离开过一样，每天中午和母亲视频时，都能看到轮椅上晒太阳的父亲。\n酒店里快速安顿下就准备去公司，出门拔房卡的瞬间瞥见了门口镜子里的自己，就是记忆中父亲的样子。父亲，今天起，我就成了您。\n","link":"https://idouba.com/in-memory-of-my-father/","section":"posts","tags":["随笔","父亲"],"title":"今天起，我就成了您"},{"body":"","link":"https://idouba.com/","section":"","tags":null,"title":"爱豆吧！"},{"body":"","link":"https://idouba.com/tags/%E7%88%B6%E4%BA%B2/","section":"tags","tags":null,"title":"父亲"},{"body":"","link":"https://idouba.com/categories/%E7%88%B6%E4%BA%B2/","section":"categories","tags":null,"title":"父亲"},{"body":"","link":"https://idouba.com/tags/%E9%9A%8F%E7%AC%94/","section":"tags","tags":null,"title":"随笔"},{"body":"","link":"https://idouba.com/categories/%E9%9A%8F%E7%AC%94/","section":"categories","tags":null,"title":"随笔"},{"body":"","link":"https://idouba.com/tags/ai/","section":"tags","tags":null,"title":"AI"},{"body":"","link":"https://idouba.com/categories/ai/","section":"categories","tags":null,"title":"AI"},{"body":"","link":"https://idouba.com/tags/linear-regression/","section":"tags","tags":null,"title":"Linear regression"},{"body":"","link":"https://idouba.com/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/","section":"tags","tags":null,"title":"云原生"},{"body":"","link":"https://idouba.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/","section":"categories","tags":null,"title":"云原生"},{"body":" 背景 最近团队的业务除了面向通用计算外，越来越多的要处理面向AI场景的软硬件资源的供给、分发、调度等。虽然还是在熟悉的云原生领域，折腾的还是哪些对象哪些事儿，适配到一种新场景。但为了避免新瓶装老酒，能有机会做的更扎实，做出价值，对这个要服务领域内的一些东西也想花点时间和精力稍微了解下。\n国庆长假环太湖一圈回来，假期最后这两天豆哥被要求上课，正好难得集中时间可以稍微看些东西。暂时没有精力系统地构建，先入个门。作为一个云原生领域的从业者，目标是知道容器里跑的是什么，怎么跑的。\n学新东西时习惯用自己的文字，尽可能简单易懂地总结记录贯通下，做不到严谨、全面、深入、专业。开始前定个小目标只要做到基本的通、透、够用即可。\n说干就干，先从深度学习基础技术神经网络开始。Google一把，内容可真叫个多。确实最近身边不管曾经做什么的，摇身一变都能与AI扯上关系。这么多信息对我们这些局外人非常不友好。很多年前自学数学等相关基础课程时，习惯从稍微了解点的东西入手，有点脸熟的东西看着不怵。重拾十来年前尚老师Data Mining那门课程的部分内容，看看老的概念和新的技术能产生哪些联系。\n切入点线性回归 线性回归可能是一个比较适当的切入点，模型简单好理解。线性回归时通过一组数据点来拟合线性模型，找出一个或者多个特征变量和目标结果之间的关系，有了这个关系就可以带入条件预测结果。一个非常经典的线性回归例子就是二手房价预测。\n影响房价的因素很多，记得当时书上形式化表达是用了一个向量乘法y = wx + b。x向量由（x1,x2,x3,x4）组成，表示若干个属性。这里简单示意下假设只有两个因素x1、x2，分别表示屋子的房间数和面积，也不用向量乘了，就直接写成 y = w1 * x1 + w2 * x2 + b，y就房子价格。其中w1、w2和b称为线性回归模型的参数，w1、w2称为权重weight，b称为偏差bias。\n可以看到，作为一种最简单的回归模型，线性回归使用这种线性回归方程对一个或者多个自变量和因变量之间的关系进行建模。有了这个假设的模型，就可以根据已有的二手房成交记录求解出模型上的参数w1、w2和b，这就是老听说的模型训练。完成模型训练求解出线性回归模型的参数，就可以把房间数、面积x1、x2带入表达式，得到房子的预测价格，这就是一个推理过程。\n这个一个简单例子把模型的的表达、训练和推理过程最简单地顺一遍。省略了太多的信息和步骤，迭代着加上去应该就是关注的神经网络的关键内容。\n首先是模型的表达，通过最基础的数学知识，这个线性回归的输入、输出和运算过程可以大致画成这样。\n即使完全不了解神经网络，基于最朴素的概念理解，瞅着这个图上这些点的关系好像也已经和神经网络有点神似了。\n神经网络的概念 神经网络这个典型术语标准定义很多，总结下简单理解神经网络就是一种模拟人脑处理信息的方式。从数据中获取关联，在输入和输出中建立关系，特别是复杂的输入和输出之间。类似我们人脑中神经元构成一个复杂、高度互联的网络，互相发送电信号处理信息。神经网络由人工神经元组成，在这些神经元上运行算法，求解各种复杂的模型，所以我们说的神经网络完整点描述其实是人工神经网络。\n只是从外形简单比较，前面线性回归那个图看上去像一种单层或者单个神经元组成的神经网络，大致可以认为是神经网络的一个简单特例。\n神经网络的结构 经典的神经网络包括输入层、输出层和隐藏层。\n**输入层：**接受外部输入的数据，将数据输入给神经网络，简单处理后发给下一层。在预测房价这个线性模型中，输入层就是影响房价的两个属性。在另外一个典型应用图片分类中输入层就是像素，如100*100像素的图片就又10000个输入。 **隐藏层：**神经网络中大量的隐藏层从上一层，如输入层或者上一个隐藏层获取输入，进行数据处理，然后传递给下一层。神经网络的关键处理都集中在隐藏层，越复杂的模型、表达能力越强的模型，隐藏层的层数越多，隐藏层上的节点也越多。 **输出层：**输出神经网络对数据的最终处理结果。因为模型固定，输出层的节点数一般也是固定的，如上一个房价预测线性回归的图中，就只有一个输出。如果是分类的模型，一般有几个分类，就对应输出层有几个节点。 不考虑内部复杂实现，只看这个外部结构，从我们程序员的语言看，一个神经网络就像我们编程的一个方法。输入层对应这个方法定义的入参，输出层对应方法定义的返回值，隐藏层可以简单类比我们的方法实现逻辑。模型训练好后，去做推理时就是传入参数调用这个方法，得到返回值的过程。\n从数学的视角可能更准确一些，一个神经网络对应一个映射函数，输入层对应函数的自变量x1、x2，输出层对应函数的因变量y。训练过程就是找到函数表达式中的各个参数。有了这个求解的函数表达式，其他任意参数x1、x2带进去也能得到相应基本正确的y。\n作为映射函数，只要有一组输入就能映射到一组输出。除了这个根据房子大小、房间数映射出房价外，其他更强大的神经网络可以拟合更复杂的函数。对于大多数深度学习的应用，虽然我们没有办法像这个预测房价的例子这么直观地写出一个具体表达式，但还是可以理解存在这样一个函数映射，或者说通过训练有办法逼近一个理想的函数映射。\n记得从哪里看到黄教主说过“AI深度学习，也是一种解决难以指定的问题的算法和一种开发软件的新方法。想象我们有一个任意维度的通用函数逼近器”。如果设计的神经网络足够深、参数足够多，足够复杂就可以逼近任意复杂的函数映射。不只是预测房价这个简单的线性回归，也不只是当年学习的Han Jiawei的Data Mining课本上的分类、聚类、啤酒尿布频繁项这些业务固定的应用。\n为了便于理解把以上简单类比总结成下表。\n神经网络 程序视角 数学视角 模型 代码方法 映射函数 输入层 入参定义 自变量 隐藏层 方法体实现 函数表达式 输出层 返回值定义 因变量 训练 构造实现并UT验证修正 求解函数参数 推理 实际方法调用 带入新的自变量求解因变量 样板特征 Feature UT测试用例输入 已知的符合表达式的自变量取值 样板标签 Label UT用例预期输出 已知的符合表达式的因变量取值 以上两个临时起意的类别，前一个更像神经网络的物理存在。不管多复杂的神经网络，最终都是一个方法调用。实际应用中通过Restful接口或者其他应用协议调到推理服务上，获得一个输出，返回给调用方使用。而数学的这个类比更像神经网络的逻辑定义，模型本身的定义和模型训练、推理过程。\n同时基于这个牵强的类比发现，较之数学和AI模型定义了各种强大、复杂的逻辑表达，我们伟大的程序员才是物理上默默地承担了一切的那个角色。\n现在Pytorch、TensorFlow、Mindspore等各种AI框架都很完善了，定义了大量神经网络的层和内置方法，AI算法工程师已经可以直接用Python语言开发算法了。既然AI算法工程师都能写程序来描述算法，我们这些下层做平台、管资源的程序员们也得尝试往上了解一些，起码知道容器里跑的是什么，怎么跑的，怎么用资源，为啥这样用。从而更高效地运行好容器，管好资源，训练出黄老板说的通用函数逼近器，尽管咱现在用的资源不只是他一家的。\n有了基本概念，下面还是基于房价预测的例子稍微探究下神经网络中的一些重要过程，首先是神经网络的训练过程。\n神经网络训练 前面介绍例子时说过，房价预测模型的训练过程就是将已有的房屋成交记录作为训练集，训练得到线性回归模型。这个求解表达式中参数的过程就可以简单理解为神经网络的训练过程。\n详细的理解这个过程前，首先几个简单概念形式化了解下：\n样板Sample：训练集中每条记录称为一个训练样板Sample。 特征Feature：样本的关键属性，即与模型表达有关系的属性。如前面提到的房间数和房子大小，称为样本的特征Feature。 标签Label：样本中对应的结果，这个例子中实际成交价格称为标签Label。 基于这个训练集进行模型训练的过程大致是：\n计算预测结果。将训练数据输入给神经网络，计算预测的结果。这一步就是神经网络中著名前向传播。 计算预期结果和训练集中实际结果的差异，并不断调整更新参数，缩小差异，使得模型预测更准确。这一步就是神经网络中最关键的后向传播。 还是基于房价预测例子详细展开下这个过程。\n首先，随机分配一组模型参数w1、w2、b的取值作为初始化参数，带入训练集中的样板（x1,x2,y）。注意这个步骤的关键字不是名词”参数“，也不是名词”样本“，而是副词”随机“和形容词”初始化“。神经网络根据输入的x1、x2会求解得到一个预测的房子售价y-predict。通过比较预测房价y-predict和实际房价y的接近程度来判定预测准确程度。对于线性回归一般采用平方函数表示。\n第 i 个样本的误差表达是y-predict和y的差平方\n如果全面评估模型的质量，使用训练数据集中所有样本误差的平均值表示：\n损失函数 不管哪种表达，函数都只跟模型参数w1、w2、b有关系，这个函数称为损失函数Loss Function。模型训练的目标正是求解出参数w1、w2、b，使得损失函数取值尽可能小。把这个损失函数表达的误差画在立体坐标的垂直轴上，参数w和b表达的参数表示在水平轴上则得到如下图。这里只是示意，这个预测房价的例子包含了3个参数w1、w2、b，误差曲面是个空间4维，更一般的多参数是更多维的。\n对于这个基于线性回归的房价预测，取得极小值的参数是比较容易算出来。但对于大多数深度学习模型，模型本身比线性回归要复杂得多，并不能直接求解方式得到参数的取值。而需要通过不断迭代的方式使得损失函数获得极小值，这可能就是黄老板说的函数逼近器。试想如果模型参数都能直接求解，训练也就没有那么费劲，当然也不需要那么多资源，黄老板的生意可能也就没有理论支持了。\n梯度下降 现在一般应用的是一种叫梯度下降的方法寻找损失函数的极小值。在维基的解释中梯度Gradient是一种关于多元导数的描述。梯度是一个由各个自变量的偏导数所组成的一个向量。平常的一元函数的导数是标量值函数，而多元函数的梯度是向量值函数。\n梯度是一个向量，梯度下降法就是函数当前点对应梯度的反方向进行迭代搜索，找一个函数的局部最小值。还是基于房价预测这个例子看，对于（w1,w2,b）组成的这个向量，w1的偏导数的反方向就是在w1这个维度上的变化使得损失函数减小；同样w2的偏导数的反方向就是在w2这个维度上的变化使得损失函数减小。它们的偏导数组成的向量就就决定了一个方向去调整向量（w1,w2,b）的取值，使得损失函数值最小。\n基于形式化定义先大致理解到这个程度，幸亏维基在梯度的词条下附了这么生动的解释，帮助我们更好地理解梯度的含义。这样描述的：一个被卡在山上的人正在试图下山，大雾导致能见度非常低，看不见下山的道路。这个人通过观察他当前位置的陡峭程度，找到下山坡度最大的方向前进，从而最快速地下山。坡度 VS 梯度，很形象生动。梯度下降就是利用局部信息找到极小值的路径，即满足损失最小的权重集合。联想到上图的那个损失函数的曲面，山的坡度对应损失函数曲面在这个点的斜率。\n粗略总结前面的形式化定义和这个类别，再理解下梯度下降这个动作，梯度指向的在当前点函数值减少最多的方向。而作为一个向量，在神经网络中梯度表示更新权重的方向。\n在前面流程描述中，梯度下降使用训练数据集的所有样本数据计算梯度，这种方式也称为批量梯度下降BGD（Batch Gradient Descent） 。很明显一般训练数据量都比较大，这种基于整个数据集计算梯度的方式开销高的基本不能承受。改善方案就是在每次迭代中，随机采样一个样本计算梯度，这就是随机梯度下降SGD（Stochastic Gradient Descent）。实际使用中更多的是随机选择一组样本，使用这一组样本来计算梯度。这组随机样本称为小批量，这种机制就是经常被提到的小批量随机梯度（Mini-batch SGD）。\n计算得到梯度后，就可以基于梯度更新参数。这里涉及到一个重要的参数学习率。每次迭代模型参数的变化由梯度和学习率共同作用求得。梯度乘以学习率，得到模型参数在本次迭代后会减少的值。理论上，每次模型参数调整，损失函数取值会减小，模型准确率会提高。\n学习率的选择要适当，太小会影响效率，一直不收敛，太大则可能会跳过极值点，错过了最佳值。这个结合下山那个例子也很好理解：下山朝一个方向一次走太多，可能会错过最佳路线，但是一次走太小，没走几步就检查方向也浪费时间。\n突然联想到吴军博士《格局》一书中提到的做成事几个关键要素：位置、方向、方法、步伐和节奏。和这里的机制也对应的上，位置对应参数当前取值，方向就是梯度决定的向量更新的方向，步伐节奏可以不严谨地近似对应到学习率。基于当前位置找到前进的方向，并保持一定的节奏前进，终将达到目标点。\n除了学习率，小批量随机梯度过程中每次迭代的批量大小batch size是另一个控制训练过程的参数，这类参数称为超参数Hyper Parameter。不同于模型的中的这些参数w1、w2、b都由训练得到，超参数都是预先设定的。一般说的模型调参，调整的就是这些超参数。\n说到模型参数，想起在学校里十多年前做过的一个当时号称语义网络的项目Wizag，加州大学的梁教授设计的提取文本Concepts的核心算法被证明非常神奇有用。梁老师作为领域专家指导我们对于这个特定模型进行调参从而获取最优结果。而神经网络中这些需要调整的超参数是模型无关的通用参数。\n小批量梯度下降 小批量梯度下降用伪代码描述过程，大致是：\n1for i in range (epochs): 2 np.random.shuffle(data) 3 for mini_batch in get_mini_batch(data, batch_size) 4 sum_grad = 0 5 for x1, x2, y in mini_batch: 6 grad = gradient(loss_function x1,x2,y, params) 7 sum_grad + = grad 8 avg_grad = sum_grad / len(data) 9 params = params - learning_rate * avg_grad 基于colab上一个Pytorch线性回归的Notebook，在本地改造为本文中房价预测的例子。在本地环境上简单执行验证，在Python命令行里，逐个片段执行，可以看到不少细节，理解前面的描述的流程。\n1import torch 2import numpy as np 3 4inputs = np.array([[]], dtype=\u0026#39;float32\u0026#39;) 5targets = np.array([[]], dtype=\u0026#39;float32\u0026#39;) 6inputs = torch.from_numpy(inputs) 7targets = torch.from_numpy(targets) 8 9# 数据集 10from torch.utils.data import TensorDataset 11dataset = TensorDataset(inputs, targets) 12dataset[:3] 13 14# 加载数据集 15from torch.utils.data import DataLoader 16batch_size = 3 17train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) 18 19# 初始化模型参数w1、w2和b，随机取一组值 20w1 = torch.randn(2, 3, requires_grad=True) #Weights 21w2 = torch.randn(2, 3, requires_grad=True) #Weights 22b = torch.randn(2, requires_grad=True) # Bias 23 24# 线性回归模型定义 25def model(x1, x2): 26 return x1 @ w1.t() + x2 @ w2.t() + b 27 28# 损失函数定义 29def mse_loss(predictions, targets): 30 difference = predictions - targets 31 return torch.sum(difference * difference) / difference.numel() 32 33# 基于初始化的模型参数，比较预测值与样本集中对应实际值的差异。开始时损失函数取值非常大 34for x1,x2,y in train_loader: 35 preds = model(x1,x2) 36 print(\u0026#34;Prediction is :\\n\u0026#34;,preds) 37 print(\u0026#34;\\nActual targets is :\\n\u0026#34;,y) 38 print(\u0026#34;\\nLoss is: \u0026#34;,mse_loss(preds, y)) 39 break 40 41# 50轮epoch后，损失不断减小 42epochs = 50 43for i in range(epochs): 44 for x1,x2,y in train_loader: 45 # 预测值 46 preds = model(x1,x2) 47 # 计算损失，并进行反向传播 48 loss = mse_loss(preds, y) 49 loss.backward() 50 # 基于计算所得的梯度更新模型参数 51 with torch.no_grad(): 52 w1 -= w1.grad * 1e-5 # w1 = w1 - w1.grad * 0.00001 53\tw2 -= w2.grad * 1e-5 # w2 = w2 - w2.grad * 0.00001 54 b -= b.grad * 1e-5 55 w1.grad.zero_() 56 w2.grad.zero_() 57 b.grad.zero_() 58 print(f\u0026#34;Epoch {i}/{epochs}: Loss: {loss}\u0026#34;) 59 60# 训练好的模型对训练集上样本数据x1,x2带入模型，其实就是线性表达式得到对应预测的y，和实际y比较很接近了 61for x1,x2,y in train_loader: 62 preds = model(x1,x2) 63 print(\u0026#34;Prediction is :\\n\u0026#34;,preds) 64 print(\u0026#34;\\nActual targets is :\\n\u0026#34;,y) 65 break 正如这个真实的代码实践最后一个片段表达的，模型训练完成后，就可以应用这个训练得到模型进行预测。对应房价预测这个示例就是求解出了参数w1,w2,b即求解出了线性回归表达式，带入x1,x2就可以预测出y。只要知道房子的大小、卧室个数，就能推测出大概能卖多少钱。\n过拟合与欠拟合 一般训练好的模型在正式应用前都会有一个机制评价模型预测的准确性。最基本的考察模型在训练数据集上表现出的误差，称为训练误差（Training Error），更实用的是要考察模型在任意一个测试数据结上表现出误差，称为泛化误差（Generalization Error）。\n对应的会评价模型是否出现欠拟合Underfitting或过拟合Overfitting的问题。其中欠拟合指模型的训练误差一直太高，即压根儿就没有训练出来。过拟合指模型在测试集上验证的误差远高于在训练数据集上的误差，即训练的挺好，实际验证却不行。\n函数复杂度低表达能力弱，不容易找到合理的参数来表达目标模型，容易出现欠拟合。但是太复杂，训练时候能凑出比较好的效果，其实是迎合了测试集的模型特点，适应性弱，在最终测试集上表现并不好，就容易出现过拟合。另外样本数过少也容易发生过拟合。同时保证不出现欠拟合和过拟合对模型设计、训练要求挺高的。个人通俗的理解就是：要平时练习成绩好，同时要最终考试考的好，就要求真正学懂学通。首先要复习到位，即训练样本数量得充分。另外最关键要学懂学通，即掌握了内在的逻辑和模型，而不是只背会了练习的题库，或者只会做题库里的题型不会举一反三，导致练习时候成绩非常好，但是同样的题目换个样到了真正考试又做不出来了。\n以上尝试基于一个最基础的线性回归为切入点，描述深度学习的过程。和一个完整的流程比较，以上也只是覆盖了其中关键步骤。\nAI基础设施 敲完“关键”两个字，发现有点冒犯同一间办公室里的小伙伴们。我们在这个过程中，完成的模型部署、资源管理、负载调度、模型监控、故障恢复这些不应该轻易说成不关键。特别是现实中当前AI领域卖铲子的重要性，大模型训练最终比拼的还是算力、资源。如何能构建充足的资源满足越来越大的模型训练要求，如何高效地利用好这些稀缺的算力资源，最大化地发挥作用，这是我们发挥作用的舞台。这也就到了我们熟悉的领域了。\n在我们支持的客户场景中，找我们要资源的一定不是一个简单的线性回归。如果都是这种简单的模型也无所谓资源管理和调度了，也不需要种卡，在差不多点的单台服务器上就可以运行。很多年前在学校的Machine Learning课上，给尚老师交了个Gini Index的分类训练大作业。当时Java写了数据集加载、模型训练、测试集加载、模型准确度评估，这种简单小程序在当时那个Thinkpad的笔记本上就可以运行。找到我们云厂商提供资源的都是运行大模型或者超大模型的应用场景，模型结构复杂，参数巨大。训练这种模型的算力和内存开销是单机资源无法满足的，单个卡或者现在大多数8个卡组成的服务器也无法存储模型数据或者训练的中间结果。\n最具扩展性的方式自然是引入并行训练，把训练任务进行拆分，在多个设备上并行地运行任务，也就是分而治之的思路。不同于之前网格布道的背景经常说到微服务的分而治之把大的单体服务基于业务进行解耦拆分，只是为了开发、测试、部署、运维便捷轻量，是技术上的一种重构，本身从资源层面可以不用拆，大单体也是很好的实践。模型训练的拆分却是不得不拆，简单理解场景更像之前Hadoop为代表的大数据的那种风格。操作对象都是大的数据集，通过分布式方式解决单机的资源瓶颈，并通过并行提高总体的计算效率。\n数据并行 模型训练的并行有多种，和Hadoop类似的对数据集进行拆分的是数据并行。类似Hadoop的MapReduce在所有任务执行节点TaskTracker上运行相同的Mapper程序，对分配到该任务节点上的数据切片执行Map操作。模型训练的数据并行一般会把训练数据集在多个GPU上拆分，每个GPU上维护完整的模型和参数。和TaskTracker上每个Mapper对分配的数据集独立的进行Map运算类似，数据并行中每个GPU独立地执行前向传播计算预测值，独立地执行后向传播计算梯度并形成本地局部梯度。同时类似Hadoop的Reduce操作聚合数据，在数据并行中也有一个步骤需要将不同设备上的梯度进行聚合，聚合后的梯度向所有GPU广播并更新参数，这个聚合操作一般通过集合通讯AllReduce实现，连术语名字都和Hadoop的Reduce相似。两者的Reduce操作逻辑上都是在一种特殊的Worker上执行，物理上参与Reduce操作的是多个计算节点。Hadoop的MapReduce会选择多个运行Reducer的工作节点，而模型训练数据并行的AllReduce操作全部GPU节点都会参与梯度聚合，充分利用了每个GPU设备的带宽和算力。\n上面这段不严谨的比较，列个表格对照下。\n比较观点 深度学习数据并行 Hadoop MapReduce 并行方式 数据切分，计算复制 数据切分，计算复制 分布式计算节点形态 每个卡设备 每个TaskTracker任务节点 计算复制方式 每个卡维护完整的模型和参数 每个TaskTracker上分发相同的Mapper的Jar，执行相同的Map操作 数据切片 小批量样本 切分的Input数据 数据集存在形式 切分后加载到显卡的内存中 存在于分布式文件系统HDFS的DataNode上，本质上是计算找数据，在存储数据切片的节点上对节点上的数据分片执行Map操作 分布式操作 每个GPU独立地执行前向传播计算预测值，独立地执行后向传播计算梯度并形成本地局部梯度 TaskTracker上每个Mapper对分配的数据集独立的进行Map运算 归并操作 集合通讯AllReduce TaskTracker上运行Reducer 华为云官方文档上这张图比较完整地描述了数据并行的细节，不但覆盖了数据分配和前向传播、反向传播的过程，并且在反向传播中AllReduce的集中也包含在里面了。\n数据并行完整点流程总结大致这几个步骤：\n在所有GPU上初始化模型的全部参数。 在每次训练迭代中，将随机小批量样本均匀地切分成多份，如表示为N份，分发到对应的N个GPU上。 每个GPU基于收到的小批量样本进行前向传播和后向传播，独立计算梯度。 N个GPU的本地梯度聚合，得到本轮小批量样本的梯度。 将聚合后的梯度分发到N个GPU上。 N个GPU根据这次小批量随机梯度，更新模型参数。 个人基于理解进一步抽象地表达了以上数据并行流程中各部分的配合关系。\n可以看到数据并行在每个GPU设备上分发完整的模型和参数，当模型比较大时很容易超过单个设备的显存容量。解决方案当然逃不出一个拆字，就是把放不下的部分进行拆分，即拆分模型，引入模型并行。有两种切法，把模型横着切，或者竖着切。对应的一种是把神经网络的不同层切分到不同的设备上，即并行发生在层间，称为流水线并行PP(Pipeline Parallelism)；另一种把神经网络层内的参数切分到不同的GPU设备上，并行发生在层内，称为张量并行TP（Tensor Parallelism）。\n流水线并行 流水线并行，将模型的各个层拆分到不同的GPU上并行运行。这样可以减少每个设备上的内存消耗，同时提高效率。数据在神经网络的层间移动，物理上就是在不同的GPU间移动。逻辑上就像Linux的管道操作，或者很多编程语言里链式编程：\n1QueryBuilder queryBuilder = new QueryBuilder() 2 .select(\u0026#34;Id\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;) 3 .from(\u0026#34;users\u0026#34;) 4 .where(\u0026#34;age \u0026gt; 22\u0026#34;) 5 .orderBy(\u0026#34;Id\u0026#34;) 6 .limit(50); 如图示意的8层神经网络，每两层规划在一个GPU设备上。当数据从0层到1层、2层到3层都是在一个卡内传播，而从1层到2层、3层到4层会垮了不同的卡设备，产生通讯开销。对于流水线并行最需要考虑好在一个阶段执行的时候另外一段流水线空等待的问题，专业上把这种空闲叫气泡Bubble。解决气泡有各种专业的设计，多数思路和典型的操作系统流水线类似，把作业划分的再细一些，并行的粒度更小一些，更容易流水线并行。\n张量并行 张量并行将神经网络的模型、权重在GPU设备之间进行拆分。将张量沿着一定维度拆分成块，每个GPU设备上只处理一块。输入张量与权重张量相乘时，就使用矩阵乘法把权重矩阵基于列方向切分，每列分别与输入相乘，每个设备只计算权重矩阵的部分矩阵乘法，最后两个设备的分片连接起来组成最终输出。\n参照Pytorch 关于TP的说明，可以基于列切分，如下图：\n也可以基于行切分，如下图：\n更多的是行列混合切分，如下图：\n混合并行 以上，从原理上大致总结对比了下数据并行、流水线并行和张量并行。实际的模型训练中极少有只使用一种并行方式的场景，经常是几种并行一起使用，即混合并行。\n开源深度学习训练优化库DeepSpeed使用混合了数据并行、张量并行、流水线并行的3D混合并行进行万亿参数的训练。结合文章《DeepSpeed: Extreme-scale model training for everyone》的几个图理解混合并行：首先从左到右划分了不同的阶段，这是流水线并行，网络的0到7层、8到15层、16到23层、24到31层分别属于一个流水线阶段；每个流水线阶段通过张量并行再划分为4块MP0、MP1、MP2、MP3；上下两个分组表示把训练数据拆分到上下两组设备Rank0、Rank1，进行数据并行。\n可以看到混合并行能兼顾各种并行策略的优点，最大限度地提高效率，提高性能。其中张量并行通讯开销最大，一般建议在一个节点内，或各个大厂的Scale Up超节点内，使用内部设备间的高速带宽如NVlink互联。流水线并行将模型的不同层分散到不同的机器上运算，运算结果和中间数据通过机器间的网络传递。在这个内部并行的基础上，外部进行数据并行，每组服务器处理小批量的不同数据切片，这样服务器的计算资源可以充分地被利用，提高并发量，进而提高训练速度。\n同一篇文章中这张图忽略了细节更抽象地表达三种并行混合效果，相同颜色的表示在一个节点的GPU上。可以看到同一个节点的GPU间作模型并行，跨节点的GPU间作流水线并行和数据并行。\n以上都是来自论文的描述，为了和实际应用结合理解，找到了一个案例：BLOOM模型训练的一个集群。由48个NVIDIA DGX-A100服务器组成，每个服务器包含8张A100 GPU，共384张卡。\n从外往里看：首先，每48张卡一个分组，组内4张卡再分一个小组。在这个4个卡组成的小组上进行张量并行，因此张量并行的并行度是4；每个组内12个小组间进行流水线并行，流水线并行的并行度是12；在外层的8个分组间进行数据并行，数据并行的并行度是8。每个分组内48个GPU卡上存放了完整的模型副本。\n分布式训练如此重要，所以大部分深度学习框架TensorFlow、Pytorch、MindSpore、PaddlePaddle等在神经网络开发库也都提供了分布式训练能力。如Pytorch内置了完备的张量并行，流水线并行和数据并行的处理。\n框架已经提供了完备的能力，怎么在资源供给上跟的上是我们这些下层平台工程师们需要考虑的问题。虽然折腾的还是我们熟悉的计算、存储、网络。提供高质量的计算资源、节点内网络、节点间网络，将这些深度学习框架开发的训练任务按照其并行的要求调度到对应的资源节点上。这是以Kubernetes为代表的云原生技术最擅长的事情，也是我们每天都在做的事情。在Kubernetes中，当用户创建工作负载时，复杂每个实例的Pod作为基本调度单元会调度到业务期望的目标节点上，丰富的调度策略基于资源、亲和性等要求控制负载调度。在AI场景下，Kubernetes提供的这种能力达到提高资源利用率，特别是稀缺的GPU资源算力，从而已经成为面向AI资源管理的标准底座。\n如根据前面不同并行策略对资源的不同要求进行拓扑亲和调度，基于最优的资源组合分配，保证模型训练性能，如优先将模型并行的任务调度到高速互联的卡上。同时，考虑到同一个训练任务基于Kubernetes调度到不同节点时，若只是部分Pod调度成功，则可能会空等待未调度成功的任务，引起资源浪费。甚至多个作业互相等待资源，可能出现资源死锁。在AI调度中引入组调度Gang Schedule实现调度过程只有满足全部资源要求是才进行调度。另外为了提高资源利用率、减少资源碎片，也有各种装箱算法优先填满一个节点或网路。在调度中也可以让离线的训练和在线的推理等任务混部。当在线任务资源低谷时，通过调度使用空闲的资源运行训练任务。当响应要求高的在线任务到来时驱逐离线任务，从而保证总体资源利用率。\n除了计算的调度外，Kubernetes对存储、网络的使用都可以以一种更Native的方式扩展到AI场景。如前面描述多轮训练时，生成的数据量大无法全部在内存存储，势必引入大量的本地缓存。基于Kubernetes的存储插件对接厂商或开源的高性能分布式文件系统已经成为一个标准选择，大量应用到AI训练场景中。\n毕竟大家越来越发现提高智力的唯一路径就是提高算力，大力出奇迹越来越被认可。这些通过云原生的方式进行资源管理是我们比较熟悉的。通过AI业务本身逐步深入的理解，希望能帮助我们更好地管好大模型，运维好大模型，构建一个强大的AI-Infra。\n写到这里发现居然点题了。\n最后 说到最后，最近才开始入坑新领域，新事物入门时，习惯用个人的文字描述出来，自说自话，基本要求比原始资料友好眼熟即可。另外一个习惯，相信事物总是通的，在跨领域或者一个大的领域范围内，不自觉地会对新东西适当抽象并和之前熟悉的领域对象进行横向对比，东拉西扯，尝试贯通。因此文中有些描述，特别是有些另类视角的比较忽略了不少细节，只是做个思路导入，就全当个托儿吧。\n","link":"https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/","section":"posts","tags":["AI","深度学习","神经网络","Linear regression","云原生"],"title":"云原生工程师入坑AI深度学习系列（一）：从线性回归入门神经网络"},{"body":"","link":"https://idouba.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"categories","tags":null,"title":"机器学习"},{"body":"","link":"https://idouba.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","section":"tags","tags":null,"title":"深度学习"},{"body":"","link":"https://idouba.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"tags","tags":null,"title":"神经网络"},{"body":"","link":"https://idouba.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"categories","tags":null,"title":"神经网络"},{"body":"","link":"https://idouba.com/tags/%E5%A4%AA%E6%B9%96/","section":"tags","tags":null,"title":"太湖"},{"body":"","link":"https://idouba.com/tags/%E6%97%85%E6%B8%B8/","section":"tags","tags":null,"title":"旅游"},{"body":"","link":"https://idouba.com/tags/%E8%B1%86%E5%93%A5/","section":"tags","tags":null,"title":"豆哥"},{"body":"","link":"https://idouba.com/categories/%E8%B1%86%E5%93%A5/","section":"categories","tags":null,"title":"豆哥"},{"body":" 路线： 游记： 未完待续。。\n","link":"https://idouba.com/doudou-and-douba-drive-around-taihu-day5-xishandao/","section":"posts","tags":["随笔","豆哥","旅游","太湖"],"title":"豆哥国庆环太湖Day5：西山岛（苏州-太湖-杭州）"},{"body":" 路线： 游记： 恐龙园一天很紧张，很早起床，很晚回到房间。答应豆哥第二天可以自由行了，目的地苏州，晚上到达西山岛就行。晚上睡得很好，自然醒也没有太晚。豆哥说他不想把出来旅游的时光浪费在房间里，醒来很快就洗漱完毕，穿上衣服，收拾好箱子退房。\n在先吃早餐回来退房，还是退房拉着箱子去吃早餐这个事上豆哥选择了后者，这样节省时间。但我们漏了一点，那就是哪里有早餐，坚决不吃昨天早上的汤面了，一定找个包子油条这种早餐铺。查了下开车过去三公里的一个巷子里但不好停车，于是豆哥领着他爸瞎晃悠。拉着箱子走了四五百米，找到了一个牛肉汤的店。\n牛肉粉丝汤、鸡丝鸡蛋汤，还有脆脆的烤饼，俩人吃的连一点汤渣都不剩。豆哥说这是他这两天吃的最舒服的一顿早餐了，不但比昨天恐的汤面好吃，比前天酒店的早餐也好吃。吃饱了心情好情绪就是高，拉着箱子原路返回也浑身是劲。\n下车库找到那个民防干厕所的位置，也就找到了车。目标苏州，准备一路狂飙。很快就上了高架，虽然不是高速，但也基本可以开到八十左右，不堵，开得很舒服，聊得也很欢乐。开了很久才导航到一个高速入口，前面车在排队，看着隔壁车道四五辆车追尾在一起，好在好像速度都不太快。豆哥兴奋地趴在车窗看，老司机提醒他要专注自己的方向盘，不要光看热闹。\n高速有一段能看见太湖，豆哥很兴奋，昨天一整天憋在恐龙园，一天没见太湖了。更兴奋的是当导航屏幕出现一大片无边的蓝色区域，我们开到太湖里面了。对，我们开到太湖下面了。太湖隧道顶上的蓝天白云图像很舒服（开车太快没有照片，从链接的新闻稿里提取了一张和那天看到类似的照片），不只是前排的驾驶员开着舒服，坐在后排看热闹的更是瞪大眼睛这边瞅瞅那边瞧瞧。豆爸之前只是修过铁路桥，当时秦沈客运专线我们的桥边上有几个小铁路涵洞的工地去过。单位的老师傅有修过隧道的，但想想我们搞个桥墩都得搞围堰把河里的水排到周边再施工，湖里面修这么长一个隧道至少得把太湖隔成两段，在中间施工。\n在一个很小的出口下了高速，接着开进了一段很窄的村庄间的公路，车不多，很安静。豆哥正在吐槽这个导航瞎导时，车子已经七拐八拐地拐到了另外太湖边。原来出发前选路线的时候专门选了一条沿太湖最多的路线。这算上了正路了，我们来到了苏州的太湖。\n和前面几段环湖的公路一样，越往里碰到的环湖骑行的车子越多。好像还有摩托车跟着的，我说猜那个摩托车是给边上自行车提供后期保障的，豆哥觉得我在瞎扯。\n和前面几段一样，停在湖边的车也越来越多，这边湖边和公路有比较宽的一片小树林，车子都停在树木稀疏的草地上，比起前面几段只能强行在公路上靠边，这种管够的停车场倒使得俩人选择困难了，比一大片空车位停哪个那种选择困难还要困难。不知道停哪里，贪婪地不知道想看哪处风景。昨天前天在园区里可憋坏了，就想着在湖边撒个欢。最终还是随便方向盘右转开下公路停在了树林一处。\n湖边草地上很多搭帐篷打牌、野餐的。其中看到一家支了个炉子在烧烤，豆哥凑在跟前看了好半天。沿着湖边往前走了不少，停下来坐在湖边才想起来爷俩各自手里就拿了一瓶水，车上吃的一点没带下来，干坐着有点单调。特别是豆哥又惦记上刚才湖边那家烧烤越想越馋，虽然出发前刚吃过早饭，居然有点饿了。午饭也是动力源泉，起身继续赶路，目标光福的二十八里太湖船菜。\n往前开绕过一片庄稼地，进入了类似开发区的市区。路特别宽，路上车并不多。开着开着豆哥居然尖叫起来，原来隔壁开来了有轨电车。不同于在长春见到的跟传统火车类似宽窄的那种有轨电车，这种电车貌似更宽一些，和武汉光谷的好像有点像。也许是和电车有缘，在一个路口居然开过了，需要往回掉头，正好左边有个电车开过来，豆哥欢呼声中我们和电车并排飚了把速。不知道豆哥手表有没有完整拍下来，开车司机的手机里没有，只好先这里来个图。\n继续往前开往吃饭的目的地，居然真的是情绪到了，越走越饿了。豆哥这才总结刚才路径规划有点不合理，没有吃饭在湖边上停留太久了。那还能怎么办，油门表示下，抓点紧吧。终于在一个类似小镇的河边上找到这个太湖船菜。\n很大的一个饭店，和之前坐在座位上扫码点菜不同。饭店门口把各种菜的配料塑封好放在哪里，每个菜边上一组牌子，需要点哪个菜，直接去拿牌子就好了。每个菜分量看着都很大，并且每个看着都很好吃，两个饿汉居然不知道怎么选。选好后在一个靠窗的四人位置坐下来，窗外小桥流水，很惬意。才发现这几天居然都没有好好吃中饭，三国城和恐龙园的中饭都是讲究来的。菜上来果然分量都很大，另外发现不管鱼、肉、虾，还是腊肉、肘子，不管口味红烧还是炒的，居然两个北方人都感觉居然咸味都比较重。菜量实在太大了，俩人可劲吃，还是有些没吃完。\n吃饱饭继续赶路，晚上西山岛休息，下午大片时间选一个经典的地方好好玩下。也没有做什么攻略，从豆哥这几天的安排看，恐龙园、三国城这些去过了，今天一定选一个自然风景的，最好是跟太湖比较亲近的。于是在太湖国家湿地公园，和太湖渔洋山风景区中间选一个。虽然两个都要门票，选后者的原因是离西山岛更近一些，在一个路线不太熟悉的地方离晚上睡觉地方近一点总能安心一点。事实证明这个决定出发点对，但过程却要曲折很多，也可以理解成精彩很多。\n主打就是一个稀里糊涂，稀里糊涂导航没找到公园正门停车在一个小路口，稀里糊涂从小路上山居然无意逃票了，看完夕阳稀里糊涂下山找不到车了，稀里糊涂打车去找车景区封路车不让进了，天黑黑的傍晚饥寒交迫在深山里沿着公路走了两三公里没找到车。有点精彩，慢慢道来。\n从太湖船菜出来目标渔阳山这段基本上都在太湖附近了。先穿过一段很窄的山间公路，路上能看到不少拿着登山杖的路人，看面貌是刚下山。穿出来就到了太湖边，极致美景的太湖边。湖面开阔，两三点的阳光从湖面上反射出来，小学生作文的大镜子用在这里及其贴切。这段湖在车行的左手边，右手边都是各种风格的民宿和饭店，每家下面人都很多。公路挨着湖边，车子沿着公路两边停满了，加上不时有人从湖边穿过公路去右边民宿，要开的很小心。\n按照导航的目标已经进入渔洋山景区了，但是景区大门在哪里，开过好几个疑似的门口，询问路人也没有人说清楚，只能硬着头皮继续往前开，在一个小路附近看着有登山者走下来，判断这条路至少能走上去。于是车停在路边，和豆哥各自包里背了几瓶水就准备沿着那个小路进山。从最后脚步绘出的步行地图看，俩人正好从正门的反方向进入的，可能也可以称为后山吧。\n开始的一段可以车行的公路不算陡，走了一段豆哥说刚才应该直接车子开进来。确实可以开进来，但是整段路走过来没有迎面碰到反方向下山的，也没有看到前面后面有同行的，有点慌兮兮的。但是豆哥却很有兴致，两个大男人，前几天刚爬过十公里的西湖群山，这点强度简直毛毛雨。走了一会儿后面有个人超了上来，一看就是本地人，背着手拿了个玻璃水杯，跟走在自己村头一样闲庭散步地就把我们超过了，很快就超的没影了。\n好在公路上山没有分叉，就一直往前走，直到前面无路可走了。有个大门锁住了前面的路，这怎么办。我有点怂了还是往回走，接着开车找景区大门吧。豆哥说，那刚才那人上哪里去了。走近一看，那个锁起来的大门边上铁丝网有个大窟窿。钻过去前面是木质台阶的上山路，挺陡的，但是每段很规则，先几段上升的，然后一小段水平的，整个就是架空在山里树林间的梯子。山林里树木比较密，木质台阶和树很近，走起来有点闷，豆哥走走就想歇。还好距离不是很长，走了二十分钟差不多I就走到了台阶终点。\n终点一个大木门，居然也锁着。但是边上木栏杆不高，豆哥就带着从栏杆上翻过去，就走到了一个大路上。比刚才走过的一段公路和台阶的上山路要干净整洁，关键能看到很多人了。好像从安静的动物植物的世界走到人的社会了，这下确认了，我们这才是真进了景区了。有个跟着家长走的小朋友一直朝豆哥看，估计是在惊讶这是从哪里杀出的一路神秘人马啊。豆哥一下子有了当年韩信暗度陈仓的感觉，不过也坐实了“2024年国庆长假第四天苏州太湖渔洋山景区发现两个外地男子绕后山逃票进入景区”的恶性事件。回头看刚才走过的木门，顶上写着“孙武道”。赶紧手机查了下，跟豆哥一起学习，原来孙武“三令五申”、“斩二妃”、拜将的故事就发生在这附近。\n从苏武道杀出沿着上山公路，随着零零散散的人流往上走。走不了几步就有一个绿地平台，从平台上往外围看太湖就在手能够到的地方。不用像刚才一样找路探路赶路，俩人终于可以放松地享受美景了。偷拍豆哥一张照片，从肢体语言看，小哥很放松。不管小哥嘴上说没啥看的，对于从树丛中钻出来，从栏杆上翻过来的人来说，眼前这景观的冲击力，小哥就是装淡定也扛不住啊。更美的路还还在前面，继续。\n沿着上山的公路一个大弯还是拐了几个弯到了另外一个歇脚点，原来是景区缆车的终点。有个刚下缆车的年轻人向豆哥打听山上路线，豆哥说自己也刚上来。然后俩人交流起来怎么上来的，然后就交流起来门票多少钱缆车多少钱。豆哥很兴奋地算起来俩人这趟误打误撞居然省了这么多钱，等下要好好吃一顿。‘\n既然是缆车终点，这边差不多应该就是山顶了吧。果然附近有一个更大的观景台，排队打卡拍照的人很多。爷俩站边上都稍微等了会儿，豆哥不耐烦地站过去，就拍了这张。豆哥疑惑为啥不管拐几个弯，太湖总在我们手边。好问题，拿出地图观察下，原来我们在的地方三面临湖，像一个半岛一样伸到太湖里。\n山顶大片区域比较开阔，往前走看到一个几层高的建筑。不知道是楼还是塔，豆爸是修路修桥，不是盖楼的，不太懂。走近看上面写着“渔洋阁”，阁应该也是楼吧。渔洋阁面向湖的方向好像是太湖的东山岛和西山岛，站在这里拍出的湖面很开阔。豆哥兴趣一般，在楼下观景台拍了张照片就要跑，硬拽着进去看看。\n拍完湖，原地转过身拍了另一面的渔洋阁，看照片才发现下面一层的“江南第一阁”五个字。和江南的雷峰塔这些看着古建筑一样，一看就是近现代修的仿古建筑，但是还是挺有特色的。进到楼里介绍才知道渔洋山三面临湖，形如鳌首，这个渔洋阁为了表达“独占鳌头”的意思，把建筑外形设计成了龙鳌背着宝塔造型。也第一次听说鳌文化和状元文化。据说每年考试的学生都会来渔洋阁祈福文运。忘了从这里还是之前哪里看到说中国科举自古南北方分开考是因为如果放在一起考北方铁定考不过南方，分开考录取100个北方50南方50，如果一起考估计北方10个都考不到。望着在在鳌文化宣传走廊间无知地看热闹的豆哥，想着他北方血液和智商遗传的背景，却在江南湖边戏水、弄潮，居然莫名地同情起来。\n往上走每层有不同的主题文化介绍，比较仔细地看了个热闹就逐层往上爬。顶楼有个大钟，微信扫码20，敲钟祈福。\n以为登顶渔阳山，并且在渔阳阁上270度远眺太湖景观就是极致体验，那知道最美风景是下山时无意走入的一个角落。\n从渔阳阁下来，在平台台阶上坐了会儿喝了点水，豆哥还有兴致地给老爸拍了几张湖景照片。天色已经有点慢慢变暗，决定下山。\n沿着下山公路走了没多久，看到一个通往一处矮树林的小径。瞄了一眼打算继续往前走，不知道哪股好奇心驱动，俩人决定进去瞅瞅。傍晚天不是很亮了，走在小树林里有点慌。在纠结继续往前走还是往回走的时候居然已经穿过了小树林。眼前突然出现了一片开阔的草地，草地的尽头是无边的太湖。对于突围出树林，突然踏上这个隐蔽奢华的观景大阳台，视觉和情绪冲击不亚于几个小时前从孙武道连爬代翻进入景区。草地上有人说这里是太湖最佳观夕阳的位置，他们经常来，今天专门坐在这里等日落。误打误撞让我们在对的时间赶到了对的地方。刚才那个人的介绍应该是有道理的，因为我们看到了一个熟悉的身影，就是下午从后山超过我们带领我们从孙武道上山那个当地人。\n","link":"https://idouba.com/doudou-and-douba-drive-around-taihu-day4-yuyangshan/","section":"posts","tags":["随笔","豆哥","旅游","太湖"],"title":"豆哥国庆环太湖Day4：渔洋山（常州-太湖-苏州）"},{"body":" 路线： 游记： 今天车子没有轨迹，老老实实地停在那个人防临时厕所的位置。 公寓型酒店没有早餐，和豆哥早起在楼下找了个面馆吃了碗面就跟着人流穿过马路准备进园了。 离大门很远地方就是各种恐龙元素的雕塑、文字，气氛很好好，和豆哥之前去上海迪士尼外围有那么一点点像。排队很长，儿童检票要量身高。豆哥说幸亏咱买了两张成人票，要贪便宜买个一大一小就进不来了。 进了园首先冲到好像叫恐龙研究院TODO的项目，据说是通过基因还原方式重现了白垩纪时期包括恐龙在内的各种物种。排在队伍最后面，队伍边上是园区中心一个大水池，里面各种巨形恐龙造型，头顶一个叫过山龙的过山车载着这个园区里最勇敢的冒险者一阵一阵呼啸而过。\n豆哥抬头看看，问我说爸你敢玩这个不，我说我不敢，豆哥说我也不敢。爷俩连互相鼓励，简单思想斗争去尝试一把的冲动都没有。老父亲也没有拿出一点点表率示范作用，怂啊。想当年东京迪士尼那个Space montain被幼儿园刚毕业的豆哥怂恿硬着头皮去排队，从排队到最终进入有五六个可能给惊险估计不足临时改变主意的人设计的TODO退出的小门，都硬撑着没有放弃。现在豆哥长大了些，反倒都怂得真实简单了。 室外排了一个小时左右，清晨阳光下，看着园区每挪一步的各种风景，也不觉得累。豆哥说他在上海迪士尼和北京环球影城的时候有很多人买那种免排队的票，他和妈妈从来不买。我说你看排队等待不也是今天游玩的一部分么，我们平时哪有这么大块时间不干别的就在这儿干聊天的。种票免排队省下来的时间都去了哪儿了，是不是做了更有意义的事情。 一个小时后进入“研究院”，发现里面还是排队，边上的指引牌提示还有五个好像被称为车间的大块区域，人流被组织引导着像动物园的巨蟒一样，七折八折地蜷在一个狭小空间里。为了缓解游客排队等待的疲劳，金属外壳的空间布置的有点像科幻电影里的飞行器舱内那种风格，透着科技感、金属感。每个区域的四周陈列了各种恐龙相关的科普研究信息。\n第一个房间是一些恐龙化石标本。豆哥凑上那些恐龙骨架的窗口，很快确认是人工复制的不是真的化石。第二个房间好像是很多合成视频，恐龙在森林里活动，视频分辨率很粗糙，看着有点低成本。排到下面一个房间，科技感和交互感更明显，各个科学家的实验室，里面有简单蜡像的科学家和各种科研设备，电子仪器、试管、天平等，墙壁上还有各种类似科学家手稿的公式。看着神神秘秘，科学气氛拉满，看不大懂，也不能给豆哥过多讲解，只能说你看这些都是各国研究恐龙的科学家。努力地试图在不认识的科学家文字中找出认识内容，突然看到一个公式中将二氧化碳中的二标记到元素符号右上角而不是右下角，瞬间神秘感散落一地。前面的专家仪器视频全部是造气氛，认真就输了。\n排了不到两个小时终于轮到上车了。\n未完待续。。\n","link":"https://idouba.com/doudou-and-douba-drive-around-taihu-day3-dinosaur-garden/","section":"posts","tags":["随笔","豆哥","旅游","太湖"],"title":"豆哥国庆环太湖Day3：恐龙园（常州-常州-常州）"},{"body":" 路线： 游记： 第一天晚上豆哥做完作业早早睡着了，简单规划了下第二天的目的地。和第一天一样，不规划详细路线，定个目的地就开过去，沿路随意。 在无锡一众鼋头渚、拈花湾、蠡园选择了三国水浒城，豆哥在很小时候就收集了三国人物卡，经常对其中人物战斗力智慧值有不同见解。现场体验下应该会很兴奋。\n预订的双床房不太好，临时升级了高级大床房。一晚上俩人抢被子盖，没抢过他。早上挺早就起来吃早餐。早餐很简单，和豆哥之前出来旅游经历的丰盛自住早餐不同。我吃了玉米红薯馒头稀饭，豆哥给自己烤面包见番茄酱，热牛奶。\n养成了个不错的习惯，下楼吃饭前各自整理各自行李，房间收拾干净，吃完饭拿行李时再检查一遍，确保没落下小东西。起床时说好了，不同于之前总住一个酒店房间，这次每天换一个地方，敏捷机动，要随时收拾好东西准备转移。\n出门开车，比起昨天的大风天，天气晴好，在小县城的街道里开着很惬意。但得开得更小心，因为随时有横穿马路的行人，还有不打转向灯随时快速变道的车子。出了县城沿范蠡大道一直开，比昨天的高速还通畅，偶尔还能飞过太湖水流过来的某个大桥，开得很舒服，豆哥坐在后排那叫一个惬意。\n好心情在一个拐弯就戛然而止，下了范蠡大道，拐进了一段好像叫周铁的县道，路窄车多。穿过村庄穿过庄稼地，就到了另外一段太湖边。明亮的湖面，和昨天湖州那边湖至少有两点不同。第一颜色没有那么黄，更清澈。第二湖面基本没有浪，很安静，猜想可能与昨天大风天今天大晴天有一点关系，更重要的应该还是不同区域自身不同的水文特点。早上九点多的太阳照在湖面上，在透过车窗反射到眼里，微微有点刺眼。但不是草泥马的远光灯那种，不难受。就想起豆哥作文中描写西湖湘湖用烂了的四个字：波光粼粼。\n经历了刚才堵车的焦躁，幸福来的太突然。俩人都还没缓过劲来。不知道豆哥有没有领悟到，这就是生活。\n故意减慢了速度，沿着湖一直开。开过一段常州的太湖，很快又进入无锡的太湖。路边走一段就有各种湖边绿地上露营，车子沿公路两边停的满满当当。因为赶时间，没有来及下车停留，和豆哥商量，地址Mark下了，下次带着妈妈一起来这边湖边走走看看。地址好像是中华孝道园附近。\n再往前就进入无锡市区了，能看见公交和地铁站了。车很多，和杭州一样多，特别是进景区的地方，和杭州一样堵。四五个车道，满满当当，看不到头。\n排着队一点点往前，经过一个大十字，就进入了市区的湖边，应该是湖边的市区，其实就是景区。豆哥说和杭州更像了，右边的湖，湖边的路，路边的树，树下的车 一模一样。这个角落的太湖，湖水和湖边的小风景都和西湖很像。不同是西湖看远处能看见尽头的群山，这里看远处，没有边。\n车子沿湖再前开过一段树荫的小路，和闻涛路一桥往西的一段感觉很像，按照导航停到了无锡蠡湖八号酒店。这边有一点点开阔，车子路边就可以停。跟着一个接驳大巴前往三国水浒城。越接近越堵，最后一段八百米走了十几分钟。\n门口的文字提示这里的标准名称是：中央电视总台无锡影视基地。\n我们进的门是水浒城，左右两边分别是一百单八将的名号，前面四个铜人分别是武松 李逵 鲁智深和林冲。和其中几个匆匆合个影，豆哥就急着去找卢俊义。费了半天劲，终于在宋江附近找到了。然后才检票进去。\n导游鼓动大家买电瓶车的票，说指着地图说从一边到另一边有多远，豆哥很兴奋地说那样走起来才得劲。全车就这爷俩没被忽悠观光车。\n进来第一感觉像北方的小公园，可能是为了复现山东好汉的生活，里面的树木各种都更像北方的，个别地方有点像济南的大明湖。\n第一站水泊梁山，可惜晁天王们的聚义堂关闭了，只在山下看了看。沿着湖往前瞎溜达，走进了一个小村庄，看介绍是阮氏三兄弟的家，水浒传里劫生辰纲就是在这里拍的。\n湖面上不断有各种游船里里外外，停靠水泊梁山码头。等了大半个小时终于等到我们的船，不同于前面看到几艘多层华丽又威猛的大船，这是一艘只有一层的小体格游船，豆哥还有点失望。登船一看船头的旗号“诸葛”，原来这是孔明先生的船。\n不同于穿梭于景区里面，会有一种只见树木不见森林的感觉，从湖上游船上在一个较远的视角更能感受到全貌。水泊梁山的山头，外侧无边的太湖水，内测沿湖修建影视城，先是水浒城、然后是三国城。比起水浒城的各种小场景，前方的三国城要宏伟的多。宏伟到了极致的是诸葛游船的终点——东吴水寨。\n电视里三国演义这段场面已经完全不记得了，眼前高大的营寨大门立在无边的太湖水中，透漏出的霸气和威武视觉上非常有冲击，特别是从游船上远眺，到船靠近时的仰望。有点像被周大都督检阅的感觉。\n要说检阅那必须是周瑜点将台。下了码头豆哥就带着爬上了这个大高台，就在水寨的附近。豆哥擂了鼓，还和周都督合了影。\n水寨门口的周都督的操练场上操练了一把，天气很好，参与的很欢乐。\n周瑜点将台往太湖边上走一点就到了周瑜的死对头诸葛亮借东风的地方。雕塑的诸葛亮披头散发地持剑正在作法。比起刚才对周瑜的热情和仰慕，豆哥对这个孔明先生貌似不太感冒，围着转了小圈就拉着下到跟前的太湖水边去看风景。\n今天早先都是站在高处观看湖水，这会儿才有机会跟湖州那边一样，能俯下身子，去感受湖水。发现这里的湖水也有浪，虽然没有湖州那边的浪大，但也有节奏地拍打着脚下的湖边大石头。\n沿湖走走就又走到了水寨。刚才下了船就只是乘客身份从水寨里穿出来赶路，这会儿才真正游客身份在水寨前观赏。不同于水上视角的广阔雄伟，在岸上看是另外一番气势。特别是迎风呼啦啦响的东吴军旗后面一艘艘战船忙碌地穿梭的背景，威武之外多了生动和活力。气氛到了，豆哥跟东吴军旗的合影也显露了几分英姿飒爽了。\n穿过深入到湖中间的一长段水上走廊，又经过岸边的一个孙尚香饭堂、微缩版的赤壁战船群。来到了岸上最雄伟的建筑吴王宫。@TODO豆哥补充\n然后穿过桃源三结义的花园和曹操点将台，终于找到了豆哥进入院门就心心念的三英战吕布马场。整个表演场的面积跟一个足球场差不多大。或者还要大一点，马跑的场子当然是比人跑的要大。提前四十分钟到现场，已经满满当当了。往里走有个坐第一排的热心人挪了挪给我们俩挤出一小块位置，坐进去迎着下午正热的太阳那叫一个晒。虽然前排有人撑伞，但是两位绅士考虑到可能影响后排人看表演，还是选择了硬扛。扛了一会儿，发现比大太阳更刺激的，更刺鼻的是浓郁的马粪味道，第一排离得最近，味道最爽。平时有点矫情的豆哥这次却非常淡定，想着半个小时后就要开场的三英战吕布这一切都是值得的。\n果然，表演的精彩大大超出了我们的预期。@TODO豆哥浓墨重彩描述 出来又回到吴王宫观看一个盛大的表演刘备招亲。三国演义中周瑜给孙权献计假传把孙权妹妹孙尚香嫁给刘备骗刘备来到东吴。场面确实很华丽壮观，豆哥却完全没有兴趣，嘀咕繁琐无聊。小哥还是年轻啊！安抚着既然来了还是看完。\n表演不好好看，小哥却给我总结他整个三国片场的感想：三国演义中虽然蜀国戏份最多但是硬件实力最弱，你看整个三国城没有蜀国一个特有的拍摄景点，估计就是室内或者战场上一般的情节 但是东吴和曹魏就不同了，各种营寨、宫殿、点将台等设施。这个有意思的特征在刚才桃园三结义后面那组三国人物雕塑群上都暗示了：曹操领着一群大将骑着马，东吴孙权和他的几个大将站在家个战车上指挥战斗，而蜀国这边，刘备领着诸葛亮关羽张飞和零零碎碎几个士兵站在哪里。这不正说明了东吴有车，曹魏有马，刘备啥都没有吗？ 有图有真相，小哥分析的对啊。\n至此三国城就完整游览了一遍。豆哥领着我又通过园区陆地杀回水浒城，目的地是水浒城的一个武松斗杀西门庆的表演。第一次现场看到这样人被威亚吊着飞来飞去，打打杀杀，豆哥看得很过瘾。\n出来时已经天慢慢黑了，在门口宋朝市集上各吃了一碗小面，就继续水浒城的游览。进到一个县衙里豆哥和一群大的小的男孩对那些刑具表现出了兴趣，林冲的枷锁非都往自己身上招呼，还让大人给拍照，结果都被拒了，有个小朋友还被他奶奶收拾了 。\n再穿过松江家的宅子和几个院子里这个时间里面灯光暗淡，进去瞅一眼就都马上出来了。直奔宋皇宫，据说宋徽宗这个才子皇帝今晚要展现才艺，来一场精彩的表演。\n宋徽宗的Rap开场非常劲爆，华丽的宫殿，秀丽的灯光，配合着徽宗的律动和音乐，因引爆了现场气氛。豆哥和其他大部分游客一样在皇宫台阶下的石板地上席地而坐，挥舞着荧光棒，很享受这个热闹的气氛。\n皇宫节目挺丰富的，都很精彩。暂告一段落时宫里公公宣布等下这边结束了徽宗还要到市集上与民同乐。\n豆哥虽然还想接着看，看着已经快八点了，想到今晚落脚点不在无锡，而是在常州，决定还是抓紧赶路。\n出门打车到停车地方，导航常州恐龙园隔壁酒店，一脚油门朝北狂奔。无锡上高速前的高架隧道都很畅通，加上高速六十公里很快就赶到常州，还没下高速恐龙的各种信息就扑面而来。豆哥发现这里从高速到市区的很多路牌上都是恐龙相关的园区设施的指引。下了高速跟着导航很快就到了酒店。\n第一次住这种没有前台的酒店，俩人都有点不习惯。车子开到地下车库找车位费了点劲，最后找到了一个划为人防干厕所的位置停下来，捎带给豆哥讲了半天车库和人防的关系。还不容易从车库电梯到地面，找到酒店入口，上楼到房间所在楼层，手机蓝牙开门终于进到房间。\n看着房间落地窗下正对着的恐龙园的五彩的灯光，豆哥对这个位置很满意。洗洗睡了，攒足精神明早早起杀向恐龙园。\n","link":"https://idouba.com/doudou-and-douba-drive-around-taihu-day2-sanguo-city/","section":"posts","tags":["随笔","豆哥","旅游","太湖"],"title":"豆哥国庆环太湖Day2：三国城（宜兴-无锡-常州）"},{"body":" 帮豆哥流水账地记录下国庆五天环太湖的经历。这是第一天。\n路线： 游记： 因为一些状况，临到放假前一天才定了行程，没法和之前一样跑到很远的一个目标地，爷俩就开着车周边闲逛，不涉及机票高铁票，也不提前订酒店。国庆当天各自收拾了一个小箱子，一个背包。豆哥除了给自己的卡拉羊小箱子里塞了五/条内裤五双袜子外，还带了不少书和作业。\n出发时已经十点了，导航设置先到达湖州的新港口灯塔，然后开始环湖。\n出门两三公里就堵在了复兴大桥入口，上了高速只是偶尔小堵一阵子，因为车多或者高速上追尾事故。\n下午两点钟到达新港口灯塔附近。掉了两次头才把车停在一个桥下，踩着泥地冲到灯塔下。湖边风很大，俩人都穿上外套还是有点冷。终于见到了心心念的太湖，停车的第一站，俩人都很兴奋。迎着风冲到灯塔下。\n习惯了西湖那种城里的一滩水，第一次见到太湖这种看不到边际的湖水，豆哥感叹，这哪是湖啊，明明就是海啊。湖里浪伴着大风拍打着岸边的水草，和西湖那种安静完全是不同的体验。\n在灯塔短暂停留后，豆哥在一块石头上刮干净脚上的泥就窜上了车，继续环湖前行。咕咕叫的肚子提醒得找个地方吃点东西。\n没走多远，就被路右边湖里一个圆形的建筑吸引了。靠过去靠过去，豆哥指挥着，完全不管规划的途径点还有一公里。绕了一圈才找到入口，原来就是传说中的月亮湾。人挺多，简单转了转就赶紧找地方吃饭。步行导航到一个太湖渔村的饭店，门口服务员通知说中饭已经结束了，需要在等半小时厨师吃好饭才能开始晚饭。豆哥饿得半分钟都不想等，拉着就往前冲，准备找前面大食堂先来碗面。幸运地半路上碰到了一家还在营业的餐厅，俩人点了个188的套餐，鱼、虾、蛋、蟹、鸡、肉、豆腐、青菜一大桌。有些吃的惯，有些不太惯，但吃的很饱。饭店紧挨着湖水，打算紧挨着湖水走回去找车，发现因为风浪太大，临近湖边的木板走廊都被拦起来了。\n找到车继续往前开，跳过几个途径点直接到达太湖观景台。其实就是西边太湖边一个小平台，不是想想中的一个大高台。站在湖边的大石头上迎着风拍照还是很拉风，不知道那一波浪就能排到脚后跟上。豆哥最拉风的是这里的一个小插曲。这边属于比较荒的小景点，没有公共卫生间，小哥急了就拉我去公路对面小树林解决，茂密的树林下是一个大河沟，担心滑下去，前进到差不多不影响马路上市容，正准备行动。突然正前方，不对准确说是正前下方传来一阵紧急的咳咳声。这是含蓄的信号在警告我们这个不文明行为啊。“stop，小心，你们下游前面深沟里有人！我知道你们要干什么，因为我正在以比你们更低的姿态做类似的事情。”爷俩会意赶紧撤离现场。\n回到车上正准备挪车走，一个穿着礼服的年轻人示意我们能不能等等再开走。原来车侧面拍婚纱照的新娘正在换道具。咱家稍微高点的车居然无意中帮了人家这么大一个忙。\n接着往前开这段一直紧贴着湖边，豆哥看的惊叹得都词穷了。湖边道路对环湖骑行非常友好，不但有紧挨着湖边的骑行道，每隔一段还有个骑行休息区。\n湖边骑行的人很多，估计都是借着这个假期在地图上在自己的心里画一个完整的圈，有一群群很专业的公路车速度很快地竞速，也有一个个山地车托着行李缓慢前行。\n问豆哥咱明年也这样来一圈，装作入神地看风景，没有回应。\n就这样贴着湖开了好长一段，开得很惬意，很放松，开车的人，坐车的人，还有车子，都很省油。傍晚了，湖边村子里的袅袅青烟升起，风好像也小了，湖里的浪也小了。车外面安静了，车里面豆哥经历了刚才无边湖景的兴奋，也安静下来了。\n直到一个导航提示左转的路口。“导航怎么提示左转呢，分明前面这个变窄的公路还可以往前开，还可以离湖边更近，我还没看够呢”。好，那咱就继续直行。\n“怎么开了这么久，前面一辆车没有，后面也一辆车没有，连骑行的也没有了，咱只碰到过一群本地老太太傍晚散步的，爸，还往前不，天快黑了，有点慌啊”，豆哥提醒我。“没事，前面总有个左拐的地方开到大路上去”。\n确实在尽头有个左拐的涵洞穿过这段临湖的高速公里。涵洞很窄，左边高一点应该是非机动车和行人，右边是机动车，里面好像有一点积水。老司机谢谢咱这虽然是城市越野，也是高底盘的越野，正要往里开。“咱家车是不是太高了啊，我看水面里这个顶好近啊，咱家车钻不过去吧”副驾驶豆哥提醒。对啊，下车检查下看。不看不知道一看吓一跳，这水面到洞子顶太近可不怪涵洞挖的矮，只是因为水太深了。刚才是准备要重进一个深水池啊。\n想想这前不着村后不着店的地方，天已经慢慢黑下来了，刚才如果一脚油冲进去…，想想都后怕，掉头沿原路赶紧往回跑。\n和刚才开过来一样，路上没有车。\n和刚才开过来一样，湖水还很美。\n导航经过的下一个途径点是香山公路驿站，我猜是跟刚才几个一样的小景点，豆哥说根据名字判断他觉得就是给那些骑车人休息的地方。\n豆哥蒙对了。比前面经过的那些休息点大一些，但就是个休息点。天已经慢慢黑了，抓紧赶路，也就没有下车。在车上对着这个1314号驿站随手拍了张照片，照片的C位是副驾驶位置豆妈给车子装饰的车载人。给豆哥说，一生一世，这个照片送给妈妈，虽然这次旅行她没来。豆哥没有像平时一样表达不屑或硬抬杠，好像嗯了一声好像没出声。可能是累了，我猜想是想妈妈了。\n到酒店的规划的最后一个途径点是蜀山古南街，越开路越窄，人越多。天彻底黑了，跟着导航行驶在黑乎乎的小镇上，会车都要小心点。豆哥说饿了要么直接去酒店，安顿好再来。行，修改目的地，不到十分钟就到酒店了。\n这趟出行前，提前和豆哥沟通过了，咋俩大老爷们，不用想你小时候一样总是订很好的度假酒店，有个地方睡觉就可以了。预期管理的不错，房卡刷开门，豆哥很满意。\n看大众点评酒店对面人民饭店的韭菜鸡蛋饺子不错，就过马路冲进去。饭店里晚上有宴会刚结束，三三两两酒足饭饱的城里人正在饭店寒暄。应该是县城顶级的饭店，很亮堂，服务员说饺子不单卖，是一大桌招待的那种俩人出来边找别的饭店边给豆哥讲“之前在人民饭店上班的都是工人，我舅舅你舅爷就在我们县城人民饭店上班”。\n在人民饭店正对面找到了另外家饺子店。饺子很大，大胃王的爷俩居然都没吃完。豆哥说他们是不是上错了，十二个咋这么大。我说你审题仔细一点，人家是十二块钱十五个，不是十五块钱十二个。\n吃饱回酒店豆哥洗完澡换上睡衣给妈妈视频，介绍自己今天精彩的旅程，特别是观景台那段精彩的经历。然后掏出课本做作业，计划两个小时，结果不到一个小时就困得直打哈欠。\n睡吧睡吧。\n“爸爸，前面是个铁索桥”晚上梦话一大堆，就记住了这句，估计还在继续他的精彩旅行。\n小县城这个小酒店睡的很安静舒服，下午那会儿还想临时改成湖州太湖旅游度假区那边酒店，感谢豆哥的坚持，让今天的旅行如此精彩。我们明天继续。今天的地图如下，明天我们继续环起来。\n","link":"https://idouba.com/doudou-and-douba-drive-around-taihu-day1-first-touch-taihu/","section":"posts","tags":["随笔","豆哥","旅游","太湖"],"title":"豆哥国庆环太湖Day1：遇太湖（杭州-湖州-宜兴）"},{"body":"","link":"https://idouba.com/tags/%E5%86%9B%E8%AE%AD/","section":"tags","tags":null,"title":"军训"},{"body":" 豆哥，你哪天军训回来激动地给爸爸讲的四天训练营里各种有意思的事，真的很精彩。说到兴奋处你在咱家小客厅里给老爸一个人来了一段队列汇演。老爸生平第一次像个老首长一样地坐着检阅队伍，那个感觉还是相当不错滴。你讲了很多，包括你说开营第一天教官喊着对你们说话，当他凑近到你脸跟前给你说话时你都吓了一跳。后来慢慢就适应了，反倒是返校后排队时王老师温柔的指令声有点听不清楚。\n但你老爸讲的所有这些中，印象最深刻的是你说自己训练第一天的故事。你说第一天队列练习时，第一次这样比较长时间的站立，你感觉自己有点站不住了，硬撑了一会儿还是有点不行，就给教官打报告出列在边上坐下来休息了一会儿，然后再申请入列参加训练，一直按照要求完成了当天和后面几天的训练，从中获得了不少乐趣。\n因为是你自己亲身经历，你描述的生动，又详细。对着老师发的你穿迷彩服的照片，我脑子里能形象地回放出我儿子当时的表情、身体动作、甚至是细节的面部表情和心路历程。 这几天老爸脑海里总是反复回放着这段生动的电影画面。作为父亲，我的第一感觉是心疼。从你小时候开始，爸爸希望你经历的总是顺利、快乐、轻松。幼儿园或者小学刚开始任何的小的事件都会让我紧张。但随着你长大，老爸也成长了，甚至有些从容淡定了。老爸能平静接受发生在你身上一些不顺利、不理想，挫折、失败和眼泪。这些都是你成长过程中自然而然应该经历的。老爸静静地观察着你经历这些的反应，关注着这些挫折困难带给你成长的积极价值。\n就像这次一样，你给我描述了当时的情况：“我有点坚持不住了，教官允许我们几个坐在旁边休息下。我休息了会儿，感觉好些了。虽然我觉得教官应该允许我再多休息一会儿，我自己也觉得到再多歇会儿真的很舒服。但我担心一直这样舒服着，会跟不上团队其他人的训练，后面可能就都跟不上了，我就掉队了。所以我就给教官报告归队了。”\n老爸很欣慰，甚至有点自豪，你对待这个困难的态度和做出的反应。你发现没，碰到这些自己不适应的情况、需要克服的困难，只要你有意愿，有勇气去挑战，不管最终结果怎么样，一定比退却或者停留在原地要强的多。你那一刻坚持了，才能在当天挑战自己，在后面几天应急并战胜更大的挑战，回来信心满满、收获满满地给老爸讲你们训练营中各种趣事。试想你当时一念之差，想多坐一会儿，再坐一会儿，没有坚持，甚至当了逃兵。老爸接你回来，会是一个什么场景。即使老爸想出好办法来安慰你，也不能弥补挫败的感觉。\n在我们的生活中，在你的成长过程中，这种时刻其实挺多的，我们鼓足勇气，过去了，除了本身学到东西，获得成长外，成就感，满足感是什么也换不来的。如老妈给我说你第一次学游泳的故事。老爸遗传给你的旱鸭子基因导致你小时候洗澡时眼睛碰一点水就大喊大叫，听说第一次到泳池你紧张哭着不敢下水，回来后还打退堂鼓不想去上课了。你后来怎么鼓足勇气跳到泳池里，你没有像这次一样能讲这么明白，但后来你成为了你们那届蛙泳新学员游得最好的。我记得你们结业考我是去了现场。你撅着自豪的屁股，作为表现最好的学员最后一个屁颠屁颠地走到泳池起点，跳下水英姿飒爽地完成了你的结业考。然后就一发不可收拾，到现在我再泳池里跟你一起游，连你的屁股都跟不上了，只能跟上脚丫子了。\n更鲜活的经历是咱家最近的情况。妈妈照顾姥爷，我们不得不和妈妈分开这么久。爸爸能感受到你有多么伤心孤单，但每天我们还是按时上班上学，回家一起吃完饭后，爸爸去加班时，在家里按照之前妈妈培养的好习惯自己完成作业，做当天的复习和预习，你说作业质量还提高了呢。周末我们俩大清早起床赶高铁窜到济南，见到了妈妈。匆匆和妈妈待了两个半天，第二天中饭后吃了老济南的鲁菜我们就又得返杭了。虽然有很多不舍，在出租车上你还是安慰自己说分开有一点点难过，鼻子有一点点酸，但是昨天哭过了，今天不是很难受。妈妈继续辛苦地留在济南，我们坐着高铁返杭继续我们的工作学习，然后等着妈妈回来。\n从你的身上老爸看到了长大的男子汉身上的力量、勇气和希望。\n勇气，能指引我们去做自己认为对的有价值的事情。除了这些让自己感觉极端不适的困难外，也体现在我们平时生活中。养成习惯那就是勇敢，就是自律。就像今天晚上老爸重要把充电器从公司带回来了，吃完晚饭你给你的Pad充上电，来了把你心心念的蛋仔游戏。我能看到你真的很喜欢，明天就是三天的假期，可以放松多玩会儿。你还是在你自己规划的40分钟（额定30分钟加上周补10分钟）后合上了Pad。老爸发现你有很强的自律能力，看电视、Pad、电话手表等这些轻松喜欢的事情，喜欢搞搞，但会控制。周三晚上咱俩给你换新手表，吃完晚饭没有刷碗就折腾起来了，为了保证旧手表的数据能完整导入到新手表，我们查说明验证操作多花了些时间，看到八点多了还没有搞好，你觉得到了作业时间了，在这个上面花的时间超了不让我搞了。幸亏后面五分钟内解决问题，才没有耽误到你的计划。’\n自律真的是非常了不起的品质，如果能养成习惯，会让我们一生受益匪浅。想着多少大人打游戏或者刷着短视频一直停不下来就能理解做到是多么不容易，爸爸直播吧一打开就停不下来，五大联赛赛况看完了，周边各种各种无聊新闻刷不停，像你学习。爸妈不会强迫你苦行僧一样的只许做这个不许做那个。希望你能按照自己的方式管理好哪些做哪些不做，哪些啥时候做，做多少，做到什么程度。一直这样坚持下去，养成习惯。时间会告诉我们不同。\n又啰嗦了。在隔壁房间啪啪的敲键盘声音是不是吵到你了。给你搞了杯热水才打发回床上。我也睡了，明早跑步不叫你了，开学确实没有睡过一次懒觉。\n","link":"https://idouba.com/doudou-military-training/","section":"posts","tags":["随笔","豆哥","军训"],"title":"豆哥军训归来"},{"body":" 很奇特的一段旅途。早上和儿子拼车，八点一个去学校参加开学的军训，另一个继续往机场赶十点的飞机。 老妈不在家这周，咱俩过得凑合没饿着。\n每天，你早晚上课，中午下午在家。有时有点忙，大多数时候监控里的身影有点无聊寂寞。我在单位一如既往地忙，好几次两路会议同时进行时，还再加入你的手表来电。\n早上，你撅着屁股还在睡，我冲下楼快速扫荡中饭和晚饭的菜。回来叫醒你，你洗漱，我也洗漱。你端盘子我拿粥碗，一起吃完早餐。中间经常你要紧急窜洗手间，搞得我们时间更赶。八点二十开车送你去上课，八点四十我到单位上班。\n中午，你在家洗好择好菜，你知道当把电饭煲那个柴火饭按钮按下后一个小时，老爸一脚油就冲回来。厨房一顿瞎操练，咱中午饭就有了。你说老爸现在脱离小红书也能给你做一顿管饱的午餐，我说我得有时间看。\n傍晚，重复中午的流程。换了菜单，流程不变，时间更短。你六点半有课，我七点有活要干。\n每顿，我快速洗刷油多的几个锅和盘，马上赶回去上班，给你留下几个油不多的碗。 第一天中午回来看着你在厨房刷碗的背影，想着大家庭最近发生的事情，你老爹鼻子一酸。\n感动妈妈的辛苦，这么多年边上班，每天回家把你喂养这么大，特别是每个这样的暑假。 除了三顿饭，同时还要辅导功课，理论上咱俩还没正式开展。\n一周里，还没有找到时间坐下来对着书看，但我们在车上，饭桌上七拉八扯从来没有停止过。\n那天，你从哪里看到青海湖绕一圈有四百公里，回想起咱之前骑行青山湖四十公里。我们讨论起前者面积大概是几个后者。你路上瞎蒙，回家在验算本上画画，发现假设形状是你之前学过的正方形，还是你正在预习下学期要学的圆形，结论居然差不多都是100。同时贯通了为什么占老师告诉你们长度单位加一个零，面积单位要加两个零。\n周日，我们在西湖边和老爸当年修铁路的同事和他家哥哥一起吃了饭。两个大人聊到东北工地的很多往事。老爸捡到同事的一本英语词汇书，在工地闲着没事拿铁一局的机械材料账本抄了一些遍，回到西安，没有上过高中大学的背景就考过四级六级。然后自学考了个研究生，认识了妈妈，有了你。你估计只听故事一样的感兴趣那个叔叔讲的当时怎么帮爸爸从桥梁处转出档案才赶上西工大研究生报名，虽然不太相信英语那段，但前天你也尝试背下xdf整本读本，体验笨方法的乐趣和不同。\n昨晚，你在自己房间对着王老师发的表格整理自己的衣服袜子牙膏牙刷洗发水洗衣皂，整齐排放在自己小箱子里。在隔壁房间，你老爸整理下出差材料，合上笔记本塞到包里，随手塞上剃须刀和两件衣服，准备临时安排的短差。\n现在，你在学校应该见到了这几天一直心心念的老师同学，大巴车浩浩荡荡开赴未来四天的军营。老爸在南航的南航飞机上，想你和妈妈，飞行模式下写点文字，居然话多情谊浓地从杭州啰嗦到深圳上空。\n飞机开始降落了，下飞机就要去赶场到地方，不得不收起啰嗦。\n祝你军营生活顺利充实有收获。祝你按照自己习惯的喜欢的方式快乐成长。 祝福咱的亲人们都健康。所有人都辛苦了，所有人都还在辛苦着。老爸不时漫游在几个角色间，感受其中的压力、恐惧和辛苦。你也感受到了一些，不用担心慢慢都会好起来。\n感谢你这几天的陪伴，有时我分不清是我在照顾你，还是你在照顾我。 谢谢你每天需要我。一起做事，一起扯淡，一起吃饱饭。谢谢你让我更充分参与到你的生活，除了之前教条概念的角色外，体验到有情感的父亲感觉。\n最感谢学校和老师们。刚才在学校门口专门走上前给张校长打了招呼。 终于开学了，特别是四天在外面独立军训，可算歇口气了。终于不用每天早上睁开眼第一件事想着今天三顿饭吃什么，买啥菜。真真快顶不住了。。\n","link":"https://idouba.com/doudou-first-day-of-new-term/","section":"posts","tags":["随笔","豆哥","军训"],"title":"开学第一天和豆哥奇妙的拼车之旅"},{"body":"","link":"https://idouba.com/tags/istio/","section":"tags","tags":null,"title":"Istio"},{"body":"","link":"https://idouba.com/categories/istio/","section":"categories","tags":null,"title":"Istio"},{"body":"","link":"https://idouba.com/tags/karmada/","section":"tags","tags":null,"title":"Karmada"},{"body":"","link":"https://idouba.com/categories/karmada/","section":"categories","tags":null,"title":"Karmada"},{"body":"","link":"https://idouba.com/tags/kubecon/","section":"tags","tags":null,"title":"KubeCon"},{"body":"","link":"https://idouba.com/categories/kubecon/","section":"categories","tags":null,"title":"KubeCon"},{"body":" 记录在2024年8月21日在香港Kubecon上发表的技术演讲《Best Practice: Karmada \u0026amp; Istio Improve Workload \u0026amp; Traffic Resilience of Production Distributed Cloud》\n议题： The Distributed cloud offers better resilience by providing redundancy, scalability and flexibility, especially for cloud native applications. However the complexity of multi-cluster workload and traffic management in hybrid or multi-cloud environment brings huge challenges in practice, such as the number of overall multi-cluster workload instances serve for customer request decreased when some unhealthy ones isolated in case of failures. In this speech, Chaomeng introduces a production practice of Karmada and Istio work together to promote resilience of multi-cluster application. How Karmada and Istio policies configured in a centralized control plane controls both replica and traffic distribution across cluster automatically. In case of failures, how Istio’s failover acts to remove unhealthy endpoints from global load balancing pool, and how Karmada rebuild the according number of instance in other healthy clusters, ensure multi-cluster instances always meet the capacity design.\n分布式云通过提供冗余、可伸缩性和灵活性，特别是对于云原生应用程序，提供了更好的弹性。然而，在混合或多云环境中的多集群工作负载和流量管理的复杂性在实践中带来了巨大挑战，例如当一些不健康的实例在故障情况下被隔离时，为客户请求提供服务的整体多集群工作负载实例数量减少。 在这次演讲中，Chaomeng介绍了Karmada和Istio共同推动多集群应用程序弹性的生产实践。Karmada和Istio策略如何在集中控制平面中配置，自动控制跨集群的副本和流量分发。在发生故障时，Istio的故障转移如何从全局负载均衡池中移除不健康的端点，以及Karmada如何在其他健康集群中重新构建相应数量的实例，确保多集群实例始终满足容量设计。\n正文： 大家好，我是张超盟，来自华为云 ，我今天带来的是一个有关服务韧性的话题。将介绍在分布式云场景下，Karmada和Istio相互配合，管理多K8s集群的负载和流量，改善服务韧性的实践。\n我是华为云分布式云原生的架构师，在过去的近十年里在华为云从事云原生相关的设计开发工作，包括过去几年里一直负责华为云应用服务网格产品。\n演讲的内容包括：韧性的背景，K8s和Istio作为云原生领域的基座技术，能力很丰富也很强大，我们从韧性角度简单审视下相关能力。然后介绍分布式云如何改善单云的韧性，又引入了哪些新的挑战。 重点是实践的内容，介绍在分布式云环境下：Karmada如何提高多集群的负载韧性，Istio如何提高多集群的流量韧性；以及Karmada和Istio相互配合提供完整的多集群应用韧性的最佳实践。\n简单讲，韧性描述了这样一种能力，系统在过载、故障或在遭受攻击的时候还能够完成基本功能。韧性告诉我们，①虽然我们不想要失败，但是我们得承认失败总是会发生。因而我们需要为失败而设计系统，减少故障对系统的影响。有个著名的说法，韧性不能保证你多挣到钱，但是可以保证你少赔钱。竞争力可能决定产品的上线，韧性才能保证产品的下线。韧性应用于工程世界的所有系统。计算机世界里韧性是系统设计需要考虑的关键因素。\n下面简单看下K8s和Istio提供的韧性能力。K8s大家都非常熟悉，K8s提供了Deployment，Replica Set和Service三个核心对象。 Deployment和Replica Set声明式控制负载实例的副本数和配置。 Service让为每个服务器提供了统一的访问入口，自动在多个实例间负载均衡。k8s基于这三种关键机制实现了应用部署、升级、访问的自动化。较之传统虚拟机方式，除了带来了轻量、敏捷、弹性的特点外，同时也提供了丰富的平台能力，提高应用的韧性。\n我们尝试通过韧性角度认识下这些我们熟悉的能力。首先K8s自动控制负载实例数，通过多实例提供冗余容错能力，提高可用性。特别是提供了节点、AZ的反亲和部署，保证局部资源故障时服务总体仍然可用。另外滚动升级，交替创建新Pod、停止老Pod。通过平滑升级减少了升级的停机时间。水平扩缩容 HPA快速自动弹性扩缩容实例，避免了业务量大资源不足导致的系统过载。Liveness和Readiness的健康检查，实现应用故障自动检测和自愈。\n此外k8s还提供了其他能力，间接改善韧性。如： 提供RBAC，保护应用和数据的安全。内置的日志、事件和监控，通过平台方式提供了应用运维和Troubleshooting的关键能力。ConfigMap和Secret，方便用户把配置从代码中独立处理，避免了重新部署带来的变更风险。CICD，对接流水线自动化提高上线变更效率，也减小了人工风险。\n可以看到大量我们平时用到并且非常熟悉的k8s能力，都是基于韧性目标设计的。\nIstio的机制大家也比较熟悉，通过透明代理拦截流量，代替应用执行流量动作，从而以非侵入方式提供了七层的流量能力。.Istio提供的能力非常丰富，这里我们也同样从韧性的视角审视Istio提供的众多能力。可能会发现原来我们经常用到的Istio能力很多都和韧性相关。\n我们都说Istio在k8s基础设能力之上，提供了面向应用的上层能力增强，这种增强的配合关系同样适用于韧性方面。Istio提供的不只是四层负载均衡，而是基于七层的流量提供了更多的能力。包括：访问亲和性、故障倒换等能力。通过自动重试提高访问成功率；通过限流防止系统过载。基于七层流量特征的灰度分流策略，在不同版本间分配流量，降低版本升级引起的风险。不同于k8s的的Readiness，Istio提供了基于熔断器的故障隔离和故障恢复能力。 另外非侵入的调用链、访问日志，跟踪服务间调用细节，方便故障定位定界。通过非侵入故障注入，提前发现产品缺陷。可以看到，Istio以非侵入方式提供了大量面向应用的韧性。\n如前面总结Kubernetes提供了负载多实例，并支持基于节点、AZ的反亲和部署提高应用韧性。但这些能力仅局限于一个Kubernetes集群内部，不能在更大范围提供应用的韧性。这样对于Kubernetes集群自身的故障无能为力。当客户业务都集中在一个集群时，集群异常引发了全局的业务断服宕机。生产中这种事故频繁发生在集群升级时。\n这种现象的根本原因是故障半径的问题。就像把所有的鸡蛋放在一个篮子里，一旦篮子有问题，没有一个鸡蛋能幸存下来。解决这类问题直观的思路就是减小故障半径，把鸡蛋分开放到多个篮子里。\n有一种分布式云的架构可以在一定程度上解决这个问题。\n分布式云是一种基础设施架构；可以在多个物理位置，包括公有云自己的数据中心、其他云提供商的数据中心、用户本地或者第三方数据中心、边缘，运行公有云的基础设施。并且从单个控制平面统一管理这些云资源。\n对于云原生场景的分布式云，我们称为分布式云原生。华为云分布式云原生服务UCS，将云原生基础设施分发到各种物理位置，使得用户可以在业务期望的任意位置运行云原生应用，并且通过公有云上集中的云原生控制面统一管理。\n可以看到，较之单云架构，分布式云提供的优势包括：\n分布式部署的数据和应用可以更接近用户，使得响应时间更短。Less latency, closer to end users. 数据和应用可以限定在规定的范围内，更容易满足合规性要求。Increased regulatory compliance 可以结合分布式的资源快速构建业务，扩展性更强 Better scalability 此外还可以通过统一的控制台，监控运维分布式环境部署的应用。Improved visibility 当然我们关注的韧性改善也包括在内。天然分布式环境部署，提供了冗余和容错，一个地域或者某个云环境故障，其他环境的可以故障倒换，接管业务。\n当然,分布式云也引入了众多挑战：\n复杂性(Complexity)：管理地理上可能跨越多个云提供商和本地数据中心分散的云资源，会带来新的复杂性。 安全性(Security)：在分布式环境中，保护数据和应用程序安全会更加困难。 异质性(Heterogeneity)：分布式云环境通常涉及不同硬件、软件、操作系统和云提供商的服务。 延时(Latency and Network Performance)分布式云在某些情况下有助于减少延迟，但如果使用不当，会引入新的网络延时 在云原生场景下，k8s本身定义了标准统一的接口，一定程度简化了其中复杂性和异构资源问题。.但是如何将分布式在不同物理位置，不同的k8s管理起来，并且提供和单个k8s集群类似的体验，还是有很大的挑战。Karmada可能是一个答案。\n简单介绍下Karmada。Karmada的设计目标，是使开发人员能够像使用单个 Kubernetes 集群一样使用多集群能力，管理跨集群的资源；对用户提供一个可以不断扩展的容器资源池；并通过多集群方式进一步提高云原生应用的韧性。\n这里简单列举了Karmada提供的关键功能。包括：多集群管理、跨集群负载分发、全局资源视图、多集群服务发现等。 我们重点关注两个与今天分享主题密切相关的特征： 一个是Karmada怎样解决前面讲到的分布式云的管理复杂性问题。另外一个是Karmada的分布式云多集群管理，具体怎么实践多集群韧性目标的。\n首先第一点：Karmada提供了集中式的控制面；这与分布式云的模型定义也完全一致。 用户无需连接甚至感知分布式环境部署了多少个集群。只需要通过集中的入口即可。\n更重要的是，Karmada使用和单集群完全兼容的API管理多集群资源，这和之前的KubeFed相关API有极大的不同。Karmada的资源模板是原始的Kubernetes单集群的资源模板，配套多集群分发策略Propagation Policy的就可以变成多集群对象。用户在多集群的管理入口上，配置单集群完全兼容的资源对象，Karmada控制器根据分发策略会自动控制在多个集群上资源创建、更新对应资源。\n对用户来说访问多集群的Kube-apiserver就跟单集群一样。极大的简化了用户的管理分布式环境下多个集群资源的复杂性。\n另外一点，多集群韧性。从前面分布式云的概念模型分析了解到，分布式云通过分布式环境的冗余和容错提供了更高的服务韧性。Karmada在韧性这个方向上提供了更丰富的能力。首先当某个节点发生故障，导致该节点上的负载实例不可用。除了和单集群一样在当前集群内重建对应负载实例，Karmada可以根据多集群应用迁移策略，在其他集群中重建出对应的负载实例。如这里当Cluster1的Node11故障时，并且Cluster1中没有可用的节点资源运行这些Pod时，Karmada可以控制在Cluster2上自动创建这些Pod。当负载迁移模式配置为优雅模式(Graciously)时，Karmada会等待负载在新集群上恢复健康，或者在一定时间后才驱逐原集群上的应用。使得故障迁移过程平滑且优雅。\n同样的，对于AZ级别的可用性保证，不仅可以应用单集群的AZ间反亲和部署。也可以应用Karmada的多集群管理，将故障AZ的节点上的实例自动迁移到本集群或其他集群的节点上。如这里AZ1整体故障后，Karmada的负载迁移策略可以自动将负载迁移到另外一个集群的AZ2的节点上。多集群的AZ高可用应用可以是每个集群多个AZ，或固定每个集群一个AZ。在实践中，我们的客户应用Karmda做多集群管理时，为了管理简单，一般每个集群会对应一个AZ。\n以上两种节点级和AZ级故障的可用性是单集群也具备的能力，应对整个集群故障才是Karmada的差异化优势。当检测到某个集群故障时，集群上的负载Pod将会被驱逐。基于副本调度策略，Karmada动态地将负载实例从故障集群迁移到另外一个可用集群。具体是集群判定为不健康时，自动添加污点。当检测到故障群集群不在被副本分发策略容忍时**，**该集群将从可调度资源中删除，随后Karmada调度器将重新调度相关负载，即分发到其他集群中。\n除了集群自动的健康判定外，Karmada也支持用户根据自身对业务的需求管理集群。如当用户不希望在某个集群上继续运行工作负载时，可以添加上污点标记集群为不可用。将一个集群的负载整体隔离掉，快速高效地进行故障隔离，最大限度保证业务总体可用性。\n下面关注下Istio韧性能力在多集群中的应用。前面介绍过istio提供了熔断、限流、重试、超时、故障注入等韧性能力。在多集群视图下，和单集群的差别是Istio管理的服务的实例跨集群。服务逻辑视图并没有改变，流量策略的作用对象仍然是策略描述的那个服务，因此内容也没有大的改变，能力也没有差别。非要说差别只是场景有所侧重。\n最典型的侧重场景是跨集群的流量管理。当目标服务的服务实例跨了不同的集群，可以通过Istio控制到不同集群的服务实例上的流量。.默认在不特殊配置时，会执行全局负载均1最典型的侧重场景是跨集群的流量管理。当目标服务的服务实例跨了不同的集群，可以通过Istio控制到不同集群的服务实例上的流量。\n默认在不特殊配置时，会执行全局负载均衡。即当访问目标服务时，网格数据面会执行全局的负载均衡策略，在跨集群的后端实例间均衡地分发流量；这里的流量来源可以是来自Gateway的南北向流量，也可以是服务间Sidecar控制的东西向流量。\n在Istio管理分布式应用的场景中，更典型的是在全局的负载均衡的基础上，基于服务实例位置进行亲和性访问和故障倒换。优先访问与源服务亲和的目标服务实例，如优先访问本集群内的服务实例。这也是生产中最常应用的方式，流量在每个集群内闭环，保证了服务间的访问效率。当本集群实例不可用时，会自动倒换流量到其他集群的服务实例上。Cluster2的Pod22不可用时，Istio会控制自动到换到Cluster1的Pod21实例上。\n在Istio管理分布式应用的场景中，更典型的是在全局的负载均衡的基础上，基于服务实例位置进行亲和性访问和故障倒换。优先访问与源服务亲和的目标服务实例，如优先访问本集群内的服务实例。这也是生产中最常应用的方式，流量在每个集群内闭环，保证了服务间的访问效率。当本集群实例不可用时，会自动倒换流量到其他集群的服务实例上。Cluster2的Pod22不可用时，Istio会控制自动到换到Cluster1的Pod21实例上。\n可以看到Isito和Karmada分别提供了丰富强大的功能保证多集群上运行的服务的韧性。二者配合在一些典型的场景下能提供更全面、更贴近用户使用要求的韧性能力。如前面提到的k8s集群升级问题，可以通过Karmada和Istio配合进行集群灰度升级。\n首先，可以选择一个待升级的集群作为灰度环境。配置Istio流量策略将全部流量切换到另一个集群；配置Karmada资源分发策略讲集群资源迁移到另外一个集群。对灰度集群执行升级，并观察各个组件的正常运行正常。修改Karmada资源分发策略将部分负载迁移到灰度集群，观察负载在灰度环境上正常运行。配置Istio流量策略将少量流量切换至灰度集群，从最终业务视角观察服务被访问后正常响应。 完成一个灰度集群的升级。\n当灰度集群业务工作正常后，将另外一个集群当成灰度集群，重复前面的过程。即基于Karmada跨集群负载迁移策略将负载迁移到对端集群。基于Istio流量策略将流量都切分到另外一个集群。直至完成两个集群升级。\n在两个集群都升级完成后，更新Karmada的资源分发策略，恢复在原有的两个集群上部署资源。通过Istio流量策略控制在集群的服务实例上进行全局负载均衡。在升级的整个过程中，不管灰度集群的升级过程怎么用，用户总是可以访问到可用的目标服务后端，甚至在升级过程中提供服务的后端实例个数也没有减少。如果出现集群升级问题，只需立即修复灰度集群，而不会影响最终用户的业务。通过这种集群灰度升级的方式，确保升级过程中出现的集群问题问题不会对用户业务造成影响，从而保证了业务总体韧性。\n除了灰度这种主动控制流量和负载的在集群间分发的场景外，在故障过程中，Istio和Karmada相互配合，提供更优的跨集群应用韧性。 如前面介绍的Istio多集群服务熔断的场景。当某个集群故障时，Istio熔断策略可以自动隔离该集群上服务的后端，将流量分发到其他集群上。\n这种方式虽然可以隔离故障实例，提高服务访问成功率。但是当部分认定为故障的实例被隔离后，实际提供服务的实例数就会比规划的实例数少。特别是当故障集群的实例数较多时，如本来有7个实例，集群1和集群2分别部署了3个和4个实例，当集群2故障时，4个实例被隔离，只有3个实例供服务。当客户端还是以原有负荷访问时，很容易导致剩下的实例过载。Isito的熔断策略中有一个配置，当判定故障的实例数超过一定比例，会放弃执行隔离动作，在这个例子里就是4个不健康的实例加3个健康的实例也好过只有3个健康的实例。\n很容易发现这种策略背后有一种万不得已的异味，以牺牲服务访问质量为代价的方案很多时候也并不是用户想要的。有没有办法能兼顾呢？\n在多集群场景下，配合Karmada的跨集群负载迁移策略，可以提供不一样的能力。。还是在刚才故障场景下，当Cluster2故障时。基于Istio的熔断策略隔离Cluster2上的4个故障实例。同时Karmada的跨集群负载分发策略，可以将Cluster2上四个实例在Cluster1中重新创建，从而保证服务的实例数保持在规划的7个实例的目标。访问服务的流量会在集群1的7个实例上负载均衡。即通过Istio的熔断机制保证满足服务访问成功率的同时，通过Karmada的跨集群负载分发策略保证了服务的容量没有折损，从而全面保证了应用的韧性。\n以上是今天分享的全部内容，但只是我们在生产中解决的部分问题。我就将继续努力，在UCS中以云平台的方式保证应用韧性，帮助用户守护好业务下限，用户可以专注构建竞争力，不断冲击业务上限，从而取得商业成功。\n谢谢大家。\n附： 演讲材料(官方Slides) 视频（国内） 官方Sched主页 ","link":"https://idouba.com/kubecon2024-best-practice-karmada-and-istio-improve-workload-traffic-resilience-of-production-distributed-cloud/","section":"posts","tags":["Kubecon","Istio","Karmada","Kubernetes","演讲","韧性","最佳实践"],"title":"KubeCon2024：Karmda和Istio提高分布式云的负载与流量韧性的最佳实践"},{"body":"","link":"https://idouba.com/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":"","link":"https://idouba.com/categories/kubernetes/","section":"categories","tags":null,"title":"Kubernetes"},{"body":"","link":"https://idouba.com/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","section":"tags","tags":null,"title":"最佳实践"},{"body":"","link":"https://idouba.com/tags/%E6%BC%94%E8%AE%B2/","section":"tags","tags":null,"title":"演讲"},{"body":"","link":"https://idouba.com/categories/%E6%BC%94%E8%AE%B2/","section":"categories","tags":null,"title":"演讲"},{"body":"","link":"https://idouba.com/tags/%E9%9F%A7%E6%80%A7/","section":"tags","tags":null,"title":"韧性"},{"body":" 记录在2023年9月27日在上海Kubecon上发表的技术演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case 》\n议题： 服务网格的访问日志在运维工作中发挥着非常关键的作用。特别是访问日志在HTTP响应码外提供应答标记Response Flags，通过针对性的标记提供有用的额外信息，帮助运维人员提高故障诊断的效率。但是Envoy社区官方文档中对应答标记的介绍非常简单，Istio社区也没有资料介绍这部分内容。在实际使用中当用户遇到包含DC、UF、UH等应答标记的日志时，很难找到权威材料参考来解决具体问题。 在本次演讲中，超盟将重现10多种生产中常碰到的应答标记的实践案例，解析每个标记的含义、产生场景，并介绍如何基于这些应答标记进行故障诊断和问题定界，进而解决案例中这些应答标记表示的问题。此外，还将解析生产用例中访问日志的6个有用的时间字段的含义，并介绍如何基于这些时间字段定界服务网格的延时相关问题。\nAccess logs of service mesh is practically important in ops work. Especially, Response Flags in each log help improve fault diagnosis efficiency by providing additional details of request. But the simple and brief definition of Response Flags in Envoy and Istio community makes it hard to refer to it to effectively find the real problem and root cause when running into logs containing “DC, UF, UH” like flags in practice. In this session, Chaomeng will introduce how Istio Access Log promote fault diagnosis efficiency in micro service Ops practice. He will reproduce 10+ different Response Flags cases of customer’s practice, analyze what each flags indicates and when such logs generated, and demonstrate how to perform fault diagnosis and problem demarcation based on the Flags, and how to solve the problem of each case. Additionally he will parse the meaning of 6 useful time information of access log, and introduce how to figure out the most time-consuming period.\n背景： 访问日志是应用系统运维的重要手段，可以有效地帮助我们进行问题的定位定界。\n在服务网格中，访问日志也是可观测性能力的一块重要内容。不同于指标提供访问的统计信息，访问日志记录了每一次访问的详细信息。不管是作为安全审计，还是做系统运维，访问日志都是最得力的手段。\n访问日志记录了每次访问的时间、请求、应答、耗时、源服务和目标服务等信息。帮助运维人员进行有效的故障定位定界。生产中我们也经常会检索分析一批日志看特点，如是否慢的请求的应答体都比较大，来自某个特定服务的服务接口总出错，或者来自某个特定源服务的访问不正常等，帮助我们发现系统问题。\n对于七层的访问日志一般我们会通过HTTP响应码了解请求的状况，如503、502、404、403等。Envoy在访问日志中引入了应答标记Response Flag，辅助HTTP响应码，进一步描述访问或连接的细节问题。如发生 了503错误后，通过503 UH、 503 UF、 503 UC、 503 NC 等区分各种不同的503产生的原因，提供线索让运维人员针对性地解决问题。\n但是Envoy 和Istio社区的访问日志对于Response Flag的信息非常少，所有的内容也只是如下非常干巴的把组合的单词展开，没有解释清楚每个标记的含义，更没有说明哪种情况下会出现这个标记。身边的同事，还有我们的客户经常在生产中碰到了这些应Response Flag不知道如何处理。有客户的工程师反馈说，看到了Response Code里那几个奇怪UC、UH等字符比看见503还让人抓狂。\nLong name Short name Description DownstreamConnectionTermination DC Downstream connection termination. FailedLocalHealthCheck LH Local service failed health check request in addition to 503 response code. UpstreamRequestTimeout UT Upstream request timeout in addition to 504 response code. LocalReset LR Connection local reset in addition to 503 response code. UpstreamRemoteReset UR Upstream remote reset in addition to 503 response code. UpstreamConnectionTermination UC Upstream connection termination in addition to 503 response code. DelayInjected DI The request processing was delayed for a period specified via fault injection. FaultInjected FI The request was aborted with a response code specified via fault injection. RateLimited RL The request was ratelimited locally by the HTTP rate limit filter in addition to 429 response code. 在日常支持客户各种环境的过程中，留意积累各种异常场景对应的Response Code，在内部给组内容同事做过一些小范围的分享，正好今年上海的KubeCon 2023，就尝试报了个相关议题做了个实践类的分享。\n内容： 话说故障随时随地都可能发生，在这个时间点Wednesday September 27, 2023 3:50pm - 4:25pm CST，这个会议室3层 305B会议室| 3F Room 305B，在我的演讲开始前，我的笔记本接到现场的投影仪上，组织方的投影出了问题。换了现场其他的笔记本，组织方的两个外国工程师在笔记本上一顿设置，趴在地上对投影仪一顿设置，还是不行。最后只能短路了一些信号，只是最基础的现场录像。当时就感叹如果有一个像网格的Response Flag一样明确地标识出故障类型，现场的两个老哥了解到链路上的笔记本、投影仪、亦或其他组件有问题，就能快速修复，不用耽误现场那么多人的十几分钟时间，当然我也能避免尬在台上近距离围观一个问题定位过程。\n在准备的这个演讲中比较详细地把我在生产中碰到的常见的Response Flag挨个构造条件重现一遍，剖析每个Flag的具体含义，不是Envoy官方文档的一行半精简文字；每个Flag表示的问题，当然构造的时候挑纯粹简单场景构造，但是能代表一类问题；对这些问题一般的处理方法，有些是故障的有针对性的处理，有些只是访问细节表达、网格配置内容体现的自然无需处理。作为参照帮助听众理解和解决生产中的类似问题。\n会场现场大家反馈比较好，提问提的拖堂了不少，会场组织方的美眉一直提醒。John zheng特别过来说比我昨天刚在IstioCon上讲的另外一个关于Istio 安全的内容还要干货，有内容。谢谢John，刚才会议现场投影不可用，怀疑笔记本问题时，你借笔记本给我用，只是抱歉会后你强烈建议后面报个北美或欧洲的议题把这部分做个更大范围的分享，觉得麻烦没有积极回应。\n前阵子家里有点事情，这个议题准备主要也是把工作中的点总结了下，没有顾上好好去规划评估内容和讲出来要花的时间，导致现场讲下来太干，内容太多。本来说话就很快，为了完成内容不得不更快，特别是看到后面组织方美眉一个劲的举牌子提示时间。导致有些内容比较粗的就过了下。想想也是，在规划的25分钟的时间里硬塞进了14个不算简单的实践，尽管硬拖堂讲了40分钟，还是讲的比较粗。\n会后现场、微信群和YouTube下有听众要材料，感动于大家的认可和热情，在把PPT传到官方后，决定把PPT里的每个Slide的实践稍微展开描述性，也算对得起演讲标题的重现案例的说法。\n下面是演讲中提到的每种Response Flag分别独立一个材料描述了下，这里汇总一个列表。\n00：正常访问\n01：下游连接终止 DC (DownstreamConnectionTermination)\n02：上游服务没有健康的后端实例 UH (NoHealthyUpstream)\n03：上游连接失败 UF (UpstreamConnectionFailure)\n04：上游超过重试次数 URX (UpstreamRetryLimitExceded)\n05：上游服务协议错误 UPE (UpstreamProtocolError)\n06：下游服务协议错误 DPE (DownstreamProtocolError)\n07：上游集群不存在 NC (NoClusterFound)\n08：没有匹配的路由NR (NoRouteFound)\n09：注入延时故障DI(DelayInjected)\n10：注入错误故障FI(FaultInjected)\n11：上游请求超时UT(UpstreamRequestTimeout)\n12：上游连接中断UC(UpstreamConnectionTermination)\n13：服务限流RL(RateLimited) 重现服务端限流\n14：服务限流RL(RateLimited) 重现客户端限流\n附： 演讲材料(官方Slides) Video(YouTube) 视频(国内) 视频(国内) 官方Sched主页 ","link":"https://idouba.com/kubecon2023-detailed-parse-and-reproduce-istio-response-flags-index/","section":"posts","tags":["KubeCon","访问日志","服务网格","演讲","最佳实践"],"title":"KubeCon2023：基于实际案例解析Istio访问日志ResponseFlag系列"},{"body":"","link":"https://idouba.com/tags/%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC/","section":"tags","tags":null,"title":"服务网格"},{"body":"","link":"https://idouba.com/tags/%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97/","section":"tags","tags":null,"title":"访问日志"},{"body":"","link":"https://idouba.com/tags/responseflags/","section":"tags","tags":null,"title":"ResponseFlags"},{"body":"","link":"https://idouba.com/tags/rl/","section":"tags","tags":null,"title":"RL"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第14个关注的Response Flag还是RL，全称是RateLimited，官方定义表示The request was ratelimited locally by the HTTP rate limit filter in addition to 429 response code. 不同于前一个RL的重现了服务端限流，本文将聚焦基于客户端限流重现RL。\n含义： **RL **表示触发服务限流。限流是保障服务韧性的重要手段，防止系统过载，保障服务总体的可用性。在网格中配置了本地限流或者全局限流策略，若在单位时间内请求数超过配置的阈值，则触发限流。访问日志记录RL，一般会伴随返回“429”的HTTP状态码。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod注入Siecar。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 和上一个限流重现类似，在原有正常访问的环境基础上，通过Envoy Filter配置本地限流策略。不同在于，通过SIDECAR_OUTBOUND表示入流量限流，即作用在客户端的sidecar代理上。配置限流阈值是60秒10次。\n1apiVersion: networking.istio.io/v1alpha3 2kind: EnvoyFilter 3metadata: 4 name: filter-local-ratelimit-client 5 namespace: accesslog 6spec: 7 configPatches: 8 - applyTo: HTTP_FILTER 9 match: 10 context: SIDECAR_OUTBOUND 11 ... 12 patch: 13 operation: INSERT_BEFORE 14 value: 15 name: envoy.filters.http.local_ratelimit 16 ... 17 value: 18 stat_prefix: http_local_rate_limiter 19 token_bucket: 20 max_tokens: 10 21 tokens_per_fill: 10 22 fill_interval: 60s 23 ... 第三步： 在客户端连续curl 超过10次，得到429的响应码，表示触发了限流。\n第四步： 不同于上一个限流重现在服务端日志中记录了限流RL的Response Flag，这个重现中在客户端访问日志中记录了RL。在客户端代理日志记录中前面若干条请求记录200，后面若干请求记录429 RL表示触发了限流。这里摘取了临近的几条日志，可以看到请求ID是8b99d2e9-d8b2-4724-bafc-8eb59d5dabd1之前的都是200，从dc9e2396-c2c8-4f87-8994-ff4a7f8c8061开始的请求都触发了限流，输出429。\n1[2023-08-21T09:38:59.826Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 615 1 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;8b99d2e9-d8b2-4724-bafc-8eb59d5dabd1\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local 10.66.0.24:47792 10.246.91.131:80 10.66.0.24:57562 - - 2[2023-08-21T09:39:00.237Z] \u0026#34;GET / HTTP/1.1\u0026#34; 429 - local_rate_limited - \u0026#34;-\u0026#34; 0 18 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;dc9e2396-c2c8-4f87-8994-ff4a7f8c8061\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;-\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local - 10.246.91.131:80 10.66.0.24:57578 - - 第五步： 和上一个限流重现很不同的是服务端只有前面200的正常请求，没有429的请求，因为被客户端限流了，请求不会再发到服务端。和服务端的请求完全对应，请求ID是8b99d2e9-d8b2-4724-bafc-8eb59d5dabd1之前的都是200，从dc9e2396-c2c8-4f87-8994-ff4a7f8c8061开始的请求在服务端就没有记录了。\n1[2023-08-21T09:38:59.826Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 615 0 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;8b99d2e9-d8b2-4724-bafc-8eb59d5dabd1\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; inbound|80|| 127.0.0.6:35687 10.66.0.28:80 10.66.0.24:47792 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 2----- 3NO further Logs 应对建议 用户主动配置限流策略保护目标服务，无需特殊处理。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-14-RL/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","RL","演讲"],"title":"RL(服务限流)--Istio访问日志ResponseFlag重现与解析14"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第13个关注的Response Flag是RL，全称是RateLimited，官方定义表示The request was ratelimited locally by the HTTP rate limit filter in addition to 429 response code.\n含义： **RL **表示触发服务限流。限流是保障服务韧性的重要手段，防止系统过载，保障服务总体的可用性。在网格中配置了本地限流或者全局限流策略，若在单位时间内请求数超过配置的阈值，则触发限流。访问日志记录RL，一般会伴随返回“429”的HTTP状态码。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod注入Siecar。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 在原有正常访问的环境基础上，通过Envoy Filter配置本地限流策略。以下策略中，通过SIDECAR_INBOUND表示入流量限流，即作用在服务端的sidecar代理上。配置限流阈值是60秒10次请求。\n1apiVersion: networking.istio.io/v1alpha3 2kind: EnvoyFilter 3metadata: 4 name: filter-local-ratelimit 5 namespace: accesslog 6spec: 7 configPatches: 8 - applyTo: HTTP_FILTER 9 match: 10 context: SIDECAR_INBOUND 11 ... 12 patch: 13 operation: INSERT_BEFORE 14 value: 15 name: envoy.filters.http.local_ratelimit 16 ... 17 value: 18 stat_prefix: http_local_rate_limiter 19 token_bucket: 20 max_tokens: 10 21 tokens_per_fill: 10 22 fill_interval: 60s 23 ... 第三步： 在客户端连续curl 超过10次，得到429的响应码，表示触发了限流。\r第四步：服务端代理日志记录中前面若干条请求记录200，后面若干请求记录429 RL表示触发了限流。这里摘取了临近的几条日志，可以看到请求ID是fc218c4e-6043-4d25-a091-4d534d221f83之前的都是200，从38f04fb8-269d-4bf8-89c4-6e18461d38f5开始的请求都触发了限流，输出429。\n1[2023-08-21T03:01:27.339Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 615 0 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;fc218c4e-6043-4d25-a091-4d534d221f83\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; inbound|80|| 127.0.0.6:47293 10.66.0.28:80 10.66.0.24:33554 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 2[2023-08-21T03:01:28.025Z] \u0026#34;GET / HTTP/1.1\u0026#34; 429 - local_rate_limited - \u0026#34;-\u0026#34; 0 18 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;38f04fb8-269d-4bf8-89c4-6e18461d38f5\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;-\u0026#34; inbound|80|| - 10.66.0.28:80 10.66.0.24:33554 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local - 第五步： 同时对应的客户端日志也记录200和429。和服务端的请求完全对应，请求ID是fc218c4e-6043-4d25-a091-4d534d221f83之前的都是200，从38f04fb8-269d-4bf8-89c4-6e18461d38f5开始的请求都触发了限流，记录了429。\n1[2023-08-21T03:01:27.339Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 615 1 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;fc218c4e-6043-4d25-a091-4d534d221f83\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local 10.66.0.24:33554 10.246.91.131:80 10.66.0.24:44886 - - 2[2023-08-21T03:01:28.024Z] \u0026#34;GET / HTTP/1.1\u0026#34; 429 - via_upstream - \u0026#34;-\u0026#34; 0 18 11 10 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;38f04fb8-269d-4bf8-89c4-6e18461d38f5\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local 10.66.0.24:33554 10.246.91.131:80 10.66.0.24:44902 - - 应对建议 用户主动配置限流策略保护目标服务，无需特殊处理。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-13-RL/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","RL","演讲"],"title":"RL(服务限流)--Istio访问日志ResponseFlag重现与解析13"},{"body":"","link":"https://idouba.com/tags/uc/","section":"tags","tags":null,"title":"UC"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第12个关注的Response Flag是UC，全称是UpstreamConnectionTermination，官方定义表示Upstream connection termination in addition to 503 response code.\n含义： UC表示上游连接中断，常见的一种现象是上游连接在返回应答前已经关闭。\n重现环境： UC是一个不太好构建的场景，环境和前面的大多数略有不同。\n客户端Pod，这里是特别写了一个Python程序。因为观测点在服务端代理，客户端是否注入Sidecar都可以。 目标服务，一个Cluster类型的Kubernetes服务，这里是一个代理了Nginx服务，多个服务实例。服务端Pod要求注入Siecar，观察服务端的访问日志。 重现步骤： 第一步： 配置nginx conf文件给Nginx添加一个后端后端服务。这里就是简单用tomcat容器在8080上起了一个服务。\n1 location /ucbackend { 2 proxy_http_version 1.1; 3 proxy_pass http://tomcat.accesslog:8080; 4 } 第二步： 不同于前面的测试，都是通过客户端命令行curl进行访问。构造UC的客户端控制稍微复杂些，这里编写一个简单的Python脚本，请求目标Nginx代理的服务，脚本中以Post方式发送请求，请求包括头域“Content-Length: 300”，说明将发送300大小的请求体 ，但实际发送的请求大小是0。\n当客户端容器中执行这个Python脚本时，服务端的Nginx会一直尝试接收300大小的请求，却一直收不齐，导致请求一直不会结束。这样就会触发Nginx默认的60秒超时，服务端Nginx在60秒后会自动断开连接，从而即构造出了上游连接断开的场景。\n第三步： 在客户端容器中执行以上Python程序， 观察Python脚本我们打印的输出，会看到执行后60秒得到了503的返回。\n第四步： 观察Nginx自身的日志记录了408，表示服务端不再等待，关闭了连接。\n1127.0.0.6 - - [25/Aug/2023:03:33:17 +0000] \u0026#34;POST /ucbackend/ HTTP/1.1\u0026#34; 408 0 \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 第五步： 同时服务端代理记录503UC，表示服务端断开了连接，能看到日志上请求60秒（日志显示60060毫米）的耗时。\n1[2023-08-25T03:32:17.193Z] \u0026#34;POST /ucbackend/ HTTP/1.1\u0026#34; 503 UC upstream_reset_before_response_started{connection_termination} - \u0026#34;-\u0026#34; 0 95 60060 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;6a6febc2-d669-4788-8bf2-989371c07372\u0026#34; \u0026#34;10.246.91.131:80\u0026#34; \u0026#34;10.66.1.2:80\u0026#34; inbound|80|| 127.0.0.6:33385 10.66.1.2:80 10.66.1.1:21701 - default 应对建议 这里是刻意模拟了一个服务端断开的场景，实际生产中，服务端断开的场景比较多，产生UC的原因也比较多比较杂，除了结合自身情况去试图消除UC外，实际应用中对于503 UC最简单有效的解决办法就是，连接断开了再客户端重连下。即配置一定的重试。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-12-UC/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","UC","演讲"],"title":"UC(上游连接中断)--Istio访问日志ResponseFlag重现与解析12"},{"body":"","link":"https://idouba.com/tags/ut/","section":"tags","tags":null,"title":"UT"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第11个关注的Response Flag是UT，全称是UpstreamRequestTimeout，官方定义表示Upstream request timeout in addition to 504 response code.\n含义： UT表示表示上游请求超时，一般伴随返回“504”的HTTP状态码。如典型场景在VirtualService中给目标服务配置了超时时间，当服务请求超过配置的超时时间，客户端代理自动超时，取消请求。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，为了模拟一个慢的服务，我们这个环境比前面的稍微复杂一些。把一个目标服务通过Ingress-gateway发布出来对外可以访问，同时给这个服务配置10秒的延迟；整个模拟一个慢的服务。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过Ingress-gateway的地址192.168.99.99:9999访问目标服务，观察代理的访问日志，得到正常的200响应码。从客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 通过Serviceentry定义这个服务服务的访问地址是nginx.external，这样这个通过Ingress-gateway访问的目标服务在网格中就完成了服务注册，可以通过这个nginx.external被网格内的服务访问，当然也可以对这个服务配置流量策略。\n**第三步：**给nginx.external这个Serviceentry描述的目标服务通过VirtualService定义流量策略，即配置3秒的访问超时。\n1apiVersion: networking.istio.io/v1beta1 2kind: VirtualService 3metadata: 4 name: nginx-se-vs 5 namespace: accesslog 6spec: 7 hosts: 8 - nginx.external 9 http: 10 - timeout: 3s 11 route: 12 - destination: 13 host: nginx.external 第四步： 在客户端容器中curl这个目标服务，3秒后得到504 的状态码提示，同时会提示request timeout。\n**第五步：**观察客户端访问日志记录504 UT，表示访问超过了配置的超时时间。\n1[2023-08-20T15:00:52.250Z] \u0026#34;GET / HTTP/1.1\u0026#34; 504 UT response_timeout - \u0026#34;-\u0026#34; 0 24 3000 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;c27e6f6d-9ae8-4c3d-8f4f-0e7117937c5d\u0026#34; \u0026#34;nginx.external\u0026#34; \u0026#34;100.85.115.86:9090\u0026#34; outbound|9999||nginx.external 10.66.0.24:40924 192.168.99.99:9999 10.66.0.24:40444 - - 第六步： 观察Ingress-gateway上对应访问日志上记录0 DI DC。\n1[2023-08-20T15:00:52.250Z] \u0026#34;GET / HTTP/1.1\u0026#34; 0 DI,DC downstream_remote_disconnect - \u0026#34;-\u0026#34; 0 0 3000 - \u0026#34;10.66.0.1\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;c27e6f6d-9ae8-4c3d-8f4f-0e7117937c5d\u0026#34; \u0026#34;nginx.external\u0026#34; \u0026#34;-\u0026#34; outbound|80|v1|nginx2.default.svc.cluster.local - 10.66.0.9:1025 10.66.0.1:25223 - - 基于前面DC的解析，这是因为当配置了超时时间后，客户端代理在到达超时时间时，自动断开了客户端的连接，从Ingress-gateway来看，就是调用的客户端断开了连接，因此报DC。同时，因为这个10秒的耗时是通过VirtualService配置自动注入的，因此和基于前面的DI的解析可以理解会报DI。\n应对建议 用户为了保证目标服务快速失败，配置了服务访问超时，超时就会返回UT的应答标记，无需特殊处理。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-11-UT/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","UT","演讲"],"title":"UT(上游请求超时)--Istio访问日志ResponseFlag重现与解析11"},{"body":"","link":"https://idouba.com/tags/fi/","section":"tags","tags":null,"title":"FI"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第10个关注的Response Flag是DI，全称是FaultInjected，官方定义表示The request was aborted with a response code specified via fault injection.\n含义： FI 表示故障注入错误。通过VirtualService给目标服务注入了一个特定状态码的故障。在客户端的访问日志中会返回配置的HTTP状态码，并记录FI。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 修改VirtualService，在路由上配置了一个HTTP状态码是418的模拟错误。\n1apiVersion: networking.istio.io/v1beta1 2kind: VirtualService 3metadata: 4 name: nginx-80 5 namespace: accesslog 6spec: 7 hosts: 8 - nginx 9 http: 10 - fault: 11 abort: 12 httpStatus: 418 13 percentage: 14 value: 100 15 route: 16 - destination: 17 host: nginx.accesslog.svc.cluster.local 18 subset: v1 第三步： 在客户端容器中还是使用原有方式访问目标服务，在客户端输出中会看到返回了418的状态码。原来正常返回200的目标服务未做任何修改，通过上一步VirtualService中注入418的状态码，在客户端就会得到对应的错误。\n**第四步：**观察客户端访问日志中记录了418 FI，说明进行了故障注入\n1[2023-08-20T14:38:38.690Z] \u0026#34;GET / HTTP/1.1\u0026#34; 418 FI fault_filter_abort - \u0026#34;-\u0026#34; 0 18 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;1f085510-06de-439a-8b00-8bc2c1e3dc6a\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;-\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local - 10.246.91.131:80 10.66.0.24:38616 - - 第五步： 不同于DI的场景，FI通过注入模拟了一个错误的应答。直接在客户端返回应答，请求没有发送到服务端，因此服务端没有流量，也没有记录访问日志。通过这种方式，就可以欺骗客户端应用程序，模拟目标服务返回任意的状态码。\n应对建议 访问日志中记录了人为注入的故障，无需特殊处理。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-10-FI/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","FI","演讲"],"title":"FI(注入错误故障)--Istio访问日志ResponseFlag重现与解析10"},{"body":"","link":"https://idouba.com/tags/di/","section":"tags","tags":null,"title":"DI"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第九个关注的Response Flag是DI，全称是DelayInjected，官方定义表示The request processing was delayed for a period specified via fault injection.\n含义： DI表示请求中注入了一个延时故障。在VirtualService中配置了延时故障注入时，会在服务请求时产生配置的延时，并在访问日志中会记录DI的应答标记。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 修改目标服务的VirtualService，在路由上配置10秒的延时。\n1apiVersion: networking.istio.io/v1beta1 2kind: VirtualService 3metadata: 4 name: nginx-80 5 namespace: accesslog 6spec: 7 hosts: 8 - nginx 9 http: 10 - fault: 11 delay: 12 fixedDelay: 10s 13 percentage: 14 value: 100 15 route: 16 - destination: 17 host: nginx.accesslog.svc.cluster.local 18 subset: v1 第三步： .在客户端容器中还是使用原有方式访问目标服务，可以看到得到了一个404错误，表示请求的资源不存在。在客户端容器中还是原有方式curl目标服务，在图上标记的位置，延迟了10秒后返回应答。\n**第四步：**观察客户端访问日志中记录了200 DI。\n1[2023-08-20T03:21:03.863Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 DI via_upstream - \u0026#34;-\u0026#34; 0 615 10000 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;3a52e6e3-df38-47af-912d-a21d65de4d79\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local 10.66.0.24:60450 10.246.91.131:80 10.66.0.24:42038 - - 第五步： 观察服务端日志。在服务端的日志中和正常日志完全相同，记录200的正常响应。\n1[2023-08-20T03:21:13.863Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 615 0 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;3a52e6e3-df38-47af-912d-a21d65de4d79\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; inbound|80|| 127.0.0.6:49911 10.66.0.28:80 10.66.0.24:60450 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 第六步： 观察服务的耗时字段可以看到在源服务的outbound日志中耗时10秒，而服务端的耗时是0秒。也反映了延时注入的效果，即使真正服务应答不怎么花时间，在客户端的应用程序看来耗费了10秒钟，客户端和服务端本身没有做任何修改。这也是网格透明的流量能力的一个比较生动的体现。 这里只是注入延时增加了服务响应时间，还是得到了正常的响应，因此服务端和客户端的HTTP响应码都是200。\n应对建议 访问日志中记录了人为注入的故障，无需特殊处理。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-09-DI/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","DI","演讲"],"title":"DI(注入延时故障)--Istio访问日志ResponseFlag重现与解析09"},{"body":"","link":"https://idouba.com/tags/nr/","section":"tags","tags":null,"title":"NR"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第八个关注的Response Flag是NR，全称是NoRouteFound，官方定义表示No route configured for a given request in addition to 404 response code or no matching filter chain for a downstream connection.\n含义： NR表示没有匹配的路由来处理请求的流量，一般伴随“404”状态码。比如实际的访问流量的特征不匹配VirtualService中定义的路由条件，因而没有找到匹配的路由处理请求，就会报404 NR。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n**第二步：**修改目标服务的VirtualService，在路由上添加一个HTTP 头域匹配条件，即只有满足条件的请求会发送到路由定义的后端上。\n1apiVersion: networking.istio.io/v1beta1 2kind: VirtualService 3metadata: 4 name: nginx-80 5 namespace: accesslog 6spec: 7 hosts: 8 - nginx 9 http: 10 - match: 11 - headers: 12 log-flag: 13 exact: enable 14 route: 15 - destination: 16 host: nginx.accesslog.svc.cluster.local 17 subset: v1 第三步： .在客户端容器中还是使用原有方式访问目标服务，可以看到得到了一个404错误，表示请求的资源不存在。\n**第四步：**在客户端的访问日志会记录404 NR的应答标记，表示没有找到匹配的路由\n1[2023-08-19T11:13:06.484Z] \u0026#34;GET / HTTP/1.1\u0026#34; 404 NR route_not_found - \u0026#34;-\u0026#34; 0 0 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;d9e4d779-ee2f-4dab-8546-46d61e789d35\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;-\u0026#34; - - 10.246.91.131:80 10.66.0.24:35016 - - 第五步： 观察服务端日志。客户端代理没有找到匹配的路由把流量发到对应的服务端实例，因此服务端没有流量，也没有日志输出。\n第六步： Configdump对应的路由会看到要求满足头域的匹配条件的请求才会转发，而curl请求中没有携带头域，没有匹配到定义的路由，因此报404。\n1\u0026#34;routes\u0026#34;: [ 2 { 3 \u0026#34;match\u0026#34;: { 4 \u0026#34;headers\u0026#34;: [ 5 { 6 \u0026#34;name\u0026#34;: \u0026#34;log-flag\u0026#34;, 7 \u0026#34;string_match\u0026#34;: { 8 \u0026#34;exact\u0026#34;: \u0026#34;enable\u0026#34; 9 } 10 } 11 ] 12 }, 13 \u0026#34;route\u0026#34;: { 14 \u0026#34;cluster\u0026#34;: \u0026#34;outbound|80|v1|nginx.accesslog.svc.cluster.local“ 15} 应对建议 和NC类似NR这类配置错误比运行期错误要好处理一些。客户端的流量满足路由中定义的流量特征，保证所有请求都有服务端定义的路由处理。\n额外解析 有个困扰初学者的问题：按照对HTTP语义的理解，没有后端资源的，类似HTTP的“No page found”，应该返回“404”状态码，但为什么在前面NC的例子里返回了“503”状态码，在NR中却返回了“404”状态码？\n我们把VirtualService这个虚拟服务想象成一个实际的Web服务端资源提供者或者我们写的一个Rest服务，将满足条件的请求通过其上定义的路由分发到后端服务，访问到了虚拟服务定义的网络资源。当没有匹配的路由，就是找不到服务里的资源，则返回“404”状态码；而在上一个NC场景中，若有匹配的路由定义，但是没有处理这个路由的后端，则属于HTTP服务端资源提供者内部的错误，所以返回“503”。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-08-NR/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","NR","演讲"],"title":"NR(没有匹配的路由)--Istio访问日志ResponseFlag重现与解析08"},{"body":"","link":"https://idouba.com/tags/nc/","section":"tags","tags":null,"title":"NC"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第七个关注的Response Flag是NC，全称是NoClusterFound，官方定义表示Upstream cluster not found\u0026quot;\n含义： NC表示没有上游集群，即在网格流量路由中定义的目标服务后端不存在。Istio中比较典型的场景如分流策略中流量发送给V2标识的服务子集，但是DestinationRule中并没有定义该版本标识的服务子集。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 在原有正常访问的环境上，给目标服务配置VirtualService 和DestinationRule，在VirtualService中定义服务的流量发给v2的服务子集，而在DestinationRule中只定义v1的服务子集。\n1apiVersion: networking.istio.io/v1beta1 2kind: VirtualService 3metadata: 4 name: nginx-80 5 namespace: accesslog 6spec: 7 hosts: 8 - nginx 9 http: 10 - route: 11 - destination: 12 host: nginx.accesslog.svc.cluster.local 13 subset: v2 # subset NOT exists 1apiVersion: networking.istio.io/v1beta1 2kind: DestinationRule 3metadata: 4 name: nginx 5 namespace: accesslog 6spec: 7 host: nginx 8 subsets: 9 - labels: 10 version: v1 11 name: v1 # Only v1 第三步： .在客户端的容器中curl目标服务，得到503错误。\n第四步： 观察源服务的outbound日志，可以看到提示503NC，表示请求的上游服务不存在。\n1[2023-08-19T10:09:13.864Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 NC cluster_not_found - \u0026#34;-\u0026#34; 0 0 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;1f8f95a5-5139-4790-9d7e-80d28fda24b7\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;-\u0026#34; - - 10.246.91.131:80 10.66.0.24:51674 - - 第五步： 观察服务端日志。因为客户端代理没有找到路由的上游服务，因此请求不会正常发出去，服务端代理上也不会有流量，对本次请求也不会有日志记录。\n第六步： 通过config_dump观察配置在数据面的配置，会看到目标服务的RDS定义，路由的后端是如图v2的一个上游服务；而通过CDS中目标服务存有2个上游集群定义，分别是带v1的一个分组，另一个是不带v1分组，没有v2的服务分组。\n应对建议 NC在生产中出现的频率还是比较高的，但比起那些运行期的错误，这种配置类的错误还是比较好处理的。一般要保证VirtualService中引用的后端在网格中正确注册，比如重现的这个场景中VirtualService的后端是一个服务子集时，要检查和确保通过DestinationRule正确定义了引用的服务子集。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-07-NC/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","NC","演讲"],"title":"NC(没有上游集群)--Istio访问日志ResponseFlag重现与解析07"},{"body":"","link":"https://idouba.com/tags/dpe/","section":"tags","tags":null,"title":"DPE"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第六个关注的Response Flag是DPE，全称是 DownstreamProtocolError ，官方定义表示\u0026quot;The downstream request had an HTTP protocol error\u0026quot;\n含义： UPE表示下游协议错误。如下游客户端通过一个错误的协议访问目标服务时，一般服务端会记录400DPE的日志\n重现环境： 客户端Pod，注入了Sidecar。注意这里选择的是busybox容器，确认容器中包含telnet命令。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 在客户端busybox容器中，telnet目标服务的服务地址和端口，会得到400 Bad Request的错误。表示因为客户端的请求错误导致访问失败，根本原因当然是客户端协议错误，没有如服务端要求发送HTTP协议的请求。\n第三步： 观察访问日志，客户端日志是一条四层的访问日志，因为是四层的访问 。\n1[2023-08-21T13:56:45.757Z] \u0026#34;- - -\u0026#34; 0 - - - \u0026#34;-\u0026#34; 25 162 53038 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;10.246.91.131:80\u0026#34; PassthroughCluster 10.66.0.38:45964 10.246.91.131:80 10.66.0.38:43958 - - 第四步： 服务端日志记录400 DPE 表示下游协议错误。\n1[2023-08-21T13:57:37.792Z] \u0026#34;- - HTTP/1.1\u0026#34; 400 DPE http1.codec_error - \u0026#34;-\u0026#34; 0 11 0 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; - - 10.66.0.28:80 10.66.0.1:19532 - - 应对建议 DPE协议无需对环境或目标服务进行任何修改，只需要客户端使用正确的协议访问目标服务即可。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-06-DPE/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","DPE","演讲"],"title":"DPE(下游协议错误)--Istio访问日志ResponseFlag重现与解析06"},{"body":"","link":"https://idouba.com/tags/upe/","section":"tags","tags":null,"title":"UPE"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第五个关注的Response Flag是UPE，全称是 UpstreamProtocolError ，官方定义表示\u0026quot;The upstream response had an HTTP protocol error.\u0026quot;\n含义： UPE表示上游服务协议错误。在网格中定义的服务的协议和服务实际的协议不一致时，当服务访问时，客户端会得到502协议错误的响应。同时服务端的入流量日志会记录502 UPE。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 在第一个正常用例基础上修改服务端口为gRPC，可以是修改端口名或者AppProtocol字段。\n1apiVersion: v1 2kind: Service 3metadata: 4 name: nginx 5 namespace: accesslog 6spec: 7 ports: 8 - name: grpc # modify protocol by port name or AppProtocol 9 port: 80 10 protocol: TCP 11 targetPort: 80 12 selector: 13 app: nginx 14 sessionAffinity: None 15 type: ClusterIP 第三步： 在客户端容器中正常的curl目标服务，得到502 Bad Gateway的错误，Reset reason 提示 protocol error。\n第四步： 观察访问日志，客户端日志记录502 。\n12023-08-19T09:42:38.460Z] \u0026#34;GET / HTTP/1.1\u0026#34; 502 - via_upstream - \u0026#34;-\u0026#34; 0 87 15 15 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;56f62238-c106-4a75-a848-429a36489142\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local 10.66.0.24:43270 10.246.91.131:80 10.66.0.24:51868 - - 第五步： 服务端日志记录502 UPE 表示上游协议错误。\n1[2023-08-19T09:42:38.464Z] \u0026#34;GET / HTTP/2\u0026#34; 502 UPE upstream_reset_before_response_started{protocol_error} - \u0026#34;-\u0026#34; 0 87 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;56f62238-c106-4a75-a848-429a36489142\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; inbound|80|| 127.0.0.6:45649 10.66.0.28:80 10.66.0.24:43270 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 应对建议 在服务中正确的定义服务端口的应用协议。Istio中读取Service中端口名中的协议信息，或AppProtocol 来判定服务的应用协议，确保协议配置正确。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-05-UPE/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","UPE","演讲"],"title":"UPE(上游服务协议错误)--Istio访问日志ResponseFlag重现与解析05"},{"body":"","link":"https://idouba.com/tags/urx/","section":"tags","tags":null,"title":"URX"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第四个关注的Response Flag是URX，全称是 UpstreamRetryLimitExceded ，官方定义表示\u0026quot;The request was rejected because the upstream retry limit (HTTP) or maximum connect attempts (TCP) was reached..\u0026quot;\n含义： URX表示超过了HTTP的请求重试阈值，或者TCP的重连阈值，而导致访问被拒绝。这时客户端的访问日志中会记录URX。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 在第一个正常用例基础上修改服务的target port为错误的服务端口888。\n1apiVersion: v1 2kind: Service 3metadata: 4 name: nginx 5 namespace: accesslog 6spec: 7 ports: 8 - name: http 9 port: 80 10 protocol: TCP 11 targetPort: 888 # Modify target port 80-\u0026gt;888，make service instance request failed 12 selector: 13 app: nginx 14 type: ClusterIP 第三步： 在客户端容器中curl 目标服务，得到503错误，提示连接失败。\n第四步： 观察客户端容器的访问日志出现503 URX，表示请求次数超过了重试次数后错误。\n1[2023-08-19T08:38:56.203Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 URX via_upstream - \u0026#34;-\u0026#34; 0 145 51 51 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;1bd7e9c8-0881-41b2-a964-525cad938d00\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:888\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local 10.66.0.24:57388 10.246.91.131:80 10.66.0.24:37236 - - 第五步： 观察服务端日志中记录503UF，结合上一个用例UF的理解，是服务端sidecar连接他的上游，即目标服务的一个实例，因为实例端口错误，连接失败了，报503UF。\n1[2023-08-19T08:38:56.207Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 UF upstream_reset_before_response_started{connection_failure,delayed_connect_error:_111} - \u0026#34;delayed_connect_error:_111\u0026#34; 0 145 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;1bd7e9c8-0881-41b2-a964-525cad938d00\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:888\u0026#34; inbound|888|| - 10.66.0.28:888 10.66.0.24:57376 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 2[2023-08-19T08:38:56.234Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 UF upstream_reset_before_response_started{connection_failure,delayed_connect_error:_111} - \u0026#34;delayed_connect_error:_111\u0026#34; 0 145 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;1bd7e9c8-0881-41b2-a964-525cad938d00\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:888\u0026#34; inbound|888|| - 10.66.0.28:888 10.66.0.24:57384 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 3[2023-08-19T08:38:56.254Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 UF upstream_reset_before_response_started{connection_failure,delayed_connect_error:_111} - \u0026#34;delayed_connect_error:_111\u0026#34; 0 145 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;1bd7e9c8-0881-41b2-a964-525cad938d00\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:888\u0026#34; inbound|888|| - 10.66.0.28:888 10.66.0.24:57388 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 第六步： 再详细解析下两组日志可以看到客户端Pod一次请求，在客户端生成一次outbound日志，在服务端产生三次inbound日志，request Id 都相同。说明客户端发起第一次原始请求失败后，还自动发起了另外2次重试。Istio中对网格管理的服务默认进行2次重试，可以配置修改。这个场景中服务targetPod错误，导致三次访问都失败了。失败后客户端日志报URX，服务端上游服务不健康，符合上一个用例UF的条件，日志记录UF。\n应对建议： 提高服务的质量，保证成功率，保证服务在有限重试后能正常的被访问。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-04-URX/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","URX","演讲"],"title":"URX(上游超过重试次数)--Istio访问日志ResponseFlag重现与解析04"},{"body":"","link":"https://idouba.com/tags/uf/","section":"tags","tags":null,"title":"UF"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第三个关注的Response Flag是UF，UF的全称是 UpstreamConnectionFailure ，官方定义表示\u0026quot;Upstream connection failure in addition to 503 response code.\u0026quot;\n含义： 表示上游连接失败。典型场景如目标服务的服务端口不通。如客户端通过错误的端口访问目标服务时，会导致客户端的服务访问失败，客户端代理的Outbound日志会记录503UF。\n目标服务的服务实例端口不通，会导致服务端的服务访问失败，同时目标服务端代理的Inbound日志会记录503UF。我们构建一个服务不通客户端Outbound日志记录UF，服务端inbound 日志的503 U后面的在另外一个URX用例里可以看到，综合起来可以更完整理解UF的含义和出现场景。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中通过目标名和服务端口访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 在第一个正常访问的用例基础上修改服务端口为错误的服务端口888。\n1apiVersion: v1 2kind: Service 3metadata: 4 name: nginx 5 namespace: accesslog 6spec: 7 ports: 8 - name: http 9 port: 888 # Modify service port 80-\u0026gt;888，make service request failed 10 protocol: TCP 11 targetPort: 80 12 selector: 13 app: nginx 14 type: ClusterIP 第三步： 在客户端容器中curl 目标服务端口80，curl命令返回503，错误信息包括：upstream connect error or disconnect/reset before headers. reset reason: connection failure 。\n第四步： 观察客户端容器的访问日志出现503 UF。\n1[2023-08-19T08:10:17.447Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 UF upstream_reset_before_response_started{connection_failure} - \u0026#34;-\u0026#34; 0 91 10001 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;819fb0d7-1ae1-4689-b10a-394a0a53c546\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.246.91.131:80\u0026#34; PassthroughCluster - 10.246.91.131:80 10.66.0.24:60926 - allow_any 第五步： 因为客户端代理连接不到目标上游服务，因此请求不会发出去，服务端代理不会有流量，服务端不会记录本次请求的访问日志。\n应对建议： 检查服务定义，确保服务（或服务实例的）端口配置正确，客户端通过正确的端口访问目标服务。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-03-UF/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","UF","演讲"],"title":"UF(上游连接失败)--Istio访问日志ResponseFlag重现与解析03"},{"body":"","link":"https://idouba.com/tags/uh/","section":"tags","tags":null,"title":"UH"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第二个关注的Response Flag是UH，UH的全称是NoHealthyUpstream，官方定义表示\u0026quot;No healthy upstream hosts in upstream cluster in addition to 503 response code.\u0026quot;\n含义： 表示上游服务没有健康的后端实例。典型场景如目标服务的后端实例不可用，比如在Kubernetes中目标服务的实例数设置为0.。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个Cluster类型的Kubernetes服务，多个服务实例。服务端Pod可以注入Siecar，也可以不用注入。 重现步骤： 第一步： 从注入了网格代理的客户端Pod中访问目标服务，观察代理的访问日志，得到正常的200响应码。从服务端和客户端的访问日志上都可以看到服务在目标服务的多个实例上负载均衡。正常访问参照本系列的环境部分描述。\n第二步： 在前面正常用例的基础上把目标服务的实例数scale到0，使得目标服务没有可用的实例。\n1kubectl scale --replicas=0 deployment/nginx -naccesslog 第三步： 重复前面客户端的访问，即从注入了sidecar的源服务负载中curl目标服务。这时观察客户端会得到503 的错误码，并且包含错误信息no healthy upstream。\n第四步： 观察客户端outbound的日志，记录了503 UH no_healthy_upstream 。\n1[2023-08-19T07:50:46.616Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 UH no_healthy_upstream - \u0026#34;-\u0026#34; 0 19 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;25e82276-6d3e-481d-9c07-c1a3404bf5a9\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;-\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local - 10.246.91.131:80 10.66.0.24:50552 - - TTT 1[2023-08-19T07:50:46.616Z] \u0026#34;GET / HTTP/1.1\u0026#34; 503 UH no_healthy_upstream - \u0026#34;-\u0026#34; 0 19 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;25e82276-6d3e-481d-9c07-c1a3404bf5a9\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;-\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local - 10.246.91.131:80 10.66.0.24:50552 - - 第五步： 如果服务端Pod注入了Sidecar，检查几个服务端实例上注入的Sidecar上的访问日志。没有找到健康的实例，流量也不会发到上游，服务端代理不会有流量，对本次请求也不会有日志记录。\n应对建议： 检查目标服务的负载配置，确认目标服务有可用的后端，可以被正常访问。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-02-UH/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","UH","演讲"],"title":"UH(上游没有健康的后端实例)--Istio访问日志ResponseFlag重现与解析02"},{"body":"","link":"https://idouba.com/tags/dc/","section":"tags","tags":null,"title":"DC"},{"body":"KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n第一个关注的Response Flag是DC，DC的全称是DownstreamConnectionTermination，官方定义是”Downstream connection termination“。\n含义： DC表示下游连接终止。\n在访问目标服务时，在收到完整应答前，客户端主动断开连接时，会产生DC特征的应答标记。客户端断开应答的场景比较多，生产中我们经常碰到的是客户端设置了请求超时，超时后客户端断开了连接。则在访问日志中一般会记录本次请求的结果为DC。\n重现环境： 客户端Pod，注入了Sidecar。 目标服务，一个花费一定时间才会返回的服务。为了有机会再客户端请求发出后，收到应答前有机会主动断开，我们访问的服务不能太快速返回，所以这里构造一个10秒才会响应的服务，模拟一个看上去有点慢的服务。可以是编码的一个10秒才相应的服务。当然基于Istio非侵入方式构造一个慢服务非常方便。这里的目标服务是把一个目标服务通过Ingress-gateway发布出来对外可以访问，同时给这个服务配置10秒的延迟，来模拟一个慢的服务。 重现步骤： 第一步： 进入客户端Pod中curl目标服务，观察客户端访问结果和客户端代理的访问日志，可以看到访问结果正常。只是目标服务有延迟，总的访问耗时10秒。这里为了突出重点，正常访问的内容略去。\n第二步： 客户端通过命令行访问目标服务，客户端curl命令访问时，携带max-time参数，设置客户端curl的最大时间为2秒。观察访问结果。\n1curl -v -s 192.168.99.99:9999s/ --header \u0026#34;Host: nginx. external\u0026#34; --max-time 2 从客户端调用的截图上可以看到请求在2秒后结束，服务访问失败。废物本身需要10秒钟返回结果，在2秒的时候客户端因为超时主动断开。\n这里是为了模拟一种更接近真实应用的场景。在模拟环境下构造客户端断开更简单的办法是不设置超时，直接curl，在得到请求返回前ctl+c结束curl请求也可以得到类似的效果。\n第三步： 观察客户端的outbound日志可以看到收到了0 DC downstream_remote_disconnect的信息。同时一个小细节，客户端访问日志可以看到本次访问的耗时DURATION是1999毫秒，与我们配置的2秒钟超时吻合。\n1[2023-08-18T11:31:40.069Z] \u0026#34;GET / HTTP/1.1\u0026#34; 0 DC downstream_remote_disconnect - \u0026#34;-\u0026#34; 0 0 1999 - \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;afe165f1-27ab-447e-823d-b5d50103d197\u0026#34; \u0026#34;nginx.external\u0026#34; \u0026#34;100.85.115.86:9090\u0026#34; outbound|9999||nginx.external 10.66.0.24:58540 192.168.99.99:9999 10.66.0.24:41660 - - 应对建议： DC一般无需特殊处理。\n大部分情况下DC的原因是，服务端耗时较长导致客户端在一定时间后断开了连接。这时候一般考虑优化目标服务，在有效的时间内返回应答。\n","link":"https://idouba.com/detailed-parse-and-reproduce-istio-response-flags-01-DC/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags","DC","演讲"],"title":"DC(下游连接终止)--Istio访问日志ResponseFlag重现与解析01"},{"body":" KubeCon 2023在上海做的一个关于Istio访问日志的演讲《Detailed Parse and Reproduce Response Flags of Istio Access Log Based on Production Use Case》。解析和重现了在当时解决客户问题时碰到的各种应答日志。\n在详细展开每种Response Flag前先介绍下本系列的必要前置信息。包括访问日志的背景、机制，以及重现这些Response Flag的基本环境，方便有兴趣的同学参照练习。\n机制 早期的访问日志一般由应用程序输出，即要求用户在业务代码中记录每次访问。在服务网格中，和指标、调用链等可观测性能力类似，Istio通过非侵入方式提供访问日志的收集。\n过程大致是：\n1.网格数据面拦截流量，并根据配置的访问日志格式输出访问日志。\n2.数据面根据配置的ALS(Access log Service)地址上报访问日志。\n3.ALS服务端收集日志，存储在日志存储，如ES中，或其他的日志系统中。\n4.服务端日志检索工具如Kibana或其他日服务索日志。\n这是一个一般性流程机制，在Istio中日志可以通过ALS的gRPC的服务收集日志，也可以写日志文件、标准输出或者对接OpenTelemetry等通道，即各种标准接口对接各种日志系统和通道，日志格式可以动态定义。\n环境 这是我这次实践的环境。\n在一个accesslog的命名空间下，我们创建了两个服务。两个服务均注入了Sidecar。源服务内有curl命令，我们会通过curl访问目标服务，生成访问日志。目标服务是一个端口是80的Nginx容器。\n1apiVersion: v1 2kind: Service 3metadata: 4 name: nginx 5 namespace: accesslog 6spec: 7 ports: 8 - name: http 9 port: 80 10 protocol: TCP 11 targetPort: 80 12 selector: 13 app: nginx 14 type: ClusterIP 可以看到整个环境是比较干净简单，我们会尽量在最简单的环境上构造各种不同的场景，重现大多数常见的Response Flag，方便大家理解。\n1# kubectl get po -naccesslog -owide 2NAME READY STATUS RESTARTS AGE IP NODE 3nginx-57d5c48b96-2wdnb 2/2 Running 0 3d17h 10.66.0.28 172.16.0.24 4nginx-57d5c48b96-k9gzh 2/2 Running 0 18h 10.66.1.2 172.16.0.35 5client-589f87c6f9-hnh5x 2/2 Running 0 7d 10.66.0.24 172.16.0.24 1# kubectl get svc -naccesslog 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3nginx ClusterIP 10.246.91.131 \u0026lt;none\u0026gt; 80/TCP 7d 4client ClusterIP 10.246.71.113 \u0026lt;none\u0026gt; 8080/TCP 7d 格式 这是用例中的访问日志格式。Istio的访问日志格式可以动态定义，在控制面配置后会动态应用到各个数据面上。\n这里没有配置特定的日志格式，从数据面的ConfigDump看应用到的是默认的访问日志格式。\n1 \u0026#34;access_log\u0026#34;: [ 2 { 3 \u0026#34;name\u0026#34;: \u0026#34;envoy.access_loggers.file\u0026#34;, 4 \u0026#34;typed_config\u0026#34;: { 5 \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\u0026#34;, 6 \u0026#34;path\u0026#34;: \u0026#34;/dev/stdout\u0026#34;, 7 \u0026#34;log_format\u0026#34;: { 8 \u0026#34;text_format_source\u0026#34;: { 9 \u0026#34;inline_string\u0026#34;: \u0026#34;[%START_TIME%] \\\u0026#34;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\u0026#34; %RESPONSE_CODE% %RESPONSE_FLAGS% %RESPONSE_CODE_DETAILS% %CONNECTION_TERMINATION_DETAILS% \\\u0026#34;%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\u0026#34; %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\u0026#34;%REQ(X-FORWARDED-FOR)%\\\u0026#34; \\\u0026#34;%REQ(USER-AGENT)%\\\u0026#34; \\\u0026#34;%REQ(X-REQUEST-ID)%\\\u0026#34; \\\u0026#34;%REQ(:AUTHORITY)%\\\u0026#34; \\\u0026#34;%UPSTREAM_HOST%\\\u0026#34; %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\u0026#34; 10 } 11 } 12 } 13 } 14 ] 访问 这是我们的第一个访问日志的用例。在正常场景下，从源服务的容器中通过服务名curl目标服务。如果服务运行正常，源服务中会得到200 OK的访问结果。同时，在源服务的访问日志中会记录Outbound的流量。在目标服务的访问日志中会记录Inbound流量。\n通过源和目标两个日志中request Id可以看到这是同一个请求。在正常场景下，从源服务的容器中通过服务名curl目标服务。如果服务运行正常，源服务中会得到200 OK的访问结果。\n同时，在源服务的访问日志中会记录Outbound的流量。在目标服务的访问日志中会记录Inbound流量。通过源和目标两个日志中request Id可以看到这是同一个请求。\n1[2023-08-19T08:21:07.115Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 615 16 16 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;9d407fb5-ee88-4afd-bdd5-2caa22084c89\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; outbound|80|v1|nginx.accesslog.svc.cluster.local 10.66.0.24:56896 10.246.91.131:80 10.66.0.24:50854 - - 1[2023-08-19T08:21:07.120Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 615 0 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.52.1\u0026#34; \u0026#34;9d407fb5-ee88-4afd-bdd5-2caa22084c89\u0026#34; \u0026#34;nginx.accesslog\u0026#34; \u0026#34;10.66.0.28:80\u0026#34; inbound|80|| 127.0.0.6:50739 10.66.0.28:80 10.66.0.24:56896 outbound_.80_.v1_.nginx.accesslog.svc.cluster.local default 文本类型的日志字段对应解析如下，方便读者对应。后续每种Response Flag的日志也是采用完全相同的日志格式。\nAccess log Field Client Side Outbound Server Side Inbound [%START_TIME%] [2023-08-19T08:21:07.115Z] [2023-08-19T08:21:07.120Z] %REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL% GET / HTTP/1.1 GET / HTTP/1.1 RESPONSE_CODE 200 200 RESPONSE_FLAGS - - RESPONSE_CODE_DETAILS via_upstream via_upstream CONNECTION_TERMINATION_DETAILS - - UPSTREAM_TRANSPORT_FAILURE_REASON - - BYTES_RECEIVED 0 0 BYTES_SENT 615 615 DURATION 16 0 RESP(X-ENVOY-UPSTREAM-SERVICE-TIME) 16 0 REQ(X-FORWARDED-FOR) - - REQ(USER-AGENT) curl/7.52.1 curl/7.52.1 REQ(X-REQUEST-ID) 9d407fb5-ee88-4afd-bdd5-2caa22084c89 9d407fb5-ee88-4afd-bdd5-2caa22084c89 REQ(:AUTHORITY) nginx.accesslog nginx.accesslog UPSTREAM_HOST 10.66.0.28:80 10.66.0.28:80 UPSTREAM_CLUSTER outbound|80|v1|nginx.accesslog.svc.cluster.local inbound|80|| UPSTREAM_LOCAL_ADDRESS 10.66.0.24:56896 127.0.0.6:50739 DOWNSTREAM_LOCAL_ADDRESS 10.246.91.131:80 10.66.0.28:80 DOWNSTREAM_REMOTE_ADDRESS 10.66.0.24:50854 10.66.0.24:56896 REQUESTED_SERVER_NAME - outbound_.80_.v1_.nginx.accesslog.svc.cluster.local ROUTE_NAME default 后面每个场景都会follow这个的形式。\n1.明确构造的条件重现用例的场景\n2.详细的重现步骤\n3.客户端请求的实际返回分析\n4.一个服务间访问关系的图解析访问中的细节，\n5.然后是实际的访问日志。会观察客户端outbound日志和服务端inbound的日志\n","link":"https://idouba.com/2023-09-27-detailed-parse-and-reproduce-istio-response-flags-00-Normal/","section":"posts","tags":["KubeCon","访问日志","ResponseFlags"],"title":"正常访问--Istio访问日志ResponseFlag重现与解析00"},{"body":"","link":"https://idouba.com/tags/istiocon/","section":"tags","tags":null,"title":"IstioCon"},{"body":"","link":"https://idouba.com/categories/istiocon/","section":"categories","tags":null,"title":"IstioCon"},{"body":" 记录在2023年9月26日在上海IstioCon上发表的技术演讲《cert-manager Help Enhance Security and Flexibility of Istio Certificate Management》\n议题： 对等身份验证是 Istio 零信任安全模型的基本组成部分。默认情况下，Istio 创建私钥和自签名根证书，使用它们自动签署和颁发 X.509 证书给每个工作负载，并帮助应用程序实现互相 TLS，以实现无需更改代码的安全服务间通信。在生产环境中，强烈建议从 PKI 提供商颁发根 CA，以增强安全性并提供更多的灵活性。在这次演讲中，超盟将分享 cert-manager 的详细实践，它是一个强大且可扩展的 X.509 证书控制器，如何帮助 Istio 构建增强的零信任网络。演讲将说明 cert-manager 如何通过自动从指定的 PKI 提供商获取证书，并在到期前的配置时间内更新证书，以避免任何服务停机，从而简化 Istio 根 CA 的生命周期管理。\nPeer authentication is fundamental part of Istio’s zero-trust security model. By default, Istio creates a private key and self-signed root certificate, uses them to automatically sign and issue X.509 certificates to every workload, and help application make mutual TLS to secure service-to-service communication without code changes. In production environment, it is strongly recommended to issue the root CA from a PKI provider to enhance the security and provide more flexibility. In this speech, Chaomeng will share a detailed practice of how cert-manager, a powerful and extensible X.509 certificate controller, help Istio build enhanced zero-trust network. That is how cert-manager simplify Istio root CA lifecycle management by automatically obtaining certificates from a specified PKI provider, and renewing certificates at a configured time before expiry to avoid any service downtime.\n正文： 大家好，我是张超盟，来自华为云。今天我分享主题是istio安全的相关内容。 大家使用istio最常见的是非侵入的流量和可观测性能力，如方便的灰度分流策略。实际上，随着使用深入，会感受到istio强大的安全能力。 实际上与其说istio提供了非侵入的认证、授权等安全能力，不如说提供了一个完备的安全模型。我们一般称为零信任安全网络。 在这个安全模型中，证书是非常基础的一块能力。今天的分享中我们将了解istio和一个证书管理服务cert-manager配合提供全面灵活的安全能力。\n我是华为云分布式云原生的架构师，从2018年开始也一直在负责华为云应用服务网格的设计开发工作。\n演讲的内容包括：\n必要的背景Istio零信任安全； Istio安全中证书碰到的问题 和cert-manager给出的解决办法 重点是两个实践：cert-manager提供Istio根证书管理和Ingress-gateway证书管理的的实践。 首先如今天的这次演讲的标题，我们的切入点和上下文是零信任安全。零信任安全的内容很多，我们这里仅介绍本次演讲必要的背景内容。\n这是零信任安全的一个比较标准的定义。零信任安全模型描述了一种用于 IT 系统策略、设计和实施的方法。\n零信任安全模型背后的主要概念是“从不信任，始终验证”。零信任就是不信任，目标在什么都不信任的环境上提供安全，千万别文字上简单理解成没有信任的安全环境。默认情况下，不应信任用户和设备，即使它们连接到许可的网络，即使它们以前已经过验证。通过建立强身份验证、在授予访问权限之前验证设备合规性，以及确保仅对明确授权的资源进行最小特权访问来实现。\n可以看到这和传统理解的安全只是使用防火墙IDS这些在入口处防护，零信任模型里，网络内部和外部都不会信任任何人，网络、人、设备、负载都是zero trust。\n零信任安全模型的一个典型实现是这样。可以看到涉及四个大框：\n右上角的Resource表示访问的资源，也是零信任安全保护的对象，所有的数据、计算都认为是Resource，对这些资源的访问都要基于零信任网络的原则进行管理。 左上角是资源请求者，或者说客户端。不管这个客户端来自网络外部还是网络内部都是不被信任。 中间的策略执行点PEP来决定哪些访问是信任哪些不信任。PEP基于负载的身份标识、认证客户端的身份、并基于请求属性动态判定对目标资源的访问是否信任。注意这里的判定基于一个独立的连接或者会话，对请求进行评估判定，因此可能出现上一次请求是允许的后面的请求不允许，可能来自一个客户端对统一资源的请求，请求中携带的某个属性变化了，请求就不被允许。 PEP应用的策略都是基于控制面PDP动态生效，就是图下面部分。 以上这个图是不是很容易联想到Istio这张经典的安全的架构图。网格的数据面代理非常适当地扮演了PEP的角色，实现了零信任模型中对PEP的要求。\n网格中支持Kubernetes Service account、云平台身份等多种身份标识，并通过证书或编码身份，进行认证；网格数据面代理应用间进行透明的双向认证，同时进行访问通道加密。并且可以自动为网格中的负载签发、续签证书；基于认证的服务身份，结合应用访问内容配置细粒度的授权策略，细粒度地控制资源访问权限，践行零信任安全要求的每会话控制要求和最小访问权限原则；还可以通过访问日志详细记录服务间访问。\n并且很重要一点，以上这些能力都是非侵入方式提供，透明地拦截服务间的流量，透明地应用配置的安全策略，应用代码不感知也无需修改，应用部署的下层基础设施无需修改。所以有时我们也说Istio提供了一个零信任的网络基础设施，部署运行在上面的服务负载天然就是满足零信任的要求。\n上面图上橘红色圆点标记了Istio安全体现中证书作用的位置。证书是在Istio的零信任安全体系的根基。我们下面展开看下Istio安全体现中证书的几处应用。\n前面说过了Istio提供了一套基于负载身份的认证体系。Istio数据面基于负载身份生成负载的证书，并基于负载的证书进行透明的双向认证和通道加密。\n主要机制是Istio代理负责维护本Pod内的证书和密钥。Istio代理生成私钥，并基于负载身份，一般是k8s的service account，向Istiod发起证书签名请求CSR（Certificates Signing Request）得到签发的证书和对应的Istio代理上的密钥一起使用。网格代理Envoy上通过SDS获取新的证书，并基于该证书进行身份认证和服务间安全通信。Istio代理负责证书和密钥的定期轮换，当监控到证书过期时重新申请，并向本地Envoy推送新的证书。\n虽然负载上证书自动签发和维护，那么签发这些证书的根证书呢？默认情况下Istio提供了一种开箱即用的方式来启用这些安全能力，Istiod会自动创建一个私钥和一个自签名的有效期为10年的证书，存储在cacerts中。Istiod则使用这个根CA为负载签发证书。但在生产环境下这个自签的证书不能满足安全要求，另外每个控制面自签的证书各自负载签发负载证书，也会导致多控制面的场景下，负载间不能互相认证。\n另外是网格ingress处的TLS Termination。在Ingress-gateway上配置服务端证书，供源服务，一般是浏览器等客户端认证，并将客户端到网格边界这段TLS的加密流量透明转化为非加密流量向下传递。在Gateway提供客户端认证，避免了入口服务frontend管理证书和与调用方TLS交换的问题。frontend仍然是普通的HTTP协议提供能力，TLS被ingress Gateway卸载或终止掉了。\n网格中与Ingress 处的TLS Termination对应的是Egress的TLS Origination。Egress-gateway接收网格内部未加密的流量，根据网格外部服务端的的认证要求进行Simple或双向TLS认证。在Egress-gateway认证网格外部的服务端，避免了出口的微服务自身管理证书和与目标服务TLS交互的问题。backend仍然是普通的HTTP或TCP协议访问外部服务，Egress-gateway代替网格服务发起了TLS请求。\n这里列了个表格简单对比了三种模式。\n位置看mTLS作用在网格内部，TLS termination和TLS origination分别作用在网格的入口和出口。\nmTLS是Istio自动签发证书，自动加载到网格数据面的Sidecar上，Sidecar代替业务透明地完成TLS。TLS termination是客户端程序和Ingress-gateway间进行TLS交换和认证。TLS origination是Egress-gateway和外部目标服务间进行TLS交换和认证。\n但有一个共同的特点，都是基于证书的认证，TLS termination和TLS origination是配置的实体证书，需要有证书管理。mTLS虽然负载的证书是由Istio自动签发，但网格的根证书管理还是要解决的问题。包括如何进行证书的管理、续签等。\n下面我们总结整理下Istio中证书相关的挑战和需求\n首先，避免证书问题引起的服务运行问题，不管是证书过期还是证书配置错误。但是遗憾的是，在实际生产中这却是导致最终业务故障的一类典型问题。另外关于证书最直接的需求是在证书有效期过期前的自动续签。生产中经常是运维人员配置相关告警，在证书快过期前收到告警后，手工去签发和替换证书。如果能有办法在到期前自动续签，可以减少这部分人工负担，并可避免人工操作引入的失误。\n可以有一种方式可以灵活方便地配置证书的相关属性，包括有效期、dns域名等。最好是能支持多种证书签发者，包括公有CA和私有CA等。另外一个是最好能和云原生有结合，方便云原生的应用或者平台使用证书。\n对于以上问题和需求，当前有一个比较合适的解决方案，那就是cert-manager。\n这是摘自cert manager官方的介绍。是一个作用于k8s的强大的可扩展的x509证书控制器。可以从多种包括公有和私有CA的不同的证书签发者获取证书，保证证书满足有效期要求，基于证书配置的时间，在过期前自动续签。\n社区描述的能力我们这里就不挨着过了，基于后面的实践会有体会。\n这里的cert就是certificate，翻译成中文我们一般指数字证书，简称证书。数字证书的基本概念这里不完全介绍，我们这里一句话补充下：根据非对称密码学的原理，每个证书持有人都有一对公钥和私钥，这两把密钥可以互为加解密。数字证书就是经过CA认证过的公钥，因此数字证书和公钥一样是公开的。\n而私钥一般情况都是由证书持有者在自己本地生成或委托受信的第三方生成的，由证书持有者自己负责保管或委托受信的第三方保管。比如前面介绍Istio签发的负载证书，私钥是在stio agent中维护，只是向Istiod请求签发一个证书；而公共或私有的CA机构，创建CA类型的证书时，只有证书本身，不会给你私钥，私钥在机构维护。\n这里是cert-manager的架构，体现了架构也体现了机制和流程。cert-manger 关键概念很多，满足基本使用要求，了解两个即可，就是图上下面两层：Issuer表示cert-manager对接的一个证书发行者CA。Issuer接受证书签发请求CSR，向请求方签发证书。Certificate表示一个要签发的x509证书的详细描述，包括哪个颁发者签发，有效期，多长时间自动续签等。。\n总体流程是：通过Issuer配置一般对接一个证书机构，通过certificate描述要签发的证书。cert-manager把CSR发给Issuer描述的证书发行者，并将证书发行者签发的证书，存储在Secret中。\ncert-manager内置了多个证书发行者，也可以扩展支持其他证书发行者，如各种厂商提供的私有证书PCA，扩展Issuer的定义，并安装Addon来对接PCA。\n这里只是概念模型上介绍下其定位和用法，具体用法后面的实践中详细了解。\n首先我们介绍cert-manager为Istio服务网格生成根证书，我们会介绍cert-manager通过对接一个CCM的私有CA来签发证书，这个证书作为网格的根CA，为网格的负载签发证书。\n这个图是我在前面介绍的Istio根证书机制上添加了基于cert-manager的证书签发机制的完整流程。\n1.首先创建 PCA的 CA 层次结构，包括根 CA 和从属 CA，这个步骤我们下页详细介绍\n2.第二步，配置将从属 CA 设置为cert-manager的证书发行者\n3.cert-manager根据certificate的配置从PCA签发证书，并存储在cacerts这个secret 中。\n4.cert-manager管理证书的生命期，根据证书对象的配置在到期前续签证书。\n下面就是Istio的固有流程和机制了：\n1.Istiod 提供 gRPC 服务来接收证书签名请求 （CSR）\n2.Istio 代理创建私钥和 CSR，并将 CSR 发送到 Istiod 进行签名\n3.Istiod 根据从cert-manager获取到的根证书签发 CSR 以生成证书\n4.Envoy 通过SDS向 Istio 请求证书和密钥\n5.Istio 代理监测工作负载证书的到期时间，定期检查证书和密钥轮换\n6.工作负载的Sidecar基于负载证书进行透明的双向 TLS （mTLS）\n这是PCA和Istio总体的CA层次结构。可以看到分四层，前两层在PCA中定义，后两层在Istio中实现。使用pca的从属ca作为证书发行者，签发证书，作为istio控制面的根证书，再签发负载实体证书。\n最下是实体证书，也称为叶子证书或终端证书，安装在终端上。进行实体身份认证，可以是客户端也可以是服务端。如我们前面提到的ingressgw和egressgw上配置的证书。是证书链的最后一层，不能签发其他证书。\n最顶层是根证书，是公钥体系的信任根，可以签发下层证书。根ca如果频繁使用，它的密钥泄露风险就增加，一旦泄露，下层所有证书都必须废弃。所以一般不直接用根证书签发最终的实体证书。而是引入中间层，从属ca。\n图上中间两层就是从属ca，用于隔离根证书与下层的叶子证书。在证书链验证过程中对下一层证书进行校验。可以向下签发从属ca或叶子证书。当一个从属ca暴露，只用吊销和和替换它这个分支下的证书，不影响根ca和其他从属ca和其他叶子证书。\nca层次从上向下有效期逐层减少。强制要求新签发的证书到期时间不能超过其父证书。根ca使用最少，保护安全等级最高，一般有效期10-30年。从属ca越往下层有效期越短，一般设置2-5年。实体证书使用最频繁，泄露风险最高，一般根据应用场景设置一两年几个月甚至几小时。\n这是在华为云CCM上创建的证书。首先创建根CA，然后根据根CA创建一个从属CA。在选择从属CA的有效期时，要求不能超过根CA的有效期。创建时可以填证书的各种属性。\n下来，配置cert-manager最重要的两个对象Issuer和Certificate。\n其中Issuer是一个扩展实现，包括接连接PCA的插件，和对应的CRD，描述对Issuer的配置。不同类型的Issuer配置会不同。这个CCM的Issuer配置连接上一步CCM的PCA创建的一个从属CA，使用这个从属CA签发证书。\n待签发的证书通过certificate对象来描述。几个值得关注的属性：\nisCA，描述这里签发的是一个可以签发证书的证书，即是一个CA。结合前面的架构图，我们理解这里签发的证书要作为网格的root CA，响应数据面的CSR请求，为数据面签发负载的证书。 Duration，表示证书的有效期，这里是以小时为单位描述，不能超过在PCA上创建的二级从属证书的有效期 Renewbefoe表示在有效期到期前10天，cert-manager调用自动从PCA续签证书。 Secret是签发证书的存储位置，存储在cacerts的secret中。 这是Isito中的典型配置，给一个目标服务启用mTLs，通过Peerauthentication配置对目标负载使用严格默认的认证；DestinationRule中定义使用ISTIO_MUTUAL的模式，即Istio提供的透明双向认证。Istio自身配置这里我们不多赘述。\n当启用了mTLS后，服务就完全无需关心tls和证书的相关问题，服务端协议HTTP，客户端通过普通HTTP协议访问。通过客户端的访问输出、客户端和服务端代理的访问日志上都可以看到应用本身没有涉及TLS和证书。\n代理根据认证策略。自动地为服务提供透明的mTLS，实现服务的双向认证、通道加密。后续配合基于身份的授权策略，管理服务间的访问授权，实现最小访问权限，只有明确的特定身份可以访问某些特征的服务。如具体哪些身份的负载可以访问目标服务的哪些接口，这里的身份就是编码在证书中。\n下面是通过cert manager为Istio的Ingress-gateway签发证书，生产中Ingress-gateway的证书过期导致的服务不通的问题时有发生。\n总体架构如和流程如下：\n1.在cert-manager中创建Issuer\n2.在cert-manager中创建证书\n3.配置的Issuer签发证书并存在配置的secret中\n4.cert-manager会一直检查证书的有效期，在过期前自动续签。\n5.Istio gateway的TLS中定义使用上一步生成的secret作为认证的资料\n6.Istio的动态把证书推送到Ingress-gateway\n7.客户端发起HTTPS请求，携带服务端的CA，Ingress-gateway和客户端TLS握手，并验证服务端证书\n8.Ingress-gateway执行TLS终止，并将HTTP流量发送给后端服务\n下面重要部分是定义cert-manager的两个重要资源对象。\n首先是Issuer：和前一个实践不同，这里的Issuer使用的是cert-manager内置的CA类型的Issuer，即在secret中存储的证书和私钥来签发证书。\n其次是证书的定义：最大的不同是，isCA是false，表示签发的不是一个CA而是一个实体证书。只是用于Ingress-gateway直接使用，不可用于签发证书，属于证书链中的最后一层，是Ingress-gateway与调用的客户端进行HTTPS通信的凭证。\n另外也配置了证书的有效期、续签时间、生成的证书存储的secret等。\n另外配置了dnsName，指定与证书相关联的域名如果客户端的，校验客户端访问的域名与 subjectAltName 扩展中该域名匹配。\n这里是配置Ingress-gateway的定义。\n协议HTTPS，TLS模式配置为SIMPLE，表示TLS终止时，只需要客户端校验服务端，服务端无需校验客户端，这也是ingress gateway 人机交互的典型配置。\nTls的credentialName配置cert-manager生成的证书的secret。\n其他Istio固有的配置这里不过多解析\n以下是解析是配置的证书内容，包括cert-manager中certificate资源对象的详细信息，配置的证书有效期是300天；在到期前10天进行证书续签。\n可以看到isCA：false，表示这是一个作用于终端的实体证书，不是一个可以再签发其他证书的CA。把envoy congfigdump出来的gateway关联的证书内容解出来，对比可见，与我们在cert-manager中通过certificate资源定义的证书的各项内容一致。包括证书有效期，是否CA、dns名等。\n这是一个端到端的测试。在网格外部，通过HTTPS协议访问Ingress-gateway，来访问网格的一个服务。访问链路上的详细流程如下：\n1.Gateway 加载cert-manager配置并生成的证书\n2.客户端HTTPS访问，并携带服务端CA\n3.从访问的输出可以看到：\n4.客户端和Ingress-gateway的TLS握手\n5.验证服务端证书，包括过期时间，SAN等\n6.最终效果是Ingress-gateway卸载了TLS请求，将HTTPS流量转化为HTTP流量发给后端服务。\n以上是我们实践的全部内容，实际项目中细节更多，时间关系并没有全部包含在以上分享中。\n附： 演讲材料(官方Slides) 官方Sched主页 ","link":"https://idouba.com/istiocon2023-cert-manager-help-enhance-security-and-flexibility-of-istio-certificate-management/","section":"posts","tags":["Istio","IstioCon","演讲","安全"],"title":"IstioCon2023：Cert-manager帮助增强Istio证书管理的安全性和灵活性"},{"body":"","link":"https://idouba.com/tags/%E5%AE%89%E5%85%A8/","section":"tags","tags":null,"title":"安全"},{"body":"\r9 月 26-28 日，由 Linux 基金会、云原生计算基金会（CNCF）主办的 KubeCon + CloudNativeCon + Open Source Summit China 2023 将在上海跨国采购会展中心隆重召开。作为全球顶级的开源和云原生盛会，本届大会以“云赋创新，无处不在”为主题，聚焦可观测、安全、平台工程、数据库、运维+性能等技术热点，邀请全球顶级技术专家、开源社区领袖和企业代表，共同探讨最新的开源云原生技术洞见、最佳实践以及来自全球的创新案例。\n作为 7月份Istio从CNCF正式毕业后云原生领域的第一次全球顶级技术盛会，全球服务网格的爱好者们除了可以参加KubeCon + CloudNativeCon + Open Source Summit China上丰富的议题外，还可以参与同场活动IstioCon 2023，业界最受欢迎的服务网格的第三届社区会议。您将在会上找到在生产环境中运行 Istio 的经验教训、实践经验，以及来自整个 Istio 生态系统的维护人员。其中就包括华为云云原生团队的两位资深技术专家，他们同时也是Istio社区成员、Istio社区指导委员会成员（Istio Steering Committee Member）。\n他们将在本期IstioCon和KubeCon上带来6场精彩的演讲。其中即包括《基于生产案例详细解析和重现Istio访问日志的各种应答标记（Response Flags）》、《cert-Manager 帮助增强 Istio 证书管理的安全性和灵活性》这种基于生产实践的技术干货，也包括《Istio数据平面的新选择：架构创新带来的全新性能体验》，《重新思考服务网格负载均衡》这样的深入技术研讨，还有《Istio毕业后的下一步发展》、《服务网格正在逐渐见证云计算更高的崛起》这些Istio和服务网格发展的讨论。\n同时作为今年5月年上市图书《Istio权威指南（上）：原理与实践》和《Istio权威指南（下）：架构与源码》的核心作者，他们也期望在会议期间和广大读者就本次演讲的服务网格相关议题、《Istio权威指南》图书中的内容、服务网格领域社区、业界和生产实践的各类问题与广大读者听众近距离交流。\nIstioCon议题分享 Istio数据平面的新选择：架构创新带来的全新性能体验 演讲嘉宾：Zhonghu Xu, Principal Engineer, Huawei Cloud\nSongyang Xie, Senior Software Engineer, Huawei Cloud\n时间：9月26号，周二 9:55-10:20\n地点：3夹层 3M3会议室\n议题简介：在像Istio这样的服务网格技术的部署中，减少数据平面代理架构引起的延迟开销已经成为网格提供者的关键问题。在本次会议中，徐中虎和谢颂杨将从操作系统的角度提出一种全新的服务网格数据平面解决方案。通过利用eBPF +内核增强，他们在操作系统中实现了原生的流量治理能力。与其他解决方案不同，这种方法显著简化了网格数据平面的转发路径，从而使数据平面转发延迟降低了60%以上。此外，它具有低资源开销和安全隔离的特点。该项目重新定义了网格数据平面，以Istiod作为控制平面，目前华为正在进行内部验证。此外，他们还将讨论服务网格的未来演变，并在不同部署场景中探索无Sidecar架构的潜力。\n听众受益：\n架构创新：从操作系统的角度引入一种新的方法来应对服务网格挑战，为基础架构之间的协作创新提供了一个很好的例子和灵感。 为 Istio 提供一个新的数据平面选项，以满足高性能应用场景的需求。 cert-Manager 帮助增强 Istio 证书管理的安全性和灵活性** 演讲嘉宾：Chaomeng Zhang, Architect, Huawei Cloud\n时间：9月26号，周二 11:50-12:15\n地点：3夹层 3M3会议室\n议题简介：Istio基于零信任安全模型构建了一个零信任的网络基础设施，在这个零信任的网络基础设施上，证书是基础关键的能力。在本次分享中，将会介绍Istio当前基于证书的安全能力遇到的挑战，并详细介绍cert-manager提供的解决方案。包括cert-manager的架构、配置、流程。并详细介绍两个Istio和cert-manager结合的实践：\n基于cert-manager对接PCA，使用PCA签发的从属CA作为Issuer，签发网格根证书，实现网格透明的双向认证。 cer-manager根据配置描述签发证书作为Ingress-gateway证书，实现网格入口TLS终止。自动续签证书，减少证书问题对系统的影响。 听众受益：\n了解零信任安全模型的概念、架构和Istio零信任安全模型的实现，即零信任安全网络基础设施。 Istio零信任安全网络各部分功能中证书的用法。以及当前用法下碰到的挑战和需求。 cert-manager的架构、概念、机制和流程。 创建PCA证书层次结构，基于cert-manager签发Istio根证书的实践和详细流程。 基于cert-manager签发Istio Ingress-gateway证书的实践和详细流程。 KubeCon议题分享 Istio 毕业后的下一步 演讲嘉宾：Zhonghu Xu, Principal Engineer, Huawei Cloud\n时间：9月27日，周三 11:50-12:25\n地点：3层 3M3会议室\n议题简介：在本次会议中，Istio维护者Zhonghu Xu将介绍当前Istio生态系统以及最近版本中Istio的支持内容：\n新的数据面模型 “Ambient Mesh” 的状态如何？ Gateway API标准对Istio的影响如何？ 在Istio孵化一年后迅速毕业之后，Istio将如何发展，这是否会改变服务网格模式？ 服务网格正在逐渐见证云计算更高的崛起 演讲嘉宾：Zhonghu Xu, Principal Engineer, Huawei Cloud\nFanbin, Software Engineer, Chinamobile\nXi Ning Wang, Senior Technical Expert, Alibaba Cloud\nHuailong Zhang, 云原生软件开发工程师, Intel China\nFei Pei, Senior Technical Expert, Netease\n时间：9月27号，周三 13:55-14:30\n地点：2层 会议室3\n议题简介：本提案邀请来自英特尔中国、华为云、阿里云、网易和中国移动通信公司的服务网格专家进行关于服务网格的专题讨论。讨论主题包括但不限于以下内容：\n服务网格的发展现状（包括但不限于对服务网格项目、各种供应商、功能和技术的讨论） 服务网格在云原生生态系统中的趋势和定位（包括但不限于服务网格在云原生中的定位、技术演进、用户需求和用户最关注的功能） 服务网格在国内外实际用户场景中的项目落地（根据用户反馈的痛点和兴趣展开讨论。例如，无边车的最佳实践和挑战，服务网格与传统框架之间的集成问题等） 听众受益：这场圆桌讨论邀请了行业内主流的云计算厂商以及服务网格领域的知名专家。主要讨论话题包括服务网格的技术趋势、未来发展方向以及实践应用。对于已使用或计划使用服务网格的开发者和用户来说,这场讨论具有很高的参考价值和意义。\n基于生产案例详细解析和重现 Istio 访问日志的各种应答标记 演讲嘉宾：\nChaomeng Zhang, Architect, Huawei Cloud\n时间：9月27号，周三 15:50-16:25\n地点：3层 305B会议室\n议题简介：访问日志是应用系统运维的重要手段，可以有效地帮助我们进行问题的定位定界。Envoy在访问日志中引入了应答标记Response Flag，辅助HTTP响应码，进一步描述访问或连接的细节，发生的问题，方便运维人员针对性地解决问题。如发生了503错误后，通过503 UH 503 UF 503 UC 503 NC 等区分各种不同的503产生的原因，提供线索让运维人员去解决。但是Istio社区和Envoy社区对应的官方文档中对Response Flag的描述非常有限，不足以支撑使用者理解其含义，也不能理解服务的问题，进而修复问题。在本次演讲中我们会在一个最简单的环境上构造复现条件，逐个重现这些典型的Response Flags，基于实际案例理解其表达的含义、产生条件。和每种Response Flag提示的问题和针对性修复方法。\n听众受益：业内首次详细全面地重现UH、UF、UC、DC、URX、UPE、DPE、NC、NR、DI、FI、UT、RL等Envoy典型的Response Flag。详细描述每种Response Flag的重现条件、重现步骤、图示交互效果、真实的访问输出、真实的上下文日志解析。帮助用户了解每种Response Flag的含义、特征、表达的系统问题和针对性修复方法。听众了解每个实践后，作为参照可以帮助理解实际生产中碰到的Response Flag，并解决生产中的类似问题。\n重新思考服务网格负载均衡 演讲嘉宾：Zhonghu Xu, Principal Engineer, Huawei Cloud\n时间：9月27日，周三 15:50-16:25\n地点：2层 会议室3\n议题简介：随着服务网格成为服务治理的事实标准，越来越多的企业正在逐步采用服务网格。像Istio这样的服务网格在应用侵入性方面具有很大优势。然而，用户觉得还远远不够令人满意，特别是在sidecar资源开销和负载均衡方面。服务网格的未来会是什么样子？负载均衡会是怎样的？服务网格和传统负载均衡器有什么区别？在本次会议中，中虎将带领大家重新思考云原生应用网络的未来。他将讲解能够导致群体行为的最短队列加入（JSQ）算法。然后介绍著名的二选一（Choice-of-2）算法，可以消除群体行为。但这些还不够，还应考虑来自“服务器报告的利用率”的更多因素。服务网格可以实现对现代云原生应用网络的最高效流量调度。\n听众受益：\n学习Istio的工作原理, 尤其是负载均衡特性\n学习用户对于Istio的期待以及使用挑战\n学习Istio负载均衡的演讲方向\n图书《Istio权威指南》 《 Istio 权威指南》由华为云云原生团队出品,旨在打造业界最系统权威的 Istio 图书。书籍分为上、下两册，上册包括原理篇、实践篇；下册包括架构篇和源码篇，总计26章。\n权威详实内容获Istio社区和Istio TOC Lin Sun和John Howard代表Istio社区强力推荐 450+详实原创图表解析关键原理、架构、模型和代码流程，归纳易错技术点 国内首本系统详细解析Istio和Envoy架构和源码的技术书籍，Istio和Envoy社区核心维护者周礼赞强力推荐 大量原创实践内容覆盖典型应用和常见疑难问题，指导客户解决生产问题 华为云CTO、CNCF CTO、信通院专家、IEEE/CCF Fellow、CNCF中国区总监、华为云首席产品等顶级领域专家推荐 9月26日-28日，KubeCon + CloudNativeCon + Open Source Summit China 2023上海见！\n","link":"https://idouba.com/meet-the-authors-in-shanghai-kubecon.md/","section":"posts","tags":["Istio权威指南","图书","Istio","Kubecon"],"title":"【MeetTheAuthors】Istio社区指导委员会成员携新书《Istio权威指南》和6场技术演讲亮相KubeCon"},{"body":"","link":"https://idouba.com/tags/istio%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/","section":"tags","tags":null,"title":"Istio权威指南"},{"body":"","link":"https://idouba.com/categories/istio%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/","section":"categories","tags":null,"title":"Istio权威指南"},{"body":"","link":"https://idouba.com/tags/%E5%9B%BE%E4%B9%A6/","section":"tags","tags":null,"title":"图书"},{"body":"","link":"https://idouba.com/categories/%E5%9B%BE%E4%B9%A6/","section":"categories","tags":null,"title":"图书"},{"body":" 中午开车接上老婆孩子，急急忙忙和二十多年的兄弟昌子见了一面，吃了个饭。\n从高德上收到六和塔附近汇合点捞上昌子，到茅家埠附近的弄堂里吃饭，加一起也没有多长时间。傻叉非要说等下和等下要和在西湖边溜达的同事回合，都是从北京铁路系统来疗养的，晚上还要一起返回无锡去。\n车上丫坐副驾驶位置总是嘚啵嘚的diss哥开车这不对哪不对的，靠。\n饭桌上，我们聊起了咱们当年。中专入学刚认识的时候，你十五岁，我也十五岁。你好像都有剃须刀了，哥那会儿毛还没长出来呢。应该是你们北京发达地区吃得好，长得快。哥来自偏远地区还只吃面，长得慢。\n那年你93斤，我88斤。四年后我们毕业了，你123斤，我也123斤。哪天我们约好去学校门口过称比体重，我灌了一大杯水，才比你多了几两赢了你。就因为这几两险胜，你才答应，我以后娶了媳妇你管叫嫂子。因为我生日只大你一个月，就冲你胡子出来早四年里总是嘚瑟地一直让我喊你哥。临到最后你哥我才扳回一局，那叫一个爽，虽然靠灌水作弊，但感觉就是正义的胜利。但现在我们对坐在饭桌上，我还是那会儿的123，你都搞到160了。行行行，还是你赢了。\n毕业后，你回北京，在四道口厂子里焊钢轨，哥分配到了铁一局修铁路。我记得那时候兴毕业留言册，你给哥写的是盼着以后来北京修地铁，咱就可以常见面了。\n哥修铁路干的第一个工程是秦沈客运专线，工地在沈阳附近。从东北回西安我总会在北京停下，来榆树庄找你玩几天。那会儿工地上有野外补助，加上哥参与修的那条线当时号称是中国第一条高铁，工地奖金高。哥钱包里掏出的票子让你这北京土著也喊叫“你这是来北京消费的啊”。对了，哥当时那个帆布钱包还是你在学校送我的，后面用了很多年。\n记得当时班上自我介绍的时候你说你们村在卢沟桥边上。小时候总去卢沟桥上玩，把卢沟桥的石狮子都让你们摸的秃噜皮了。我好像讲了我们村附近埋着汉武帝，边上有个茂陵博物馆里还有卫青和霍去病的墓。后来我记得你拿着个破地图给我指说你们村离天安门可近了，比房山那些地方近多了。\n二年级时候咱们班倒洛阳重机厂的车间实习。一个多月的实习时间里在车间干了啥我到现在一点印象都没有，只记得你带我和几个人在重机厂的龙门吊下面打扑克。印象更深的厂子里有个带假山的水池，水池里有些小鱼，你后来天天带着我在那个水池里捞鱼。不知从哪里搞来个破草帽，用根绳子系在一木棍上，每次中饭从食堂带了些米饭放在草帽里。最终一个鱼毛都没有捞到，却就这样干了一个月。想想咱们得同龄人那时候正在高二紧张地学习，我们天天在摸鱼也是很有意思。那时候咱十六岁了吧，我给我儿子说你幼儿园前也喜欢在小区的水池里捞鱼，也是鱼毛都捞不到，也是天天都要去。\n饭桌上你兴奋地拿出手机里的照片，一个五毛的钢镚儿本人立在高铁的桌子上。你给哥吹牛说你们把100米一段的钢轨焊成500米，高铁才跑的这么稳。哥说哥参与修建了中国第一条高铁，只是后来大家说法变了，我们当时修的那个叫第一天客运专线，几年后修成的你们北京到天津那个被认定为第一条高铁。我们都曾经为为中国铁路事业做贡献，咱们同学很多人还在持续贡献，哥惭愧当了逃兵。\n因为等会儿要开车，下午还要上班，不能陪你喝。给你自个儿要了啤酒。一手拿着酒杯，掏出的铁路职工乘车证给哥炫耀这次北京到无锡疗养全免票。你显摆优越感的时候是不是忘了你哥也从铁路上出来的，当年也是持证的，拿着这个证从沈阳工地到西安免费火车瞎溜达，中间在北京还歇歇脚找你玩了半个月。白天你去四道口焊钢轨，哥跟着大老吴坐公交车从榆树庄出来，到西单劝业场，大老吴去那里一个写字楼跟着一堆喊口号的人搞他那个5000一个东陵附近墓地的房地产买卖，哥就在上面的新华书店看一天书，然后晚上回到榆树庄在你家蹭饭睡觉。\n有个夏天那是东北工地都结束了，我们在家里等通知，哪里有活接着去哪里。有个啥事来着，我这个免费乘车证还能用，就又来北京找你玩，好像就住在四道口你们职工宿舍里。天天中午跟着你们一帮人去附近馆子吃喝，你们还总撩骚人家上菜的姑娘，应该都是川菜馆子里的川妹子吧。\n除了干铁路时候经常来北京，哥后来转了赛道去北京出差的次数也很多。基本上每次办完事，不管在大北京的那个角落，得一点空都会窜到丰台来找你。而你算上这次也才是哥第二次接待你。\n第一次是在西安，比这次还要仓促。哥那会儿正在复习考研，记得在西电的阶梯教室里接到你的电话。你说你们在四川绵阳有个啥活干完了坐火车回北京，火车会路过西安。你们是铁路上的通票，不能随意下车停留。但是过兄弟跟前，就想着要怎么能见一面。于是我们就约定再西安站的站台上，你告诉我到站时间。哥撂下电话就跑到附近超市买了点小米柿饼这些看着像陕西特产又不太贵的东西搞了一包。准备考研的四个月里你哥完全没有收入，之前几年在西安边自学点东西，边打点零工，干的也是些跑腿的活，在铁路上挣得钱也霍霍的差不多了，日子比较拮据，大方给你买点东西也大方不到哪里去。结账时搞了几个易拉罐的啤酒和小瓶白酒塞了进去。\n忘了当时靠站台票还是我的铁路职工乘车证，我好像给检票的说了我们有个铁路的兄弟要从这里过，他就告诉我站台让进去了，天下铁路人是一家啊。火车停下来，你穿着秋裤下了车。车在西安站就停车这几分钟，我们对着干了酒，没聊几句，真的都在酒里了。哥干了剩下的半瓶酒，目送你的火车继续开动。你继续北上回家的路，我也要回我的出租屋里继续复习。\n咱那天见面后有一段糗事哥当时没给你说，后面几次北京见面我也从来没有给你说过。因为你的火车到西安的时间已经十一点多了，从火车站回我那个出租屋所在的村子的公交车停运了，哥只能从火车站打了个车回去。当时没有钱，吃饭都紧张，每月手里固定的一点现金抠着花。在超市买完东西加上咱的酒，口袋里好像只剩下十几块零五毛了。在出租车计价器跳到那个十几块的时候，摸摸口袋就提前下车了。走着回去走了多少里不知道，只是大半夜在当年号称贼城的街道一个人走路有点害怕，有一段还很偏，回到屋里躺到床上时估计你的那个慢车都至少开到华山了。\n遗憾怎么也说不动你这个傻叉再留一晚，我家附近过了一桥下面酒店哥都给你看好了。你干了两瓶啤酒，哥只赔了两杯西瓜汁。如果能留下来，咱可以重复六七年前在北京牡丹园那一顿，牛二加土豆丝，无限续杯。搞得人家饭店打烊了，老板催了几次还不走，气的老板把电扇给关了。最终也没有赶走这俩光着膀子流着大汗吃喝的流氓。吃喝到一两点在酒店楼下又扯了一个多小时，我回到房间老婆孩子早睡着了，而你要赶回丰台估计就更晚了。第二天还是第三天我们刚起床，你又拿着小米粥和牛肉火烧过来了，这么远就为给哥和家人送个早点。\n当年你比较时髦，哥比较土。四年级时候你开始练滑板，还领着我在学校后面找有没有哪个店能给你的T恤上印上“滑板梦之队”的字，现在你别说印“滑板梦之队”了，你就是让人给你印个“滑板屎之队”淘宝两天就能给你寄过来。那时候这东西好像不是很容易，忘了你最终装逼成功没。\n最羡慕你搭飘逸的齐耳的碎发，对比哥入学时大家评价又矮又挫小黄毛没发育好的样子。现在哥摸摸渐秃的头顶，对你夹杂着大片白发的大叔平头也是满满的敬畏和羡慕。\n我的好兄弟，这些个年，这些个十年。看着很多都变了，很多都没变。\n多保重，希望咱都顺当平安，祝愿咱们的同学都顺顺当当。\n","link":"https://idouba.com/meet-changzi-in-hangzhou/","section":"posts","tags":["随笔","铁路"],"title":"西湖会昌子"},{"body":"","link":"https://idouba.com/tags/%E9%93%81%E8%B7%AF/","section":"tags","tags":null,"title":"铁路"},{"body":"本节书摘来自华为云原生技术丛书《Istio权威指南（上）：云原生服务网格Istio原理与实践》一书原理篇的第7章 异构基础设施，7.3节多集群的关键技术，更多内容请参照原书。\n第7章 异构基础设施 对多云、混合云、虚拟机等异构基础设施的服务治理是Istio重点支持的场景之一。为了提高服务的可用性，避免厂商锁定，多云、混合云甚至虚拟机和容器混合部署都成为常态，因此Istio社区将多集群、混合服务治理作为了重点发展方向。根据Flexera 2022 State of the Cloud Report，89%的组织选择了多云，随着越来越多的组织寻求使用最佳解决方案，混合云和多云有望实现持续增长，跨云的服务通信、服务治理将成为困扰开发人员的主要问题。本章从Istio的角度，重点解读Istio针对多集群服务治理提供的能力及实现原理。\n7.3 多集群的关键技术 多集群相对于单集群，其服务在跨集群互访时比较复杂，其中最棘手的问题有以下两个。\n◎ 异构环境下的DNS解析：如何解析多集群的服务域名。\n◎ 多网络环境下的服务跨网络访问：东西向网关如何转发跨网络的服务访问。\n7.3.1 异构环境DNS 如图7-9所示为多集群、虚拟机异构服务网格典型的服务访问拓扑。\n​ 图7-9 多集群、虚拟机异构服务网格典型的服务访问拓扑\n在Kubernetes中，Kube-dns只负责集群内的服务域名解析，对其他集群或者传统虚拟机服务的域名解析束手无策。为此，我们必须借助其他技术方案，通过级联DNS的方式向集群内的应用提供服务域名DNS解析的能力。级联DNS一般通过级联上游中心式的DNS服务器实现，但是如何向级联DNS服务器注册服务的DNS记录（DNS SRV）依然比较困难。\n另外，Istio依赖名为“istio-coredns”的CoreDns扩展插件，进行Remote集群服务的域名解析。这强制要求用户创建ServiceEntry以向istio-coredns注册服务，其中ServiceEntry中服务域名的表示形式被约定为..global，同时需要修改Kube-dns的配置，使其级联到istio-coredns。除此之外，还需要用户自己管理服务IP地址的分配。由此可见，在生产中使用这种方案非常困难。\n为降低在多集群、虚拟机等异构环境下使用Istio的难度，Istio在1.8版本中实现了DNS代理的功能。DNS代理的用法更加简单，无须用户额外创建任何配置。因此，Istio彻底废除了istio-coredns插件，不再需要为其他Kubernetes集群里面的服务在本地集群中创建影子服务。\nDNS代理完全是Istio内部实现的一个DNS服务器，负责解析所有应用程序发送的DNS解析请求。它的上游级联DNS默认为Kube-dns。DNS代理在提供服务时所需的DNS Records由Istiod通过NDS（NameTable Discovery Service）发送，其中NDS完全是基于xDS协议实现的。Istiod负责监听服务网格内部所有的服务（既包括Kubernetes服务，也包括ServiceEntry服务），然后根据服务的地址及域名等信息构建DNS记录。NDS配置的发送采用异步通知的机制，任何服务的更新都会及时触发NDS配置的发送。从功能上来讲，DNS代理完全分担了Kube-dns的压力，而且支持远端集群及ServiceEntry服务的域名解析。\n总之，本地DNS代理有三种优势：①由于DNS代理是Pilot-Agent中的子模块，所以Sidecar自动包含此功能，无须单独部署；②它与应用被部署在同一Pod中，属于同一网络空间，因此可以大大降低应用的DNS解析时延；③DNS代理属于分布式部署，可以分担中心式Kube-dns服务器的压力，避免因为Kube-dns过载而导致整个集群的可用性下降。\nDNS代理的基本工作原理如图7-10所示，流程如下。\n（1）应用程序在访问目标服务时，首先发起DNS解析。Istio通过Iptables规则拦截应用的DNS解析请求，并将其转发到本地15053端口，15053端口正是DNS代理监听的端口。\n（2）DNS代理在接收到DNS解析请求后，首先检索本地的DNS记录，如果本地存在，则直接返回DNS响应，否则继续向上游级联DNS服务器（Kube-dns）发起解析请求。\n（3）本集群的Kube-dns首先在本地查找DNS记录，如果找到，则直接返回DNS响应，否则会遵循标准的DNS配置（/etc/resolv.conf），将DNS请求转发到上游级联DNS服务器。这里的上游级联DNS服务器可能是公有云厂商自有的DNS服务器。\n​ 图7-10 DNS代理的基本工作原理\nDNS域名与IP地址映射表\n如图7-11所示，Istiod通过监听所有集群的Kube-apiserver，获取整个服务网格中的所有Service/ServiceEntry，并且为ServiceEntry自动分配IP地址。DNS代理通过NDS（Istio扩展的xDS协议）从Istiod中获取所有服务的DNS域名与IP地址的映射关系表，并将其缓存在本地。\n​ 图7-11 DNS代理NDS的发现原理\n对于Kubernetes原生的Service来说，DNS解析直接使用其ClusterIP。当然存在这么一种情况，Service B在集群1和集群2中均存在，但是具有不同的ClusterIP地址，这时应该选择哪个地址作为服务的地址呢？答案是：Istiod选择与DNS Proxy在同一集群服务中的ClusterIP作为Service B的IP地址。如果DNS Proxy在集群1中，则Istiod选择集群1的ClusterIP 10.96.0.10作为Service B的地址；如果DNS Proxy在集群2中，则Istiod选择集群2的ClusterIP 10.10.0.10作为Service B的地址。也就是说，在多集群场景中，同一个服务名在不同的集群中可能被解析成不同的IP地址，当然这里完全不影响服务的访问，因为Istiod在生成监听器及路由匹配条件时，也遵循优先选择代理所在集群的服务ClusterIP的原则。\nServiceEntry一般用来表示虚拟机上或者服务网格外部的服务，DNS或STATIC解析类型的ServiceEntry本身并没有IP地址，Istiod会从保留的E类地址（240.0.0.1～255.255.255.254）中为其随机分配一个假的IP地址，并发送给DNS代理。当应用访问如下ServiceEntry指定的mymongodb.somedomain域名时，实际上DNS代理会返回一个240.240.x.x的IP地址。STATIC类型的服务在Envoy中的Cluster类型为EDS，因此Envoy会将请求发往2.2.2.2或者3.3.3.3中的任意一个目标实例。\n1apiVersion: networking.istio.io/v1beta1 2kind: ServiceEntry 3 metadata: 4 name: external-svc-mongocluster 5 spec: 6 hosts: 7 - mymongodb.somedomain 8 ports: 9 - number: 27018 10 name: mongodb 11 protocol: MONGO 12 location: MESH_EXTERNAL 13 resolution: STATIC 14 endpoints: 15 - address: 2.2.2.2 16 - address: 3.3.3.3 因此，我们能够清晰地看到DNS代理的设计大大降低了服务网格中域名解析的难度，尤其是在多集群、虚拟机异构环境下，大大降低了用户运维部署中心式DNS服务器的复杂性。同时，DNS代理能够分解集群中的Kube-dns压力，减小爆炸半径，同时提高应用DNS解析的响应速度。当然，鱼和熊掌不可兼得，DNS代理在简化用户运维、管理的同时，不可避免地增加了一些Sidecar内存开销。\n7.3.2 东西向网关 Istio除了有PASSTHROUGH模式，还有AUTO_PASSTHROUGH模式。PASSTHROUGH模式指Envoy将客户端TLS握手的SNI信息作为路由匹配条件，选择TLS路由进行流量的转发。PASSTHROUGH模式一般绑定VirtualService，根据SNI匹配条件选择目标服务。AUTO_PASSTHROUGH模式与PASSTHROUGH类似，区别在于AUTO_PASSTHROUGH不需要绑定相应的VirtualService，而是根据SNI信息直接选择目标服务和端口路由，这里隐含的要求是在源端发送的SNI信息中包含目的服务名称和端口。在ISTIO_MUTUAL模式下，SNI默认为\u0026lt;outbound_.... .svc.cluster.local\u0026gt;格式，因此AUTO_PASSTHROUGH属于更加自动的透传模式。\n进行跨网络通信时，东西向网关必须工作在AUTO_PASSTHROUGH模式下，并默认开启15443端口，所有跨网络的流量都流入网关的同一个端口，因此严重依赖AUTO_PASSTHROUGH模式根据流量特征将其路由。\nIstio东西向网关在工作时使用基于SNI的路由，它根据TLS请求的SNI，自动将其路由到SNI对应的Cluster（Envoy概念）。因此，非扁平网络的跨网络访问要求所有流量都必须经过TLS加密。AUTO_PASSTHROUGH模式的好处是不要求在东西向网关上创建每个服务对应的VirtualService。试想：进行跨网络服务访问时，先创建VirtualService对用户来讲是多么糟糕的体验！\n在 AUTO_PASSTHROUGH模式下，Istio为东西向网关配置“envoy.filters.network.sni_ cluster”的网络过滤器。这里特别说明一下，sni_cluster过滤器的作用主要是根据请求中的SNI值，获取转发的目标Cluster。所以，也可以说东西向网关在跨网络访问中工作在SNI-DNAT模式下。在图7-12中，天气预报服务调用时，Istio设置使用的SNI是“outbound_. 3002_..forecast.weather.svc.cluster.local”，东西向网关接收到此流量后，经过sni_cluster过滤器的过滤，选择同名的Cluster“outbound.3002_._.forecast. weather.svc.cluster.local”转发。这种命名格式的Cluster比较特殊，在普通应用的Sidecar中不存在，Istio只会给东西向网关这种包含AUTO_PASSTHROUGH模式的网关生成这种类型的Cluster。\n​ 图7-12 跨网络访问\n下面是典型的基于SNI的路由配置Gateway的示例，每个集群的东西向网关都能接收并转发所有带“*.local”后缀的服务的请求：\n1apiVersion: networking.istio.io/v1beta1 2kind: Gateway 3metadata: 4 name: cross-network-gateway 5spec: 6 selector: 7 istio: eastwestgateway 8 servers: 9 - port: 10 number: 15443 11 name: tls 12 protocol: TLS 13 tls: 14 mode: AUTO_PASSTHROUGH 15 hosts: 16 - \u0026#34;*.local\u0026#34; ","link":"https://idouba.com/multi-cluster-of-the-definitive-guide-istio/","section":"posts","tags":["Istio权威指南","图书","Istio","多集群"],"title":"Istio多集群关键技术 –《Istio权威指南》书摘"},{"body":"","link":"https://idouba.com/tags/%E5%A4%9A%E9%9B%86%E7%BE%A4/","section":"tags","tags":null,"title":"多集群"},{"body":"","link":"https://idouba.com/tags/authorizationpolicy/","section":"tags","tags":null,"title":"AuthorizationPolicy"},{"body":"本节书摘来自华为云原生技术丛书《Istio权威指南（上）：云原生服务网格Istio原理与实践》一书原理篇的第5章 服务安全的原理，5.4节AuthorizationPolicy（服务授权策略），更多内容请参照原书。\n第5章 服务安全的原理 Istio以非侵入方式透明地提供面向应用的安全基础设施。在Istio中有两种不同的认证方式：①基于mTLS的对等身份认证；②基于JWT（JSON Web Token）令牌的服务请求认证。本章重点介绍这两种认证方式，以及基于这两种认证方式的细粒度的服务访问授权，会详细介绍其中认证、授权的通用原理、模型，以及Istio基于服务网格形态的实现原理和机制。本章还会详细介绍如何通过配置PeerAuthentication、RequestAuthentication和AuthorizationPolicy使用这些认证、授权能力。\n5.4 AuthorizationPolicy（服务授权策略） 认证的大部分应用场景最终是基于授权的访问控制，以上两种认证策略也大多配套本节的授权策略使用。从Istio 1.4开始引入的AuthorizationPolicy替代了之前ClusterRbacConfig、ServiceRole和ServiceRoleBinding三个对象来进行授权配置，避免了配置多个API的麻烦，AuthorizationPolicy的自身功能也非常丰富。\n5.4.1 入门示例 在如下示例中定义了作用于forecast负载v2版本的AuthorizationPolicy：来自cluster.local/ns/weather/sa/frontend的服务，当其携带的请求头域group是admin，并且通过PUT和POST方法访问目标服务才被允许时，其他条件的访问都会被拒绝：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: forecast 5 namespace: weather 6spec: 7 selector: 8 matchLabels: 9 app: forecast 10 version: v2 11 rules: 12 - from: 13 - source: 14 principals: [\u0026#34;cluster.local/ns/weather/sa/frontend\u0026#34;] 15 to: 16 - operation: 17 methods: [\u0026#34;PUT\u0026#34;,\u0026#34;POST\u0026#34;] 18 when: 19 - key: request.headers[group] 20 values: [\u0026#34;admin\u0026#34;] 5.4.2 配置模型 AuthorizationPolicy的配置模型如图5-16所示，主要包括以下三部分。\n◎ selector：描述策略应用的目标对象。\n◎ rules：描述详细的访问控制规则。\n◎ action：定义满足rule规则后执行的动作是允许还是拒绝。\n​ 图5-16 AuthorizationPolicy的配置模型\nrule的规则主体由from、to和when构成，从字面上也能大致理解其各自的意思。这样，AuthorizationPolicy的配置模型也如图5-16所示，即通过selector给目标负载定义授权策略：对于从from发起的访问to的请求，当满足在when里定义的Condition条件时，执行action动作。\n5.4.3 配置定义 AuthorizationPolicy包括三个核心配置：selector、action和rules，selector、action相对简单，本节重点介绍rules的灵活用法。\n选择器selector是个通用的结构，用于描述AuthorizationPolicy生效的负载，和前面两个认证策略的选择器用法类似。比如，在5.4.1节的示例中，selector通过app和version两个标签将AuthorizationPolicy应用到forecast v2版本的负载上：\n1spec: 2 selector: 3 matchLabels: 4 app: forecast 5 version: v2 授权动作action表示匹配条件后执行的动作，取值有ALLOW、DENY、AUDIT和CUSTOM。其中的默认值是ALLOW，表示匹配条件的请求授权允许；类似的DENY表示匹配条件的请求授权禁止；AUDIT表示匹配的请求会被审计，需要对应的插件Plugin支持；CUSTOM表示可扩展来进行授权判定，配合provider字段定义外置的授权扩展。\nAuthorizationPolicy的核心主体rules是一个规则数组，描述匹配请求的一组规则。如果在这组规则中有一个规则匹配请求，则匹配成功。如果没有设置rules，则表示总是不匹配。rules数组中的每个规则rule都通过from、to和when三部分描述。\n1）from（请求来源） from描述请求来源的相关属性，可以不设置，表示匹配所有来源的请求。当在from中包含多个属性匹配条件时，要求满足所有条件。AuthorizationPolicy的from条件如表5-1所示，可以看到，为了方便规则定义，几个条件字段除提供了常用的正向匹配，也都提供了负向的匹配条件。\n如下配置表示来源是cluster.local/ns/weather/sa/frontend身份的服务，并且IP地址不是10.2.0.0/16网段，或者来自default命名空间的请求。其中，两个from数组间是或关系，from元素内部的各个条件间是与关系。\n1from: 2 - source: 3 principals: 4 - cluster.local/ns/weather/sa/frontend 5 notIpBlocks: 6 - 10.2.0.0/16 7 - source: 8 namespaces: 9 - default 表5-1中principals、notPrincipals、namespaces和notNamespaces等条件需要启用mTLS才能生效。在如下授权条件定义中，来自bff的命名空间的服务访问会被拒绝，但如果未启用mTLS，则因为源命名空间条件匹配不成功，不满足这个授权条件，该访问被允许通过：\n1spec: 2 action: DENY 3 rules: 4 - from: 5 - source: 6 namespaces: [\u0026#34;bff\u0026#34;] 注意：在AuthorizationPolicy中如果配置了依赖mTLS才能提取的属性，但是未启用mTLS，则会导致请求总被允许。\n另外，表5-1中的两个字段ipBlocks/notIpBlocks和remoteIpBlocks/notRemoteIpBlocks，前者匹配到达服务网格的IP地址，后者匹配真正的访问者的源IP地址。当经过七层代理时，源IP地址可以从X-Forwarded-For头域中获取；当经过四层代理并启用了代理协议时，源IP地址可以从代理协议中获取。在实际的授权配置中，remoteIpBlocks/ notRemoteIpBlocks的应用更广泛一些。\n2）to（目标服务操作） to条件描述对目标服务的操作属性。若不设置to条件，则表示匹配所有操作。AuthorizationPolicy的to条件如表5-2所示，匹配请求中的对应信息。to条件也提供了规则的正向匹配和负向匹配的字段定义。\n如下示例表示匹配host是cloudnative-istio.book后缀的GET请求，或路径不是/admin前缀的PUT或POST请求：\n1to: 2 - operation: 3 methods: 4 - GET 5 hosts: 6 - \u0026#39;*.cloudnative-istio.book\u0026#39; 7 - operation: 8 methods: 9 - PUT 10 - POST 11 notPaths: 12 - /admin* 3）when（请求条件） when描述请求的附加条件，当不设置时，表示匹配任何条件的请求。AuthorizationPolicy的when条件如表5-3所示，基于请求的内容可以进行丰富的条件定义。类似from和to条件，when中的条件表达式也提供了针对特定属性的正向匹配和负向匹配。\n如下配置条件匹配HTTP请求头域中版本标识是v1和v2的请求：\n1 when: 2 - key: request.headers[version] 3 values: [\u0026#34;v1\u0026#34;, \u0026#34;v2\u0026#34;] 有些条件，例如请求主体身份，既可以在from中通过requestPrincipals定义，也可以在when中通过request.auth.principal定义。同样，when中的source.principal和source.namespace也需要启用mTLS，当未启用时，也会碰到前面from对应字段类似的问题。source.ip和remote.ip的用法与前面的ipBlocks/notIpBlocks和remoteIpBlocks/ notRemoteIpBlocks完全相同。\n5.4.4 典型应用 AuthorizationPolicy可以配置的信息很丰富，多种条件搭配能支持的场景也非常多，下面通过几个典型场景了解如何解决我们的实际问题。\n1. 认证身份访问和非认证身份访问 在AuthorizationPolicy中可以通过将source的principals设置为通配符“*”来表示只匹配认证的身份。在如下示例中只有认证的身份可以通过多个HTTP方法访问weather命名空间的服务：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: all-method-for-identity 5 namespace: weather 6spec: 7 action: ALLOW 8 rules: 9 - from: 10 - source: 11 principals: [\u0026#34;*\u0026#34;] 12 to: 13 - operation: 14 methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;DELETE\u0026#34;] 将soucre设置为空，表示所有认证和非认证的身份均可访问。在如下示例中，所有身份都可以通过GET方式访问开放的接口：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: get-only-for-unidentified 5 namespace: weather 6spec: 7action: ALLOW 8 rules: 9 to: 10 - operation: 11 methods: [\u0026#34;GET\u0026#34;] 2. TCP服务访问授权 Istio授权主要应用于应用的访问授权，可以看到rule的条件大多定义在HTTP上。但也有部分条件定义在TCP上，可以配置对TCP的流量进行授权管理。在如下示例中，只允许来自特定网段的服务访问目标forecast的3002端口：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: tcp-auth-policy 5 namespace: weather 6spec: 7 selector: 8 matchLabels: 9 app: forecast 10 action: ALLOW 11 rules: 12 - from: 13 - source: 14 ipBlocks: [\u0026#34;10.2.0.0/16\u0026#34;] 15 to: 16 - operation: 17 ports: [\u0026#34;3002\u0026#34;] 3. JWT服务请求授权 基于认证的授权不但可以是常用的基于证书认证的身份标识，也可以是基于服务请求的认证。在如下示例中，只有携带特定认证标识的请求才可以访问目标服务的特定接口，这个标识既可以通过when条件的request.auth.principal配置，也可以通过from条件的requestPrincipals直接配置：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: forecast 5 namespace: weather 6spec: 7 rules: 8 - from: 9 - source: 10 namespaces: [\u0026#34;default\u0026#34;] 11 to: 12 - operation: 13 methods: [\u0026#34;GET\u0026#34;] 14 paths: [\u0026#34;/list\u0026#34;] 15 when: 16 - key: request.auth.claims[iss] 17 values: [\u0026#34;weather@cloudnative-istio.book\u0026#34;] 4. 服务网格入口流量访问授权 AuthorizationPolicy除了可以用来控制内部服务间的访问，还可以用来控制服务网格出入流量的授权管理。在如下示例中可以只允许特定IP段的外部流量通过网关访问服务网格管理的服务，AuthorizationPolicy的selector选中Ingress-gateway，将策略应用到网关上。当然，这类基于源IP地址的访问授权的前提是保证网关能获取访问的源IP地址：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: ingress-authz-policy 5 namespace: istio-system 6spec: 7 selector: 8 matchLabels: 9 app: istio-ingressgateway 10 action: ALLOW 11 rules: 12 - from: 13 - source: 14 ipBlocks: [\u0026#34;10.2.0.0/16\u0026#34;] 5. 全局授权控制 Istio中基于优先级的授权策略可以方便地支持以下全局授权控制场景。\n（1）以白名单方式授权。在比较严格的授权管理场景中，我们经常采用白名单的方式进行授权管理，即只允许名单中的服务访问，对其他服务都拒绝。在如下示例中，不配置AuthorizationPolicy中的规则rules，就不会匹配任何条件。如图5-17所示，这个策略会拒绝所有的授权访问，只有特定的条件开启了授权，才允许访问。\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: allow-nothing-for-whitelist 5namespace: weather 6spec: 7 action: ALLOW ​ 图5-17 以白名单方式授权\n（2）临时关闭所有授权。权限管理在大多数时候都是极其复杂的，多变、复杂的业务需求落实在灵活的授权体系上，有时还会出现各种权限漏洞问题，比如一些本来不应该有权限的访问莫名地被授权允许了。如果是一个重要的服务或接口有安全漏洞，那就需要强制将所有授权都临时关闭。\n在如下AuthorizationPolicy配置中，rules的空结构表示匹配所有请求，而动作是拒绝，结果如图5-18所示，不管其他AuthorizationPolicy的内容是什么，通过这个策略就可以拒绝所有的访问授权：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: authz-deny-all 5namespace: weather 6spec: 7 action: DENY 8 rules: 9 - {} ​ 图5-18 临时关闭所有授权\n（3）临时开启所有授权。除了上面可以通过一个全匹配的AuthorizationPolicy临时关闭所有授权，也可以如图5-19所示，通过一个全匹配的AuthorizationPolicy临时开启所有条件的访问授权。当然，用户配置的动作为DENY或CUSTOM的授权不受影响。\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: authz-allow-all 5namespace: weather 6spec: 7 action: ALLOW 8 rules: 9 - {} ​ 图5-19 临时开启所有授权\n6. 外部授权配置 实际使用中的授权判定逻辑一般业务性较强。AuthorizationPolicy可以通过CUSTOM定义一个扩展动作。这个CUSTOM动作在默认的ALLOW和DENY动作前执行。一般设置授权动作为CUSTOM，并通过provider配置一个外置的授权服务。当满足条件的请求到来时，通过外部授权服务custom-authz进行授权判定：\n1apiVersion: security.istio.io/v1beta1 2kind: AuthorizationPolicy 3metadata: 4 name: custom-authz 5 namespace: weather 6spec: 7action: CUSTOM 8 provider: 9 name: \u0026#34;custom-authz\u0026#34; 10 rules: 11 - to: 12 - operation: 13 methods: [\u0026#34;PUT\u0026#34;,\u0026#34;POST\u0026#34;] ","link":"https://idouba.com/authorization-policy-of-the-definitive-guide-istio/","section":"posts","tags":["Istio权威指南","图书","Istio","AuthorizationPolicy","安全"],"title":"Istio服务授权策略AuthorizationPolicy详解 –《Istio权威指南》书摘"},{"body":"本节书摘来自华为云原生技术丛书《Istio权威指南（上）：云原生服务网格Istio原理与实践》一书原理篇的第3章 流量治理的原理，3.5节ServiceEntry（服务条目），更多内容请参照原书。\n第3章 流量治理的原理 本章讲解Istio提供的流量治理相关内容，即Istio流量治理要解决的问题、实现原理、配置模型、配置定义和典型应用，包括负载均衡、服务熔断、故障注入、灰度发布、故障转移、入口流量和出口流量等流量管理能力的通用原理、模型，以及Istio基于服务网格形态实现的原理和机制；同时会详细解析如何通过Istio中的VirtualService、DestinationRule、Gateway、ServiceEntry、WorkloadEntry、WorkloadGroup、Sidecar、EnvoyFilter、WasmPlugin等重要的服务管理配置来实现流量治理能力。在内容安排上，每节在讲解治理能力前都会从一个最精简的入门示例入手，再详细解析配置模型和定义，并辅以典型的应用案例来呈现其使用方法和应用场景。\n3.5 ServiceEntry（服务条目） 在第2章介绍架构和服务模型时提到，在Istio中管理的大部分服务都是自动注册的Kubernetes服务。但在实际应用中经常还有其他类型的服务并不能自动注册，这就需要一种服务注册机制。在Istio中提供了ServiceEntry对象进行服务注册，以这种方式注册的服务和Kubernetes服务一样被服务网格管理，可以对其配置各种流量规则。\n早期的ServiceEntry主要用于服务网格外部服务注册，比如注册外部的SaaS API或中间件云服务等。当前ServiceEntry的应用范围更加广泛，包括一些非容器的内部服务，比如比较典型的虚拟机类型的服务，配合要在3.6节和3.7节介绍的WorkloadEntry和WorkloadGroup可以实现完整的服务定义和服务实例注册功能。\n3.5.1 入门示例 下面通过一个入门示例了解ServiceEntry的基本用法，在该示例中通过ServiceEntry包装了一个对api.forecast.weather的服务网格外部服务的访问。通过如下配置即可把这个服务网格外部服务注册到服务网格中，并管理其访问流量：\n1apiVersion: networking.istio.io/v1beta1 2kind: ServiceEntry 3metadata: 4 name: weather-external 5spec: 6 hosts: 7 - api.forecast.weather 8 ports: 9 - number: 80 10 name: http 11 protocol: HTTP 12 resolution: DNS 13 location: MESH_EXTERNAL 3.5.2 配置模型 如图3-68所示，ServiceEntry的配置模型主要由以下两部分组成。\n◎ 服务自身定义：定义服务的访问信息，主要包括服务域名hosts和端口ports等，表示服务的访问入口；还包括服务位置（location）和解析方式（resolution）。服务自身的定义类似Kubernetes上Service的功能和定义。\n◎ 服务实例关联：通过workloadSelector关联到服务对应的实例。类似Kubernetes中Service的后端实例选择机制。\n​ 图3-68 ServiceEntry的配置模型\n3.5.3 配置定义 1．hosts（服务域名） 在服务发现模型中，最重要的自然是服务名和服务访问地址。Istio在通过ServiceEntry定义服务时，通过hosts来表示这个访问入口。在使用上有以下几点需要说明。\n◎ 对于HTTP流量，hosts匹配HTTP头域的Host或Authority。\n◎ 对于HTTPS或TLS流量，hosts匹配SNI。\n◎ 对于其他协议的流量，不匹配hosts，而是使用下面的addresses和port字段。\n◎ 当resolution被设置为DNS类型并且没有指定endpoints时，这个字段用作后端的域名来解析后端地址。\n在Istio的流量规则被应用时，VirtualService和DestinationRule也会匹配这个hosts，来决定生效的流量规则。\n2．address（虚拟IP地址） address表示与服务关联的虚拟IP地址，可以是CIDR这种前缀表达式：\n1spec: 2 hosts: 3 - recommendation # not used 4 addresses: 5 - 192.168.99.99 # VIPs 对于TCP服务，当设置了address字段时，在Enovy上会创建对应地址的监听器，并将流量转发到定义的后端服务。如果addresses为空，则只能根据目标端口来识别，在这种情况下，这个端口不能被服务网格的其他服务使用，即服务网格数据面只是作为一个TCP代理，把某个特定端口的流量转发到配置的目标后端。\n如下两个ServiceEntry定义了两个相同端口的服务网格外部服务，对应两个外部域名。两个ServiceEntry都没有配置address，在Envoy生成的配置中会对每个ServiceEntry都生成一个cluster，分别是：\u0026quot;outbound|8099|| api.forecast.weather\u0026quot;和\u0026quot;outbound|8099||api. forecast2.weather\u0026quot;。但是只会生成一个监听器lister：0.0.0.0_8099，只会关联到先创建的ServiceEntry，即outbound|8099|| api.forecast.weather，也就是说，Sidecar在服务网格范围内只能通过这个端口向一个目标后端转发流量。\n但是对HTTP的ServiceEntry，情况会有很大的不同。若将以上ServiceEntry协议改为HTTP，则在Envoy上也会生成两个cluster，同时会生成一个0.0.0.0_8099的监听器；在七层流量管理器上关联了一个8099的路由router；在router中对两个服务域名分别生成两个不同的虚拟主机：api.forecast.weather:8099和api.forecast2.weather:8099，根据请求头域的不同，Host或Authority会将流量分发到不同的后端服务。\n3．ports（服务端口） ports是服务定义的必选字段。ServiceEntry支持多端口形式，每个端口都可以配置服务的端口号（number）、协议（protocol）、端口名（name）和目标端口（targetPort）。\n如下示例中的ports配置，表示定义了一个HTTP服务端口是8099。服务网格数据面Envoy会根据这个端口的配置生成监听器，将这个端口上的流量通过七层过滤器再关联特定的路由配置，转发到ServiceEntry定义的后端服务：\n1ports: 2 - number: 8099 3 name: http 4 protocol: HTTP 4．location（服务位置） location用于设置服务是在服务网格内部还是在服务网格外部，相应地包含以下两种模式。\n◎ MESH_EXTERNAL：表示注册为服务网格外部服务，比如通过API访问的服务网格外部服务。示例中的api.forecast.weather就是这样一个服务网格外部服务。\n◎ MESH_INTERNAL：表示服务网格内部服务，比如虚拟机等服务可以通过这种方式注册和管理，和被服务网格管理的Kubernetes服务具有相同的能力。\n对于虚拟机等MESH_INTERNAL类型的服务网格内部服务，一般在源服务实例和目标服务实例上都会安装服务网格代理，因此具有完整的服务网格能力；但是对于MESH_EXTERNAL类型的服务网格外部服务，服务端不会安装服务网格代理，只有通过Sidecar或Egress-gateway的服务网格出流量可以被服务网格管理。流量规则被定义在目标服务上，但是大部分执行在客户端，所以服务网格对MESH_EXTERNAL类型的服务网格外部服务仍然可以应用丰富的管理手段，只有少量的mTLS等依赖服务端Sidecar的功能不适用。\n5．resolution（服务解析方式） resolution表示服务网格代理在转发流量前，通过哪种方式解析得到服务实例。如图3-69所示，服务还是根据原有的方式去发出请求，首先在集群中将域名解析到一个服务的IP地址上，类似Kubernetes中的Cluster IP；服务访问Cluster IP的出流量随后被服务网格代理拦截；服务网格代理最后根据配置获取服务实例列表，并选择一个目标服务实例发出请求。这里的resolution配置的解析方式影响的只是图上服务网格代理解析服务实例的流程，对前面的应用解析没有影响。\n​ 图3-69 ServiceEntry服务的解析流程\nServiceEntry主要支持如下几种解析方式。\n1）STATIC（静态解析）\n表示服务网格代理决定接收流量的服务实例。一般在服务网格中注册的服务都使用STATIC方式，类似Kubernetes里普通ClusterIP的Service。在如下示例中，当服务网格内部服务访问ServiceEntry定义的recommendation时，服务网格会做服务发现并得到10.118.12.12和10.118.12.13两个实例地址，在两个实例上做负载均衡。实际上Envoy会为这种STATIC解析方式的ServiceEntry创建一个EDS类型的cluster。\n1spec: 2 hosts: 3 - recommendation # not used 4 addresses: 5 - 192.168.99.99 # VIPs 6 ports: 7 - number: 8099 8 name: tcp 9 protocol: TCP 10 location: MESH_INTERNAL 11 resolution: STATIC 12 endpoints: 13 - address: 10.118.12.12 14 - address: 10.118.12.13 注意：ServicEntry中的STATIC解析方式和Envoy服务发现中的STATIC解析方式略有不同，前者指服务网格代理基于EDS进行服务发现，后者指服务的后端地址在Envoy中已被静态配置。\n2）DNS（域名解析）\n表示用查询环境下的DNS进行解析。如果没有设置endpoints，代理就会使用在hosts中指定的域名进行DNS解析，要求在hosts中未使用通配符；如果设置了endpoints，则使用endpoints中的DNS地址解析出目标IP地址。示例如下。\n◎ 不配置后端：比如在本节入门示例中使用域名api.forecast.weather配置hosts，定义了一个服务网格外部服务，当服务网格的服务通过域名访问这个服务网格外部服务时，服务网格会查询DNS，返回这个域名对应的IP地址并进行访问，这也是ServiceEntry定义服务网格外部服务的常见做法。\n◎ 配置后端：更一般的做法是按照如下方式定义一个服务网格外部服务，包含多个实例，每个实例都通过域名表达。这样当应用通过服务网格访问api.forecast.weather服务时，服务网格会把流量分发到两个后端weatherdb1.com和weatherdb2.com，对这两个后端的访问会基于DNS进行解析。\n其实观察ServiceEntry在Envoy中生成的配置会发现，二者都是在Envoy上生成了STRICT_DNS类型的Cluster。区别在于，前者只有一个后端实例，实例地址就是这个服务域名的地址api.forecast.weather；后者的实例地址是配置的两个地址weatherdb1.com和weatherdb2.com。\n1spec: 2 hosts: 3 - api.forecast.weather 4 ports: 5 - number: 8099 6 name: http 7 protocol: HTTP 8 resolution: DNS 9 location: MESH_EXTERNAL 10 endpoints: 11 - address: weatherdb1.com 12 - address: weatherdb2.com 在配置这种解析策略时，代理的DNS解析是异步的，不会阻塞服务请求。另外，DNS解析的每个IP地址都会作为目标实例地址，当一个域名解析出多个IP地址时，会在这几个IP地址上都分配流量。\n3）DNS_ROUND_ROBIN（轮转域名解析）\n和DNS解析类似，不同之处在于，DNS_ROUND_ROBIN仅在建立新连接时使用返回的第1个IP地址。观察Envoy的配置会发现，在该类型的ServiceEntry上生成了一个Logical DNS的Cluster。\n4）NONE：（无须解析）\n代理直接转发流量到请求的IP地址，在这种方式下连通的已经是一个具体的可访问地址了，不需要再进行解析。比如典型的微服务框架的服务发现和内部负载均衡已经选到一个服务端的实例地址；或者在流量到达服务网格代理前已经通过iptables或eBPF转到了明确的后端地址。ServiceEntry的NONE模式类似于Kubernetes的Headless Service的处理模式。\n6．endpoints（后端实例） 表示ServiceEntry关联的服务实例，在Istio 1.6之前的版本中是如下Endpoints类型的结构嵌套在ServiceEntry中，在Istio 1.6中通过一个独立的资源对象WorkloadEntry定义后端服务实例。关于WorkloadEntry，请参照3.6节的详细介绍。当在ServiceEntry的endpoints中配置后端时，一般只配置IP地址等基础信息。\n1 endpoints: 2 - address: 10.118.12.12 3 - address: 10.118.12.13 7．workloadSelector（负载选择器） 除了基于endpoints配置后端实例，更推荐的做法是基于workloadSelector关联服务实例。通过workloadSelector可以动态选择ServiceEntry的服务后端进行服务注册，既可以选择Kubernetes的Pod，也可以选择WorkloadEntry描述的服务实例。如图3-70所示，一个ServiceEntry描述的服务可以同时选择这两种类型的实例，这样即可在不改变客户端调用方式的前提下在虚拟机和Kubernetes间进行服务实例的迁移。\n​ 图3-70 ServiceEntry基于标签的负载选择\n对于一个ServiceEntry，可通过endpoints或workloadSelector方式任选其一配置服务实例。\n8．subjectAltNames（主题备用名，SAN） 表示这个服务负载的主题备用名即SAN列表。在Istio安全相关配置的多个地方被用到。当被配置时，代理将验证服务证书的主题备用名是否匹配。\n9．exportTo（导出） 控制ServiceEntry的可见性，控制在一个命名空间下定义的资源对象是否可以被其他命名空间下的Sidecar、Gateway和VirtualService使用。\n3.5.4 典型应用 1. 服务网格外部服务注册 ServiceEntry的解析方式resolution很灵活，再加上location表示的服务网格内部服务和服务网格外部服务的组合，经常让初学者困惑。下面结合典型的外部访问示例来解析其用法。\n（1）解析方式：DNS。这是ServiceEntry早期的基本用法，在服务网格中注册一个外部服务，配置DNS解析到服务地址。如图3-71所示，当frontend访问外部目标服务api.forecast.weather时，根据配置的解析方式DNS，Sidecar会执行DNS的域名解析，解析到139.xx.xx.29，并发起访问。\n​ 图3-71 解析方式：DNS\n1spec: 2 hosts: 3 - api.forecast.weather 4 ports: 5 - number: 9999 6 name: http 7 protocol: HTTP 8 resolution: DNS 9 location: MESH_EXTERNAL （2）解析方式：NONE。设置为NONE方式时，Sidecar不执行解析，采用直通的方式访问。以这种方式访问会存在潜在的安全隐患。如图3-72所示，frontend访问服务网格外部服务，在请求头域Host上设置正确的目标服务域名api.forecast.weather，但连接的可以不是这个域名对应的地址139.xx.xx.39，而是另一个地址139.xx.xx.29。这样即使请求匹配了Sidecar的对外访问检查，仍然可能访问到一个不安全或者不允许的外部地址。\n​ 图3-72 解析方式：NONE\n（3）解析方式：STATIC。如图3-73所示，通过如下方式配置STATIC的解析时，使用Istio的EDS机制进行服务发现，将对目标地址api.forecast.weather的访问分发到配置的后端139.xx.xx.39。在实际应用中，这种解析方式被更多地用于后面要介绍的服务网格内部服务定义，比如虚拟机类型的服务在服务网格中的管理。\n​ 图3-73 解析方式：STATIC\n1spec: 2 addresses: 3 - 192.168.99.99 4 endpoints: 5 - address: 139.xx.xx.39 6 hosts: 7 - api.forecast.weather 8 location: MESH_EXTERNAL 9 ports: 10 - name: http 11 number: 9999 12 protocol: http 13 resolution: STATIC 2. 服务网格内部服务注册 除了在服务网格中注册一个可以管理的外部服务，当前的另一类广泛应用是使用ServiceEntry注册各种服务网格内部服务。如下配置可以在服务网格内部注册一个虚拟机类型的服务：\n◎ 将locattion设置为MESH_INTERNAL，表示内部类型；\n◎ 将resolution设置为STATIC，表示使用静态服务发现方式；\n◎ 标签选择器app选择WorkloadEntry定义的负载forecast。\n这种模式的使用方式、工作机制都和Kubernetes服务在Istio中的基本相同，也是服务网格管理内部服务的典型方式：\n1spec: 2 hosts: 3 - forecast 4 location: MESH_INTERNAL 5 ports: 6 - number: 8080 7 name: http 8 protocol: HTTP 9 targetPort: 8090 10 resolution: STATIC 11 workloadSelector: 12 labels: 13 app: forecast 关于本应用的完整示例和模型，请参照3.6.4节WorkloadEntry的第1个示例。\n3. 治理ServiceEntry注册的服务 通过ServiceEntry注册到服务网格的服务，可以像其他服务网格内部服务一样使用本章前面介绍的VirtualService和DestinationRule定义服务的流量路由、访问策略。比如通过如下VirtualService可以对3.5.1节示例中的服务网格外部服务访问api.forecast.weather配置2秒的访问超时，这样服务网格内部服务在访问这个服务网格外部服务时，在源服务的Sidecar上会执行对应的超时规则，代理应用程序在超过2秒后自动取消请求。对于服务网格内部服务，也可以使用类似的方式进行管理，和对Kubernetes服务配置流量规则没有区别。\n1spec: 2 hosts: 3 - api.forecast.weather 4 http: 5 - timeout: 2s 6 route: 7 - destination: 8 host: api.forecast.weather 4. 用Egress访问ServiceEntry服务 可以看到，不管配置哪种解析方式，最终都是访问服务网格外部服务的应用的Sidecar拦截了对外流量，执行ServiceEntry中resolution配置的服务发现，并执行配置的流量规则。在生产环境下更多地是使用出口网关Egress-gateway来统一管理对外的流量，即通过ServiceEntry注册服务网格外部服务，再配置出口网关对服务网格外部服务进行访问。如3.4.4节的典型应用所示，出口网关可以参照应用5通过HTTP发起外部访问，也可以参照应用6通过TLS协议访问。\n","link":"https://idouba.com/service-entry-of-the-definitive-guide-istio/","section":"posts","tags":["Istio权威指南","图书","Istio","ServiceEntry"],"title":"Istio重要对象ServiceEntry详解 –《Istio权威指南》书摘"},{"body":"","link":"https://idouba.com/tags/serviceentry/","section":"tags","tags":null,"title":"ServiceEntry"},{"body":"Istio从2017年开源第1个版本到当前版本，已经走过了5年多的时间。在此期间，伴随着云原生技术在各个领域的飞速发展，服务网格的应用也越来越广泛和深入。作为服务网格领域最具影响力的项目，Istio快速发展和成熟，获得越来越多的技术人员关注和应用。我们希望通过《Istio权威指南》系统且深入地讲解Istio，帮助相关技术人员了解和熟悉Istio，满足其日常工作中的需求。《Istio权威指南（上）：云原生服务网格Istio原理与实践》是《Istio权威指南》的上册，重点讲解Istio的原理与实践；《Istio权威指南（下）：云原生服务网格Istio架构与源码》是《Istio权威指南》的下册，重点讲解Istio的架构与源码。\n近年来，服务网格在各个行业中的生产落地越来越多。CNCF在2022年上半年公布的服务网格调查报告显示，服务网格的生产使用率已达到60%，有19%的公司计划在接下来的一年内使用服务网格。当然，服务网格作为云原生的重要技术之一，当前在Gartner的评定中仍处于技术发展的早期使用阶段，有很大的发展空间。\nCNCF这几年的年度调查显示，Istio一直是生产环境下最受欢迎和使用最多的服务网格。其重要原因是，Istio是功能非常全面、扩展性非常好、与云原生技术结合得非常紧密、非常适用于云原生场景的服务网格。像早期Kubernetes在编排领域的设计和定位一样，Istio从2017年第1个版本开始规划项目的应用场景和架构时，就致力于构建一个云原生的基础设施平台，而不是解决某具体问题的简单工具。\n作为基础设施平台，Istio向应用开发人员和应用运维人员提供了非常大的透明度。Istio自动在业务负载中注入服务网格数据面代理，自动拦截业务的访问流量，可方便地在多种环境下部署和应用，使得业务在使用Istio时无须做任何修改，甚至感知不到这个基础设施的存在。在实现上，Istio提供了统一的配置模型和执行机制来保证策略的一致性，其控制面和数据面在架构上都提供了高度的可扩展性，支持用户基于实际需要进行扩展。\n2022年9月28日，Istio项目被正式批准加入CNCF。这必将推动Istio与Envoy项目的紧密协作，一起构建云原生应用流量管理的技术栈。正如Kubernetes已成为容器编排领域的行业标准，加入CNCF也将进一步促进Istio成为应用流量治理领域的事实标准。Istio和Kubernetes的紧密配合，也将有助于拉通规划和开发更有价值的功能。根据Istio官方的统计，Istio项目已有8800名个人贡献者，超过260个版本，并有来自15家公司的85名维护者，可见Istio在技术圈和产业圈都获得了极大的关注和认可。\n本书作者所在的华为云作为云原生领域的早期实践者与社区领导者之一，在Istio项目发展初期就参与了社区工作，积极实践并推动项目的发展，贡献了大量大颗粒特性。本书作者之一徐中虎在2020年Istio社区进行的第一次治理委员会选举中作为亚洲唯一代表入选，参与Istio技术策略的制定和社区决策。\n本书作者作为Istio早期的实践者，除了持续开发满足用户需求的服务网格产品并参与社区贡献，也积极促进服务网格等云原生技术在国内的推广，包括于2019年出版《云原生服务网格Istio：原理、实践、架构与源码解析》一书，并通过KubeCon、IstioCon、ServiceMeshCon等云原生和服务网格相关的技术峰会，推广服务网格和Istio相关的架构、生产实践和配套解决方案等。\n写作目的 《Istio 权威指南》作为“华为云原生技术丛书”的一员，面向云计算领域的从业者及感兴趣的技术人员，普及与推广Istio。本书作者来自华为云云原生团队，本书基于作者在华为云及Istio社区的设计与开发实践，以及与服务网格强相关的Kubernetes容器、微服务和云原生领域的丰富经验，对Istio的原理、实践、架构与源码进行了系统化的深入剖析，由浅入深地讲解了Istio的概念、原理、架构、模型、用法、设计理念、典型实践和源码细节。\n本书是《Istio权威指南》的上册，适合入门级读者从零开始了解Istio的概念、原理和用法，也适合有一定基础的读者深入理解Istio的设计理念。\n《Istio权威指南》的组织架构 《Istio权威指南》分为原理篇、实践篇、架构篇和源码篇，总计26章，其组织架构如下。\n◎ 原理篇：讲解Istio的相关概念、主要架构和工作原理。其中，第1章通过讲解Istio与微服务、服务网格、Kubernetes这几个云原生关键技术的联系，帮助读者立体地理解Istio的概念。第2章概述Istio的工作机制、服务模型、总体架构和主要组件。第3、4、5章通过较大篇幅讲解Istio提供的流量治理、可观测性和策略控制、服务安全这三大核心特性，包括其各自解决的问题、实现原理、配置模型、配置定义和典型应用，可以满足大多数读者在工作中的具体需求。第6章重点讲解自动注入和流量拦截的透明代理原理。第7章讲解Istio正在快速发展的多基础设施流量管理，包括对各种多集群模型、容器、虚拟机的统一管理等。\n◎ 实践篇：通过贯穿全书的一个天气预报应用来实践Istio的非侵入能力。其中，第8章讲解如何从零开始搭建环境。第9章通过Istio的非侵入方式生成指标、拓扑、调用链和访问日志等。第10章讲解多种灰度发布方式，带读者了解Istio灵活的发布策略。第11章讲解负载均衡、会话保持、故障注入、超时、重试、HTTP重定向、HTTP重写、熔断与连接池、熔断异常点检测、限流等流量策略的实践。第12章讲解两种认证策略及其与授权的配合，以及Istio倡导的零信任网络的关键技术。第13章讲解入口网关和出口网关的流量管理，展示服务网格对东西向流量和南北向流量的管理。第14章则是对多集群和虚拟机环境下流量治理的实践。\n◎ 架构篇：从架构的视角分别讲解Istio各组件的设计思想、数据模型和核心工作流程。在Istio 1.16中，Istiod以原有的Pilot为基础框架构建了包含Pilot、Citadel、Galley等组件的统一控制面，第15、16、17章分别讲解以上三个组件各自的架构、模型和流程机制。第18、19、20章依次讲解服务网格数据面上Pilot-agent、Envoy和Istio-proxy的架构和流程，包括三者的结合关系，配合Istio控制面组件完成流量管理，特别是Envoy的架构、模型和关键流程。\n◎ 源码篇：包括第21～26章，与架构篇的6章对应，分别讲解Istio管理面组件Pilot、Citadel、Galley与数据面Pilot-agent、Envoy、Istio-proxy的主要代码结构、代码流程和关键代码片段。本篇配合架构篇中每个组件的架构和机制，对Istio重要组件的实现进行了更详细的讲解和剖析，为读者深入研读Istio相关代码，以及在生产环境下进行相应代码的调试和修改提供指导。\n学习建议 对于有不同需求的读者，我们建议这样使用本书。\n◎ 对云原生技术感兴趣的所有读者，都可通过阅读《Istio权威指南（上）：云原生服务网格Istio原理与实践》，了解服务网格和Istio的概念、技术背景、设计理念与功能原理，并全面掌握Istio流量治理、可观测性和安全等功能的使用方式。通过实践篇可以从零开始学习搭建Istio运行环境并完成多种场景的实践，逐渐熟悉Istio的功能、应用场景，以及需要解决的问题，并加深对Istio原理的理解。对于大多数架构师、开发者和其他从业人员，通过对原理篇和实践篇的学习，可以系统、全面地了解Istio的方方面面，满足日常工作需要。\n◎ 对Istio架构和实现细节感兴趣的读者，可以阅读《Istio权威指南（下）：云原生服务网格Istio架构与源码》，了解Istio的整体架构、各个组件的详细架构、设计理念和关键的机制流程。若对Istio源码感兴趣，并且在实际工作中需要调试或基于源码进行二次开发，那么还可以通过阅读源码篇，了解Istio各个项目的代码结构、详细流程、主要数据结构及关键代码片段。在学习源码的基础上，读者可以根据自己的兴趣或工作需求，深入了解某一关键机制的完整实现，并作为贡献者参与Istio或Envoy项目的开发。\n勘误和支持 您在阅读本书的过程中有任何问题或者建议时，都可以通过本书源码仓库提交Issue或者PR（源码仓库地址参见本书封底的读者服务），也可以关注华为云原生官方微信公众号并加入微信群与我们交流。我们十分感谢并重视您的反馈，会对您提出的问题、建议进行梳理与反馈，并在本书后续版本中及时做出勘误与更新。\n本书还免费提供了Istio培训视频及Istio常见问题解答等资源，请通过本书封底的读者服务获取这些资源。\n致谢 在本书的写作及成书过程中，本书作者团队得到了公司内外领导、同事及朋友的指导、鼓励和帮助。感谢华为云张平安、张宇昕、李帮清等业务主管对华为云原生技术丛书及本书写作的大力支持；感谢华为云容器团队张琦、王泽锋、张永明、吕赟等对本书的审阅与建议；感谢电子工业出版社博文视点张国霞编辑一丝不苟地制订出版计划及组织工作。感谢章鑫、徐飞等一起参与华为云原生技术丛书《云原生服务网格Istio：原理、实践、架构与源码解析》的创作，你们为国内服务网格技术的推广做出了很大贡献，也为本书的出版打下了良好的基础。感谢四位作者的家人，特别是豆豆、小核桃、毛毛小朋友的支持，本书创作的大部分时间源自陪伴你们的时间；也感谢CNCF及Istio、Kubernetes、Envoy社区众多开源爱好者辛勤、无私的工作，期待和你们一起基于云原生技术为产业创造更大价值。谢谢大家！\n华为云容器服务域总监 黄 毽\n华为云应用服务网格架构师 张超盟\n","link":"https://idouba.com/the-definitive-guide-istio-preface-1/","section":"posts","tags":["Istio权威指南","图书","Istio"],"title":"《Istio权威指南-上》前言"},{"body":"原 理 篇 第1章 你好，Istio. 2 1.1 Istio是什么... 2\n1.2 Istio能做什么.. 3\n1.3 Istio与服务治理... 5\n1.3.1 关于微服务... 5\n1.3.2 服务治理的形态.. 7\n1.3.3 Istio不只解决微服务问题.. 9\n1.4 Istio与服务网格... 11\n1.4.1 云原生选择服务网格... 11\n1.4.2 服务网格选择Istio. 14\n1.5 Istio与Kubernetes 17\n1.5.1 Istio，Kubernetes的好帮手... 18\n1.5.2 Kubernetes，Istio的好基座... 20\n1.6 本章小结... 23\n第2章 Istio的架构概述.. 25 2.1 Istio的架构及原理... 25\n2.2 Istio的服务模型... 28\n2.2.1 Istio的服务.. 29\n2.2.2 Istio的服务版本... 30\n2.2.3 Istio的服务实例... 32\n2.3 Istio的主要组件... 34\n2.3.1 控制面的组件... 34\n2.3.2 数据面的组件... 38\n2.3.3 其他组件.. 43\n2.4 本章小结... 43\n第3章 流量治理的原理.. 44 3.1 概念和原理.. 44\n3.1.1 负载均衡.. 46\n3.1.2 服务熔断.. 48\n3.1.3 故障注入.. 54\n3.1.4 灰度发布.. 55\n3.1.5 故障转移.. 60\n3.1.6 入口流量.. 63\n3.1.7 出口流量.. 68\n3.2 VirtualService（虚拟服务）.. 71\n3.2.1 入门示例.. 71\n3.2.2 配置模型.. 72\n3.2.3 配置定义.. 73\n3.2.4 HTTPRoute（HTTP路由）... 75\n3.2.5 TLSRoute（TLS路由）... 89\n3.2.6 TCPRoute（TCP路由）... 91\n3.2.7 三种协议的路由规则对比... 92\n3.2.8 典型应用.. 93\n3.3 DestinationRule（目标规则）... 101\n3.3.1 入门示例.. 101\n3.3.2 配置模型.. 101\n3.3.3 配置定义.. 103\n3.3.4 典型应用.. 116\n3.4 Gateway（服务网关）... 120\n3.4.1 入门示例.. 120\n3.4.2 配置模型.. 122\n3.4.3 配置定义.. 122\n3.4.4 典型应用.. 127\n3.5 ServiceEntry（服务条目）.. 138\n3.5.1 入门示例.. 139\n3.5.2 配置模型.. 139\n3.5.3 配置定义.. 140\n3.5.4 典型应用.. 146\n3.6 WorkloadEntry（工作负载）... 149\n3.6.1 入门示例.. 149\n3.6.2 配置模型.. 150\n3.6.3 配置定义.. 151\n3.6.4 典型应用.. 152\n3.7 WorkloadGroup（工作负载组）.. 154\n3.7.1 入门示例.. 154\n3.7.2 配置模型.. 154\n3.7.3 配置定义.. 155\n3.7.4 典型应用.. 156\n3.8 Sidecar（服务网格代理）... 157\n3.8.1 入门示例.. 157\n3.8.2 配置模型.. 158\n3.8.3 配置定义.. 158\n3.8.4 典型应用.. 161\n3.9 EnvoyFilter（Envoy过滤器）... 163\n3.9.1 入门示例.. 164\n3.9.2 配置模型.. 164\n3.9.3 配置定义.. 165\n3.9.4 典型应用.. 177\n3.10 WasmPlugin（Wasm插件）.. 183\n3.10.1 入门示例... 183\n3.10.2 配置模型... 184\n3.10.3 配置定义... 184\n3.10.4 典型应用... 185\n3.11 本章小结... 186\n第4章 可观测性和策略控制的原理.. 188 4.1 概念和原理.. 188\n4.1.1 可观测性的概念.. 188\n4.1.2 访问指标（Metrics）... 194\n4.1.3 调用链... 198\n4.1.4 访问日志.. 202\n4.1.5 限流... 205\n4.2 Istio访问指标采集... 207\n4.2.1 指标采集的原理.. 208\n4.2.2 指标采集的配置.. 211\n4.3 Istio调用链采集... 218\n4.3.1 调用链采集的原理... 218\n4.3.2 调用链采集的配置... 220\n4.4 Istio中的访问日志采集... 226\n4.4.1 访问日志采集的原理... 226\n4.4.2 访问日志采集的配置... 228\n4.5 Istio限流... 236\n4.5.1 限流的原理... 236\n4.5.2 限流的配置... 238\n4.6 元数据交换.. 248\n4.6.1 元数据交换的原理... 249\n4.6.2 元数据交换的配置... 250\n4.7 本章小结... 252\n第5章 服务安全的原理.. 255 5.1 概念和原理.. 255\n5.1.1 对等身份认证... 257\n5.1.2 服务请求认证... 263\n5.1.3 服务访问授权... 268\n5.2 PeerAuthentication（对等身份认证）.. 273\n5.2.1 入门示例.. 273\n5.2.2 配置模型.. 274\n5.2.3 配置定义.. 274\n5.2.4 典型应用.. 275\n5.3 RequestAuthentication（服务请求认证）... 278\n5.3.1 入门示例.. 278\n5.3.2 配置模型.. 279\n5.3.3 配置定义.. 279\n5.3.4 典型应用.. 281\n5.4 AuthorizationPolicy（服务授权策略）.. 283\n5.4.1 入门示例.. 283\n5.4.2 配置模型.. 284\n5.4.3 配置定义.. 284\n5.4.4 典型应用.. 288\n5.5 本章小结... 293\n第6章 服务网格数据面代理Sidecar 294 6.1 Sidecar的透明注入原理... 294\n6.1.1 Sidecar的注入原理... 294\n6.1.2 自动注入服务... 296\n6.1.3 自动注入流程... 298\n6.2 Sidecar的流量拦截原理... 301\n6.2.1 iptables的基本原理.. 302\n6.2.2 Sidecar Redirect模式... 308\n6.2.3 Sidecar Tproxy模式.. 317\n6.2.4 Ingress网关模式.. 330\n6.3 本章小结... 334\n第7章 异构基础设施.. 335 7.1 多集群服务治理的原理... 335\n7.1.1 Istio多集群相关的概念... 335\n7.1.2 Istio多集群管理... 336\n7.2 多集群的服务网格模型... 337\n7.2.1 扁平网络单控制面模型.. 337\n7.2.2 非扁平网络单控制面模型... 339\n7.2.3 扁平网络多控制面模型.. 344\n7.2.4 非扁平网络多控制面模型... 346\n7.3 多集群的关键技术.. 347\n7.3.1 异构环境DNS. 347\n7.3.2 东西向网关... 351\n7.4 异构服务治理的原理... 352\n7.4.1 治理纯虚拟机服务... 352\n7.4.2 治理混合服务... 354\n7.5 本章小结... 355\n实 践 篇 第8章 环境准备.. 358 8.1 在本地搭建Istio环境... 358\n8.1.1 部署Kubernetes集群... 358\n8.1.2 安装Istio. 359\n8.2 尝鲜Istio命令行.. 361\n8.3 应用示例... 362\n8.3.1 Weather Forecast简介.. 362\n8.3.2 部署Weather Forecast 363\n8.4 本章小结... 364\n第9章 可观测性实践.. 365 9.1 预先准备... 365\n9.2 调用链跟踪.. 366\n9.3 指标监控... 368\n9.3.1 指标配置.. 369\n9.3.2 指标采集：Prometheus 372\n9.3.3 指标管理：Grafana. 374\n9.4 服务网格应用拓扑.. 379\n9.5 访问日志... 383\n9.6 本章小结... 384\n第10章 灰度发布实践.. 385 10.1 预先准备... 385\n10.2 基于流量比例的路由... 386\n10.3 基于请求内容的路由... 389\n10.4 组合条件路由... 391\n10.5 多服务灰度发布... 393\n10.6 本章小结... 395\n第11章 流量治理实践.. 396 11.1 ROUND_ROBIN负载均衡... 396\n11.2 RANDOM负载均衡.. 397\n11.3 地域负载均衡.. 399\n11.3.1 基于权重的地域负载均衡... 399\n11.3.2 用于故障转移的地域负载均衡.. 402\n11.4 会话保持... 407\n11.5 故障注入... 409\n11.5.1 延迟注入.. 409\n11.5.2 中断注入.. 410\n11.6 超时... 411\n11.7 重试... 413\n11.8 HTTP重定向.. 415\n11.9 HTTP重写... 416\n11.10 熔断与连接池... 417\n11.11 熔断异常点检测.. 420\n11.12 限流... 423\n11.12.1 全局限流.. 423\n11.12.2 本地限流.. 426\n11.13 服务隔离... 429\n11.14 流量镜像... 431\n11.15 本章小结... 433\n第12章 服务安全实践.. 434 12.1 双向认证... 434\n12.2 JWT认证... 437\n12.3 特定命名空间的访问授权.. 440\n12.4 特定源地址的授权.. 442\n12.5 本章小结... 444\n第13章 网关流量实践.. 445 13.1 入口网关... 445\n13.2 单向TLS网关.. 447\n13.3 双向TLS网关.. 450\n13.4 访问服务网格外部服务... 452\n13.5 出口网关... 454\n13.6 本章小结... 456\n第14章 异构基础设施实践.. 457 14.1 多集群灰度发布... 457\n14.2 多集群非扁平网络负载均衡... 461\n14.3 多集群非扁平网络故障转移... 464\n14.4 管理虚拟机应用... 469\n14.5 本章小结... 472\n附录A Istio访问日志的应答标记案例.................................................................... 473 结语.......................................................................................................................... 491 ","link":"https://idouba.com/2023-06-01-the-definitive-guide-istio-index-1/","section":"posts","tags":["Istio权威指南","图书","Istio"],"title":"《Istio权威指南-上》目录"},{"body":"Istio从2017年开源第1个版本到当前版本，已经走过了5年多的时间。在此期间，伴随着云原生技术在各个领域的飞速发展，服务网格的应用也越来越广泛和深入。作为服务网格领域最具影响力的项目，Istio快速发展和成熟，获得越来越多的技术人员关注和应用。我们希望通过《Istio权威指南》系统且深入地讲解Istio，帮助相关技术人员了解和熟悉Istio，满足其日常工作中的需求。《Istio权威指南（上）：云原生服务网格Istio原理与实践》是《Istio权威指南》的上册，重点讲解Istio的原理与实践；《Istio权威指南（下）：云原生服务网格Istio架构与源码》是《Istio权威指南》的下册，重点讲解Istio的架构与源码。\n近年来，服务网格在各个行业中的生产落地越来越多。CNCF在2022年上半年公布的服务网格调查报告显示，服务网格的生产使用率已达到60%，有19%的公司计划在接下来的一年内使用服务网格。当然，服务网格作为云原生的重要技术之一，当前在Gartner的评定中仍处于技术发展的早期使用阶段，有很大的发展空间。\nCNCF这几年的年度调查显示，Istio一直是生产环境下最受欢迎和使用最多的服务网格。其重要原因是，Istio是功能非常全面、扩展性非常好、与云原生技术结合得非常紧密、非常适用于云原生场景的服务网格。像早期Kubernetes在编排领域的设计和定位一样，Istio从2017年第1个版本开始规划项目的应用场景和架构时，就致力于构建一个云原生的基础设施平台，而不是解决某具体问题的简单工具。\n作为基础设施平台，Istio向应用开发人员和应用运维人员提供了非常大的透明度。Istio自动在业务负载中注入服务网格数据面代理，自动拦截业务的访问流量，可方便地在多种环境下部署和应用，使得业务在使用Istio时无须做任何修改，甚至感知不到这个基础设施的存在。在实现上，Istio提供了统一的配置模型和执行机制来保证策略的一致性，其控制面和数据面在架构上都提供了高度的可扩展性，支持用户基于实际需要进行扩展。\n2022年9月28日，Istio项目被正式批准加入CNCF。这必将推动Istio与Envoy项目的紧密协作，一起构建云原生应用流量管理的技术栈。正如Kubernetes已成为容器编排领域的行业标准，加入CNCF也将进一步促进Istio成为应用流量治理领域的事实标准。Istio和Kubernetes的紧密配合，也将有助于拉通规划和开发更有价值的功能。根据Istio官方的统计，Istio项目已有8800名个人贡献者，超过260个版本，并有来自15家公司的85名维护者，可见Istio在技术圈和产业圈都获得了极大的关注和认可。\n本书作者所在的华为云作为云原生领域的早期实践者与社区领导者之一，在Istio项目发展初期就参与了社区工作，积极实践并推动项目的发展，贡献了大量大颗粒特性。本书作者之一徐中虎在2020年Istio社区进行的第一次治理委员会选举中作为亚洲唯一代表入选，参与Istio技术策略的制定和社区决策。\n本书作者作为Istio早期的实践者，除了持续开发满足用户需求的服务网格产品并参与社区贡献，也积极促进服务网格等云原生技术在国内的推广，包括于2019年出版《云原生服务网格Istio：原理、实践、架构与源码解析》一书，并通过KubeCon、IstioCon、ServiceMeshCon等云原生和服务网格相关的技术峰会，推广服务网格和Istio相关的架构、生产实践和配套解决方案等。\n写作目的 《Istio权威指南》作为“华为云原生技术丛书”的一员，面向云计算领域的从业者及感兴趣的技术人员，普及与推广Istio。本书作者来自华为云云原生团队，本书基于作者在华为云及Istio社区的设计与开发实践，以及与服务网格强相关的Kubernetes容器、微服务和云原生领域的丰富经验，对Istio的原理、实践、架构与源码进行了系统化的深入剖析，由浅入深地讲解了Istio的概念、原理、架构、模型、用法、设计理念、典型实践和源码细节。\n本书是《Istio权威指南》的下册，适合入门级读者从零开始了解Istio的架构，也适合有一定基础的读者深入研究Istio的源码。\n《Istio权威指南》的组织架构 《Istio权威指南》分为原理篇、实践篇、架构篇和源码篇，总计26章，其组织架构如下。\n◎ 原理篇：讲解Istio的相关概念、主要架构和工作原理。其中，第1章通过讲解Istio与微服务、服务网格、Kubernetes这几个云原生关键技术的联系，帮助读者立体地理解Istio的概念。第2章概述Istio的工作机制、服务模型、总体架构和主要组件。第3、4、5章通过较大篇幅讲解Istio提供的流量治理、可观测性和策略控制、服务安全这三大核心特性，包括其各自解决的问题、实现原理、配置模型、配置定义和典型应用，可以满足大多数读者在工作中的具体需求。第6章重点讲解自动注入和流量拦截的透明代理原理。第7章讲解Istio正在快速发展的多基础设施流量管理，包括对各种多集群模型、容器、虚拟机的统一管理等。\n◎ 实践篇：通过贯穿全书的一个天气预报应用来实践Istio的非侵入能力。其中，第8章讲解如何从零开始搭建环境。第9章通过Istio的非侵入方式生成指标、拓扑、调用链和访问日志等。第10章讲解多种灰度发布方式，带读者了解Istio灵活的发布策略。第11章讲解负载均衡、会话保持、故障注入、超时、重试、HTTP重定向、HTTP重写、熔断与连接池、熔断异常点检测、限流等流量策略的实践。第12章讲解两种认证策略及其与授权的配合，以及Istio倡导的零信任网络的关键技术。第13章讲解入口网关和出口网关的流量管理，展示服务网格对东西向流量和南北向流量的管理。第14章则是对多集群和虚拟机环境下流量治理的实践。\n◎ 架构篇：从架构的视角分别讲解Istio各组件的设计思想、数据模型和核心工作流程。在Istio 1.16中，Istiod以原有的Pilot为基础框架构建了包含Pilot、Citadel、Galley等组件的统一控制面。第15、16、17章分别讲解以上三个组件各自的架构、模型和流程机制。第18、19、20章依次讲解服务网格数据面上Pilot-agent、Envoy和Istio-proxy的架构和流程，包括三者的结合关系，配合Istio控制面组件完成流量管理，特别是Envoy的架构、模型和关键流程。\n◎ 源码篇：包括第21～26章，与架构篇的6章对应，分别讲解Istio管理面组件Pilot、Citadel、Galley与数据面Pilot-agent、Envoy、Istio-proxy的主要代码结构、代码流程和关键代码片段。本篇配合架构篇中每个组件的架构和机制，对Istio重要组件的实现进行了更详细的讲解和剖析，为读者深入研读Istio相关代码，以及在生产环境下进行相应代码的调试和修改提供指导。\n学习建议 对于有不同需求的读者，我们建议这样使用本书。\n◎ 对云原生技术感兴趣的所有读者，都可通过阅读《Istio权威指南（上）：云原生服务网格Istio原理与实践》，了解服务网格和Istio的概念、技术背景、设计理念与功能原理，并全面掌握Istio流量治理、可观测性和安全等功能的使用方式。通过实践篇可以从零开始学习搭建Istio运行环境并完成多种场景的实践，逐渐熟悉Istio的功能、应用场景，以及需要解决的问题，并加深对Istio原理的理解。对于大多数架构师、开发者和其他从业人员，通过对原理篇和实践篇的学习，可以系统、全面地了解Istio的方方面面，满足日常工作需要。\n◎ 对Istio架构和实现细节感兴趣的读者，可以阅读《Istio权威指南（下）：云原生服务网格Istio架构与源码》，了解Istio的整体架构、各个组件的详细架构、设计理念和关键的机制流程。若对Istio源码感兴趣，并且在实际工作中需要调试或基于源码进行二次开发，那么还可以通过阅读源码篇，了解Istio各个项目的代码结构、详细流程、主要数据结构及关键代码片段。在学习源码的基础上，读者可以根据自己的兴趣或工作需求，深入了解某一关键机制的完整实现，并作为贡献者参与Istio或Envoy项目的开发。\n勘误和支持 您在阅读本书的过程中有任何问题或者建议时，都可以通过本书源码仓库提交Issue或者PR（源码仓库地址参见本书封底的读者服务），也可以关注华为云原生官方微信公众号并加入微信群与我们交流。我们十分感谢并重视您的反馈，会对您提出的问题、建议进行梳理与反馈，并在本书后续版本中及时做出勘误与更新。\n本书还免费提供了Istio培训视频及Istio常见问题解答等资源，请通过本书封底的读者服务获取这些资源。\n致谢 ​\n在本书的写作及成书过程中，本书作者团队得到了公司内外领导、同事及朋友的指导、鼓励和帮助。感谢华为云张平安、张宇昕、李帮清等业务主管对华为云原生技术丛书及本书写作的大力支持；感谢华为云容器团队张琦、王泽锋、张永明、吕赟等对本书的审阅与建议；感谢电子工业出版社博文视点张国霞编辑一丝不苟地制订出版计划及组织工作。感谢章鑫、徐飞等一起参与华为云原生技术丛书《云原生服务网格Istio：原理、实践、架构与源码解析》的创作，你们为国内服务网格技术的推广做出了很大贡献，也为本书的出版打下了良好的基础。感谢四位作者的家人，特别是豆豆、小核桃、毛毛小朋友的支持，本书创作的大部分时间源自陪伴你们的时间；也感谢CNCF及Istio、Kubernetes、Envoy社区众多开源爱好者辛勤、无私的工作，期待和你们一起基于云原生技术为产业创造更大价值。谢谢大家！\n华为云容器服务域总监 黄 毽\n华为云应用服务网格架构师 张超盟\n","link":"https://idouba.com/the-definitive-guide-istio-preface-2/","section":"posts","tags":["Istio权威指南","图书","Istio"],"title":"《Istio权威指南-下》前言"},{"body":"目 录\n架 构 篇 第15章 Pilot的架构.. 2 15.1 Pilot的基本架构... 2\n15.1.1 Istio的服务模型.. 4\n15.1.2 xDS协议... 6\n15.2 Pilot的原理.. 12\n15.2.1 xDS服务器... 13\n15.2.2 服务发现... 24\n15.2.3 配置规则发现.. 29\n15.2.4 xDS的生成和分发... 35\n15.3 安全插件... 42\n15.3.1 认证插件... 43\n15.3.2 授权插件... 46\n15.4 Pilot的关键设计... 48\n15.4.1 三级缓存模型.. 48\n15.4.2 去抖动分发... 50\n15.4.3 防过度分发... 51\n15.4.4 增量EDS. 51\n15.4.5 资源隔离... 53\n15.4.6 自动管理虚拟机工作负载... 54\n15.5 本章小结... 55\n第16章 Citadel的架构.. 56 16.1 Istio的证书和身份管理... 56\n16.2 Citadel的基本架构.. 59\n16.3 Citadel的核心原理.. 60\n16.3.1 核心组件的初始化... 61\n16.3.2 CA服务器... 62\n16.3.3 证书签发... 63\n16.3.4 证书轮转器... 65\n16.4 本章小结... 67\n第17章 Galley的架构.. 68 17.1 简化的Galley. 68\n17.2 Galley的整体架构... 69\n17.2.1 早期的MCP. 70\n17.2.2 基于xDS的MCP. 72\n17.3 Galley的核心工作原理.. 72\n17.3.1 启动初始化... 72\n17.3.2 API校验... 75\n17.3.3 对API配置的管理... 78\n17.4 本章小结... 79\n第18章 Pilot-agent的架构.. 80 18.1 Pilot-agent的用途... 81\n18.2 Pilot-agent的核心架构.. 81\n18.3 Pilot-agent的原理... 83\n18.3.1 Envoy的启动... 84\n18.3.2 优雅退出... 85\n18.3.3 xDS代理... 87\n18.3.4 证书管理... 90\n18.3.5 DNS服务器... 91\n18.3.6 应用健康检查.. 92\n18.4 本章小结... 93\n第19章 Envoy的架构.. 94 19.1 Envoy的整体架构... 95\n19.1.1 Envoy的内部架构... 96\n19.1.2 Envoy的通信架构... 100\n19.2 Envoy的内存管理... 110\n19.2.1 堆内存管理... 110\n19.2.2 Buffer管理.. 111\n19.3 Envoy过滤器的架构.. 114\n19.3.1 过滤器的注册.. 115\n19.3.2 过滤器的回调方法... 117\n19.3.3 过滤器的挂起与恢复.. 118\n19.4 Envoy的初始化流程.. 119\n19.4.1 静态配置... 120\n19.4.2 动态配置... 121\n19.4.3 Envoy的创建及初始化流程... 124\n19.4.4 Envoy的运行流程... 128\n19.4.5 目标服务Cluster的创建... 129\n19.4.6 监听器的创建.. 131\n19.5 Envoy的网络及线程模型... 133\n19.5.1 Server主线程.. 134\n19.5.2 Accesslog线程.. 136\n19.5.3 工作线程... 138\n19.5.4 GuardDog线程.. 139\n19.5.5 线程间的同步.. 139\n19.6 Envoy的热升级流程.. 141\n19.7 Envoy的新连接处理流程... 144\n19.8 Envoy的请求及响应数据处理流程.. 145\n19.8.1 对下游请求数据的接收及处理... 146\n19.8.2 对上游请求数据的处理及发送... 149\n19.8.3 对上游响应数据的接收及发送... 151\n19.9 xDS的原理及工作流程... 153\n19.10 安全证书处理... 155\n19.11 WASM虚拟机的原理... 158\n19.12 本章小结.. 161\n第20章 Istio-proxy的架构.. 162 20.1 Istio-proxy的基本架构.. 162\n20.2 Istio-proxy的原理... 163\n20.2.1 Istio-proxy的整体工作流程.. 163\n20.2.2 L4 metadata_exchange的工作流程.. 164\n20.2.3 L7 metadata_exchange扩展的工作流程... 169\n20.2.4 Stats的工作流程... 170\n20.3 本章小结... 173\n源 码 篇 第21章 Pilot源码解析.. 175 21.1 启动流程... 175\n21.2 关键代码解析... 177\n21.2.1 ConfigController 178\n21.2.2 ServiceController 186\n21.2.3 xDS的异步分发.. 194\n21.2.4 对xDS更新的预处理... 202\n21.2.5 xDS配置的生成及分发.. 208\n21.3 本章小结... 211\n第22章 Citadel源码解析.. 212 22.1 启动流程... 212\n22.1.1 Istio CA的创建... 213\n22.1.2 SDS服务器的初始化.. 214\n22.1.3 Istio CA的启动... 215\n22.2 关键代码解析... 216\n22.2.1 CA 服务器的核心原理.. 216\n22.2.2 证书签发实体IstioCA. 218\n22.2.3 CredentialsController的创建和核心原理.. 222\n22.3 本章小结... 224\n第23章 Galley源码解析.. 225 23.1 启动流程... 225\n23.1.1 Galley WebhookServer的初始化... 226\n23.1.2 ValidatingWebhookConfiguration控制器的初始化.. 226\n23.2 关键代码解析... 228\n23.2.1 配置校验... 228\n23.2.2 Validating控制器的实现.. 232\n23.3 本章小结... 235\n第24章 Pilot-agent源码解析.. 236 24.1 整体架构... 236\n24.2 启动及监控.. 238\n24.3 xDS转发服务... 243\n24.4 SDS证书服务... 248\n24.5 健康检查... 255\n24.5.1 应用容器的LivenessProbe探测... 255\n24.5.2 应用容器的ReadinessProbe探测.. 257\n24.5.3 Envoy进程的ReadinessProbe探测... 258\n24.5.4 Pilot-agent进程的LivenessProbe探测... 262\n24.6 本章小结... 265\n第25章 Envoy源码解析.. 266 25.1 Envoy的初始化... 266\n25.1.1 启动参数bootstrap的初始化.. 267\n25.1.2 初始化观测指标... 268\n25.1.3 过滤器注册及信息补齐... 269\n25.1.4 Envoy自身信息解析... 273\n25.1.5 Admin API的初始化.. 273\n25.1.6 Worker的初始化... 276\n25.1.7 Dispatcher内存延迟析构... 279\n25.1.8 CDS的初始化.. 283\n25.1.9 LDS的初始化.. 286\n25.1.10 初始化观测管理系统.. 287\n25.1.11 启动Stats定期刷新.. 292\n25.1.12 GuardDog的初始化... 292\n25.2 热重启的流程... 296\n25.3 Envoy的运行和连接创建... 298\n25.3.1 启动Worker工作线程... 299\n25.3.2 监听器的加载.. 301\n25.3.3 接收连接... 304\n25.4 Envoy接收及处理数据... 309\n25.4.1 读取数据... 310\n25.4.2 接收数据... 311\n25.4.3 处理数据... 312\n25.5 Envoy发送数据到服务端... 317\n25.5.1 路由匹配... 317\n25.5.2 获取连接池... 320\n25.5.3 创建上游请求.. 325\n25.6 Envoy收到服务端响应... 333\n25.6.1 接收响应数据.. 333\n25.6.2 发送响应数据.. 335\n25.7 xDS流程解析... 337\n25.7.1 xDS公共订阅... 337\n25.7.2 xDS推送... 342\n25.7.3 LDS更新... 343\n25.7.4 SDS订阅... 350\n25.8 遥测元数据存储... 352\n25.8.1 创建遥测元数据... 352\n25.8.2 收集Stats观测数据.. 360\n25.8.3 定义静态指标.. 361\n25.9 WASM扩展... 363\n25.9.1 WASM虚拟机的启动... 363\n25.9.2 WASM虚拟机的运行... 374\n25.10 本章小结.. 387\n第26章 Istio-proxy源码解析.. 388 26.1 metadata_exchange. 388\n26.2 遥测数据Stats的上报.. 395\n26.3 源码地址... 406\n26.4 本章小结... 408\n附录A 源码仓库介绍.............................................................................................. 409 附录B 实践问题总结.............................................................................................. 416 附录C 服务网格术语表.......................................................................................... 432 结 语... 447 ","link":"https://idouba.com/2023-06-01-the-definitive-guide-istio-index-2/","section":"posts","tags":["Istio权威指南","图书","Istio"],"title":"《Istio权威指南-下》目录"},{"body":" 推荐序一\n随着企业数字化转型的全面深入，企业在生产、运营、创新方面都对基础设施提出了全新要求。为了保障业务的极致性能，资源需要被随时随地按需获取；为了实现对成本的精细化运营，需要实现对资源的细粒度管理；新兴的智能业务则要求基础设施能提供海量的多样化算力。为了支撑企业的数智升级，企业的基础设施需要不断进化、创新。如今，企业逐步进入深度云化时代，由关注资源上云转向关注云上业务创新，同时需要通过安全、运维、IT治理、成本等精益运营手段来深度用云、高效管云。云原生解决了企业以高效协同模式创新的本质问题，让企业的软件架构可以去模块化、标准化部署，极大提高了企业应用生产力。\n从技术发展的角度来看，我们可以把云原生理解为云计算的重心从“资源”逐渐转向“应用”的必然结果。以资源为中心的上一代云计算技术专注于物理设备如何虚拟化、池化、多租化，典型代表是计算、网络、存储三大基础设施的云化。以应用为中心的云原生技术则专注于应用如何更好地适应云环境。相对于传统应用通过迁移改造“上云”，云原生的目标是通过一系列的技术支撑，使用户在云环境下快速开发和运行、管理云原生应用，并更好地利用云资源和云技术。\n服务网格是CNCF（Cloud-Native Computing Foundation，云原生计算基金会）定义的云原生技术栈中的关键技术之一，和容器、微服务、不可变基础设施、声明式API等技术一起，帮助用户在动态环境下以弹性和分布式的方式构建并运行可扩展的应用。服务网格在云原生技术栈中，向上连接用户应用，向下连接多种计算资源，发挥着关键作用。\n◎ 向下，服务网格与底层资源、运行环境结合，构建了一个理解应用需求、对应用更友好的基础设施，而不只是提供一堆机器和资源。服务网格帮助用户打造“以应用为中心”的云原生基础设施，让基础设施能感知应用且更好地服务于应用，对应用进行细粒度管理，更有效地发挥资源的效能。服务网格向应用提供的这层基础设施也经常被称为“应用网络”。用户开发的应用程序像使用传统的网络协议栈一样使用服务网格提供的应用层协议。就像TCP/IP负责将字节码可靠地在网络节点间传递，服务网格负责将用户的应用层信息可靠地在服务间传递，并对服务间的访问进行管理。在实践中，包括华为云在内的越来越多的云厂商将七层应用流量管理能力和底层网络融合，在提供传统的底层连通性能力的同时，基于服务的语义模型，提供了应用层丰富的流量、安全和可观测性管理能力。\n◎ 向上，服务网格以非侵入的方式提供面向应用的韧性、安全、动态路由、调用链、拓扑等应用管理和运维能力。这些能力在传统应用开发模式下，需要在开发阶段由开发人员开发并持续维护。而在云原生开发模式下，基于服务网格的非侵入性特点，这些能力被从业务中解耦，无须由开发人员开发，由运维人员配置即可。这些能力包括：灵活的灰度分流；超时、重试、限流、熔断等；动态地对服务访问进行重写、重定向、头域修改、故障注入；自动收集应用访问的指标、访问日志、调用链等可观测性数据，进行故障定界、定位和洞察；自动提供完整的面向应用的零信任安全，比如自动进行服务身份认证、通道加密和细粒度授权管理。使用这些能力时，无须改动用户的代码，也无须使用基于特定语言的开发框架。\n作为服务网格技术中最具影响力的项目，Istio的平台化设计和良好扩展性使得其从诞生之初就获得了技术圈和产业界的极大关注。基于用户应用Istio时遇到的问题，Istio的版本在稳定迭代，功能在日益完善，易用性和运维能力在逐步增强，在大规模生产环境下的应用也越来越多。特别是，Istio于2022年9月被正式批准加入CNCF，作为在生产环境下使用最多的服务网格项目，Istio在加速成熟。\n华为云在2018年率先发布全球首个Istio商用服务：ASM（Application Service Mesh，应用服务网格）。ASM是一个拥有高性能、高可靠性和易用性的全托管服务网格。作为分布式云场景中面向应用的网络基础，ASM对多云、混合云环境下的容器、虚拟机、Serverless、传统微服务、Proxyless服务提供了应用健康、韧性、弹性、安全性等统一的全方位管理。\n作为最早一批投身云原生技术的厂商，华为云是CNCF在亚洲唯一的初创成员，社区代码贡献和Maintainer席位数均持续位居亚洲第一。华为云云原生团队从2018年开始积极参与Istio社区的活动，参与Istio社区的版本特性设计与开发，基于用户的共性需求开发了大量大颗粒特性，社区贡献位居全球第三、中国第一。华为云云原生团队成员入选了每届Istio社区指导委员会，参与了Istio社区的重大技术决策，持续引领了Istio项目和服务网格技术的发展。\n2021年4月，华为云联合中国信通院正式发布云原生2.0白皮书，全面诠释了云原生2.0的核心理念，分享了云原生产业洞察，引领了云原生产业的繁荣。此外，华为云联合CNCF、中国信通院及业界云原生技术精英们成立全球云原生交流平台——创原会，创原会当前已经在中国、东南亚、拉美、欧洲陆续成立分会，探索前沿云原生技术、共享产业落地实践经验，让云原生为数字经济发展和企业数字化转型贡献更多的价值。\n《Istio权威指南》来源于华为云云原生团队在云服务开发、客户解决方案构建、Istio社区特性开发、生产环境运维等日常工作中的实践、思考和总结，旨在帮助技术圈的更多朋友由浅入深且系统地理解Istio的原理、实践、架构与源码。书中内容在描述Istio的功能和机制的同时，运用了大量的图表总结，并深入解析其中的概念和技术点，可以帮助读者从多个维度理解云原生、服务网格等相关技术，掌握基于Istio实现应用流量管理、零信任安全、应用可观测性等能力的相关实践。无论是初学者，还是对服务网格有一定了解的用户，都可以通过本书获取自己需要的信息。\n华为云CTO 张宇昕\n","link":"https://idouba.com/the-definitive-guide-istio-ref1/","section":"posts","tags":["Istio权威指南","图书","Istio"],"title":"《Istio权威指南》推荐序一"},{"body":"推荐序二\n我很高兴向大家介绍这本关于Istio服务网格技术的权威书籍。Istio是一种创新性的平台，在云原生计算领域迅速赢得人们的广泛关注。企业在向微服务和容器化架构转型的过程中，对强大且可扩展的服务发现、流量管理及安全平台的需求变得比以往更加迫切。Istio在2022年9月正式被CNCF接受为孵化项目，并成为一种领先的解决方案，为云原生应用提供了无缝连接、可观察性和控制等能力。\n本书提供了全面且实用的Istio指南，涵盖了Istio的核心概念、特性和对xDS协议等主题的深入探讨，还包括对Envoy和Istio项目源码的深入解析，这对潜在贡献者非常有用。无论您是软件工程师、SRE还是云原生开发人员，本书都将为您提供利用Envoy和Istio构建可扩展和安全的云原生应用所需的知识和技能。\n我要祝贺作者们完成了杰出的工作，并感谢他们在云原生社区分享自己的专业知识。我相信本书将成为对Envoy、Istio及现代云原生应用开发感兴趣的人不可或缺的资源。\nCNCF CTO Chris Aniszczyk\n（原文）\nI am thrilled to introduce this definitive book on Istio service mesh technology, a revolutionary platform that has been rapidly gaining popularity in the world of cloud-native computing. As businesses shift towards microservices and containerized architectures, the need for a robust and scalable platform for service discovery, traffic management, and security has become more critical than ever before. Istio was officially accepted in the CNCF as an incubation project in September 2022 and has emerged as a leading solution that provides seamless connectivity, observability, and control for cloud native applications.\nThis book provides a comprehensive and practical guide to Istio, covering its core concepts, features and deep dives into topics like the xDS protocol. It also includes a deep dive source code analysis of the Envoy and Istio projects which can be very useful to potential contributors. Whether you are a software engineer, an SRE or a cloud native developer, this book will provide you with the knowledge and skills which are necessary to leverage the power of Envoy and Istio to build scalable and secure cloud native applications.\nI would like to congratulate the authors for their outstanding work and thank them for sharing their expertise with the wider cloud native community. I am confident that this book will be an invaluable resource for anyone interested in both Envoy and Istio and their roles in modern cloud native development.\nCNCF CTO Chris Aniszczyk\n","link":"https://idouba.com/the-definitive-guide-istio-ref2/","section":"posts","tags":["Istio权威指南","图书","Istio"],"title":"《Istio权威指南》推荐序二"},{"body":" 感谢各位读者阅读本书的全部内容！希望书中的内容能给您和您的日常工作带来帮助。下面谈谈笔者对服务网格技术的一些观点，以与各位读者共勉。\n随着多年的发展，服务网格技术在用户场景中的应用及技术本身都进入了比较务实的阶段。以Istio为代表的服务网格项目通过自身的迭代和对用户应用场景的打磨变得逐渐稳定、成熟和易用。Istio已加入CNCF，这进一步增加了技术圈对服务网格技术的信心。通过这几年的发展，服务网格技术逐渐成熟，形态也逐步被用户接受，并越来越多地在生产环境下大规模应用。\n在这个过程中，服务网格技术不断应对用户的实际应用问题，也与周边技术加速融合，更聚焦于解决用户的具体问题，在多个方面都呈现积极的变化。\n除了Istio得到人们的广泛关注和大规模应用，其他多个服务网格项目也得到关注并实现了快速发展。除了开源的服务网格项目，多个云厂商也推出了自研的服务网格控制面，提供面向应用的全局的应用基础设施抽象，统一管理云上多种形态的服务（包括容器、虚拟机和多云混合云等），并与自有的监控、安全等服务结合，向最终用户提供完整的应用网络功能，解决服务流量、韧性、安全和可观测性等问题。\n一个较大的潜在变化发生在网格API方面，Kubernetes Gateway API获得了长足的发展。原本设计用于升级Ingress管理入口流量的一组API在服务网格领域获得了意想不到的积极认可。除了一些厂商使用Kubernetes Gateway API配置入口流量，也有服务网格使用其来配置管理内部流量。社区专门设立了GAMMA（Gateway API for Mesh Management and Administration）来推动Kubernetes Gateway API在服务网格领域的应用。\n较之控制面的设计和变化大多受厂商和生态等因素的影响，服务网格数据面的变化则更多来自最终用户的实际使用需求。在大规模的落地场景中，资源、性能、运维等挑战推动了服务网格数据面相应的变革尝试。\n首先，服务网格数据面呈现多种形态，除了常规的Sidecar模式，Istio社区在2022年下半年推出了Ambient Mesh，在节点代理Ztunnel上处理四层流量，在拉远的集中式代理Waypoint上处理七层流量。Cilium项目基于eBPF和Envoy实现了高性能的网格数据面，四层流量由eBPF快路径处理，七层流量通过每节点部署的Envoy代理处理。华为云应用服务网格ASM上线节点级的网格代理Terrace，处理本节点上所有应用的流量，简化Sidecar维护并降低了总的资源开销。同时，华为云ASM推出完全基于内核处理四层和七层流量的数据面Kmesh，进一步降低了网格数据面代理带来的延迟和资源开销。\n然后，在云厂商的网络产品中，七层的应用流量管理能力和底层网络融合的趋势越来越显著。即网络在解决传统的底层连通性的同时，开始提供以服务为中心的语义模型，并在面向服务的连通性基础上，提供了越来越丰富的应用层的流量管理能力，包括流量、安全和可观测性等方面。虽然当前提供的功能比一般意义上服务网格规划的功能要少，颗粒度要粗，但其模型、能力甚至场景与服务网格正逐步趋近。\n其次，除了向基础设施进一步融合，网格数据面也出现了基于开发框架构建Proxyless模式的尝试。这种模式作为标准代理模式的补充，在厂商产品和用户解决方案中均获得了一定的认可，gRPC、Dubbo 3.0等开发框架均支持这种Proxyless模式。开发框架内置了服务网格数据面的能力，同时通过标准数据面协议xDS和控制面交互，进行服务发现、获取流量策略并执行相应的动作。这种模式比代理模式性能损耗少，也会相应地节省一部分代理的资源开销，但也存在开发框架固有的耦合性、语言绑定等问题。\n再次，Proxyless模式从诞生时期开始就引发了较大的争论。一种观点认为其是服务网格的正常演进，是代理模式的有益补充；也有一种观点认为其是向开发框架模式的妥协，更有甚者批评其是技术倒车。笔者若干年前做过微服务框架的设计开发工作（项目后来开源并从Apache毕业），近些年一直聚焦于服务网格相关技术和产品，认为没必要太纠结技术形态细节。在为用户提供产品和解决方案的过程中，近距离深入了解各类用户的实际业务需求和痛点，我们认为几乎所有技术呈现的变化都是适应用户实际业务的自我调整。具体到网格数据面的这些变化，说明服务网格技术正进入了快速发展时期。在这个过程中，希望我们这些有幸参与其中的技术人员能够以更开放的心态接纳和参与这些变化，深刻洞察用户碰到的问题，并以更开阔的技术视野解决用户问题，避免各种无休止的技术形态空洞之争。我们认为技术唯一的价值就是解决用户问题，产生有用性。正是不断涌现的用户业务需求，推动了技术的进步和发展，也提供给我们参与其中的机会和发挥作用的空间。\n最后，再次感谢各位读者阅读本书，也很期待将来有机会就其中的内容和您进行技术交流。假如您需要更深入地学习服务网格及云原生相关技术，欢迎关注我们的“容器魔方”公众号，一起学习并讨论服务网格及云原生领域内的最新技术进展。\n​ 张超盟\n","link":"https://idouba.com/conclusion-of-the-definitive-guide-istio/","section":"posts","tags":["Istio权威指南","图书","Istio"],"title":"《Istio权威指南》结语"},{"body":" 记录在2022年4月28日在IstioCon上发表的技术演讲《Istio multi-cluster traffic management speed up automobile company new business dev,deploy and ops》，和Smart的研发总监Kexing一起介绍了Istio多集群在Smart的实践。希望为Smart新车的大卖贡献了一点力量。\n议题： smart, a brand to fully transform from fuel vehicles to electric vehicles, is committed to exploring the best solutions for future urban transportation. On its IT infrastructure, cloud-native technologies such as Kubernetes and service mesh help simplify the technology stack, accelerate business innovation, and greatly improve the efficiency of new business development, deployment, operation and maintenance.\nIn this meeting, Kexing and Chaomeng will share their multi-cluster practice in production environment. That is how Istio provides distributed traffic management across 10+ clusters in smart’s production and testing environments. It includes performing canary release by deploying and splitting traffic of new version in different clusters, providing high availability by failover east-west traffic between instances of different clusters, unified authentication and authorization security management, unified topology view and distributed tracing across clusters, etc.\n正文： 附： 演讲材料(官方Slides) 官方Sched主页 ","link":"https://idouba.com/istiocon2022-istio-multi-cluster-traffic-management-speed-up-automobile-company-new-business-dev-deploy-and-ops/","section":"posts","tags":["Istio","IstioCon","演讲","多集群"],"title":"IstioCon2022：Istio 多集群流量管理加速汽车公司新业务开发、部署和运营"},{"body":" 记录在KubeCon2021上发表的技术演讲《Online Video upgrades resilience from SC Circuit Breaker to Service Mesh》，和世宇做的一个技术实践分享，总结了下一起把网格在人人视频中落地的部分经验。。\n议题： 作为中国领先的在线视频共享平台，人人视频业务的快速发展给其 IT 基础设施带来了巨大挑战。日益增长的复杂性、容量和韧性要求给当前基于 Spring Cloud 熔断器的微服务带来了新的问题。\n在KubeCon2021上，华为云应用服务网格架构师张超盟和人人视频技术主管徐世宇介绍了大规模生产环境中的服务网格韧性实践，包括不健康实例的透明自动隔离、故障自动恢复和自我修复、连接池管理、重试、限流、超时和分布式跟踪等。通过分析熔断器模式和比较 Spring Cloud 熔断器与服务网格在各自生产实践中不同的实现方式，结果表明优化不只是改善了系统的可靠性和可用性，还使得开发和操作工作更简单便捷。\nAs one leading Online Video sharing platform in China, RR's rapid business development introduce great challenge on its IT infrastructure. The increasing complexity, capacity and resilience requirement brings new problems to current Spring Cloud circuit breaker based micro services.\nIn this presentation, Chaomeng and Shiyu will focus on service mesh resilience practice in large scale production environment, including transparent auto-isolation of the unhealthy instance, auto-recovery and self-healing, connection pool management, retry, fine gained rate limit and distributed tracing, latency metrics. By analyzing circuit breaker pattern and comparing the different implementation of Spring Cloud circuit breaker and service mesh in their production practice, they show that the optimization not only improves system reliability and availability but also makes dev and ops works simpler and easier.\n正文： 我是张超盟，来自华为云。本次大会我和人人视频的架构师徐世宇带来关于服务韧性的分享。结合一个生产中的实际案例，介绍网格等云原生解决方案替换原有基于Spring Cloud Hystrix在提升服务韧性的实践细节。\n我是华为云应用服务网格的架构师，在华为云主要从事容器、网格等云原生相关设计开发工作；世宇是人人视频的架构师，负责人人视频后台服务云原生落地的架构、方案和实施工作。\n演讲主要包含三部分的内容：\n首先，概要的介绍韧性的背景； 第二部分，案例的业务背景和架构，包括原有Spring Cloud框架中基于Hystrix的韧性能力的使用细节； 第三部分是本次演讲的重点内容，介绍服务网格等云原生技术全面提升服务韧性的实践。 关于韧性 “任何事物任何时候都可能故障”，这是AWS的沃纳关于故障的经典描述。在系统架构设计，特别是韧性、可靠性可用性设计中被广泛引用。因为不断的经验教训告诉我们，对于一个系统，我们所面临的不是是否失败，而是什么时候失败的问题。\n不管前期我们投入多少财力、精力和资源去加固系统，失败总不可避免。预防失败是一方面，更重要的是接受失败，在失败时候保证业务影响小，并尽快的从失败中恢复。\n韧性正是描述了这样一种能力，韧性强调的是系统在过载、故障或在遭受攻击的时候还能够使用。韧性告诉我们，虽然我们并不想要失败，但是我们承认失败会发生的现实。因而我们需要为失败而设计系统，在故障发生时，减少故障对系统的影响，进而减小对用户业务的影响，特别是核心业务的影响。即构建能处理这些故障并自我修复的系统。有个著名的说法，韧性不能保证你多挣到钱，但是可以保证你少赔钱。套用当前一个流行的说法是，产品的竞争力或者业务能力能帮我们冲击更高的上线，但是韧性能帮助我们守住我们的下线。\n韧性应用于工程世界的所有系统。计算机世界里韧性设计一直是一个非常重要的研究方向。不管是自研的传统服务，还是现网上运行的云服务。\n在本次分享中我们将聚焦服务间访问的韧性，主要是客户场景中微服务的服务间访问比较频繁的场景。\n以上关于故障的观点在规模小的系统里体现可能不明显，在规模比较大的系统里尤其是微服务场景下体现的非常明显。局部的访问影响整个系统，进而影响最终业务。\n如Hystrix关于韧性的理论模型中描述了：对于依赖 30 个服务的应用程序，即使每个服务的正常运行时间为 99.99%，系统总的正常运行也只有99.7%，每个月会引入超过2个小时的停机。考虑到微服务分布式系统的网络带宽、延时、可靠性、安全、业务自身问题、资源等情况会变的更加复杂。\n业务场景和挑战 接下来由人人视频的架构师徐世宇介绍实践的实际场景、系统架构，和早期基于Spring Cloud的熔断器Hystrix提供微服务韧性保护的实践细节以及遇到的挑战。\n人人视频是以美日韩泰视频内容为主的在线视频点播APP。当前拥有2亿+注册用户，日活最高达到1000万，月活用户5500万，并且近日人人视频迎来了第七个周年纪念日。作为中国领先的在线视频共享平台，人人视频业务的快速发展给其 IT 基础设施带来了巨大挑战。\n人人视频主要业务架构如上图所示，该业务架构主要分为四层：网关层、业务聚合BFF层、基础服务层、中间件层。其中基础服务层由用户中心、内容中心、市场变现中心、数据中心五大中心构成：\n用户中心主要以用户信息、用户标签鉴权构成； 内容中心主要以视频基础信息、视频解析、视频分发、视频标签等媒资处理构成； 社区中心主要包含评论、弹幕交互、社区广场； 市场变现中心主要包含活动、任务、广告、商城、支付等内容； 数据中心以智能推荐、海量数据搜索、业务风控等构成； 中间件层主要包含kafka、redis等高并发场景组件，并且采用了mysql、mongoDB、Elasticsearch、Hbase等多元化数据存储方案。整个业务容器由CCE进行托管编排，并且采用了ASM进行服务的韧性保护。\n随着人人视频业务蓬勃发展，其架构模式也进行了多次迭代调整。早期由于业务量级不够大，架构上也缺乏相应的容错机制保护，比如未采用熔断机制进行微服务治理。此架构模式下，当下游服务出现故障时会积压阻塞上游服务的请求，从而使得上游服务进行级联性的崩溃，最终导致服务集群的雪崩而完全不可用。\n为解决此致命性问题，我们在架构中引入了hystrix熔断保护机制。此保护模式下，当下游服务出现故障时，上游服务能快速的对下游服务采用熔断降级的措施，从而使得该服务不会受到下游异常服务的影响。\n下面主要介绍hystrix配置在人人视频的实践，例如在updateUser场景主要设置coreSize为20，maximumSize为40，maxQueueSize为1000，queueSizeRejectionThreshold为800；此设置和基本的线程池原理一致，当业务请求创建的线程数还未达到coreSize时会新建线程去处理，当创建的线程数达到coreSize之后的业务请求会放入队列等待处理，当队列里等待的业务数达到maxQueueSize时会再新建线程处理，直到达到maximumSize。这是hystrix的一个线程池设置，此时我们又该如何设置熔断触发的参数。熔断触发主要由断路器参数进行控制，比如我们在默认的时间窗10s内至少有200个请求（requestVolumeThreshold：200）并且错误率达到了50%（errorThresholdPercentage：50）即触发熔断，触发熔断10000ms（sleepWindowInMilliseconds：10000）后会释放少量请求去探测下游服务是否正常，如果正常则断路器关闭，后面的所有请求则正常请求下游服务，如果不正常断路器则继续打开直到下一个休眠时间后继续探测下游服务正常与否。\n但随着业务架构的不断迭代调整，使用hystrix进行熔断保护的弊端也随之产生。当前人人视频正在利用go语言的优势将BFF层服务采用go进行重构，但由于hystrix组件的语言限制，并不能在go的框架中进行使用，并且hystrix的使用代码侵入性强，比如需要引入相应的jar包，使用相关的注解，开启相关的配置等。并且当我们需要使用限流方案时，hystrix也不能直接提供成熟的解决方案。当我们使用混沌工程来进行正常业务的故障注入以便更早的暴露出问题时，hystrix也将无能为力。针对这些问题，我们也在探索一些新的方案来解决，实践证明网格等云原生技术能很好地解决业务中碰到的这些问题。\n服务网格韧性实践 下面我们介绍服务网格的云原生解决方案中，如何提供完整的韧性能力，在实践中帮助用户商业成功。\n在基于云原生的韧性方案中，我们不只提供了面向应用的熔断器，而是提供了从开发、测试到基础设施，到应用运行的整个韧性保证。也包括运行期的Ops，保证快速发现问题，进而解决问题。从而做到故障模拟与测试、隔离与恢复、定界与定位等全纬度的处理。进而避免故障蔓延与故障影响业务，特别是对核心业务的影响。\n熔断 Circuit Breaker 左图是项目中之前实施的经典的Hystrix的状态迁移图。一段时间内实例连续的错误次数超过阈值则进入熔断开启状态，不接受请求；隔离一段时间后，会从熔断状态迁移到半熔断状态，如果正常则进入熔断关闭状态，可以接收请求；如果不正常则仍然进入熔断开启状态。\n网格中虽然没有显式提供这样一个状态图，但是Istio中异常点检查的阈值规则也都是这样设计的。两者的不同是Spring Cloud的熔断是在SDK中Hystrix执行，Istio中是数据面proxy执行。Hystrix因为在业务代码中，允许用户通过编程做一些控制。\n下面看下网格的熔断实施的效果。这是一个典型的故障场景。其中一个服务实例故障，当没有进行任何故障处理措施时，流量还是均衡的分发到三个实例上，对于服务访问者而言，将会有三分之一的几率得到失败的应答，影响最终用户的业务。\n**韧性的重要一点要求是故障发生时不影响用户最终业务。**对于这种部分实例故障，基于网格的异常点检查，隔离故障实例使得请求只发到健康的实例上。具体规则是：考察服务实例的访问情况，在一段时间内如果连续失败次数达到阈值条件，则该实例会被隔离，得不到流量。\n如图配置：当一个实例在4分钟内，连续5次502 503 或504故障，将会被隔离10分钟；在这10分钟里，隔离的实例会被标记为不健康，不能得到流量。在10分钟后，这个实例会被自动加回来，尝试重新接收流量。如果继续检测出是故障，则隔离时间会加倍。如这个例子中，第二次连续故障会被隔离20分钟，下次30分钟，从而使得一直故障的实例一直被隔离，减少对业务的影响。\n**隔离故障的详细过程如下：**从拓扑图上可以看到第一个实例异常满足熔断阈值，触发了熔断，网格数据面向这个故障实例上分发的流量逐渐减少，直到完全没有流量，即故障实例被隔离。\n这样，所有访问流量只会分发到两个健康实例上，通过这种熔断保护保障服务整体访问的成功率。\n**除了隔离外，韧性中另外有一个非常重要的要求是系统的故障自愈能力。这里三个流量拓扑演示了从刚才的故障中恢复的过程。**可以看到：初始状态这个故障实例被隔离中，没有流量；当实例自身正常后，网格数据面在将其隔离配置的间隔后，重新尝试分配流量，当满足阈值要求则该实例会被认为是正常实例，可以和其他两个实例一样接收请求。最终可以看到三个实例上均衡的处理请求。即实现了故障恢复。\n网格熔断提供的另外一组保护机制是非侵入的连接池管理。可以对四层的连接，七层的请求进行限制。当实际的连接和请求超过配置的阈值时，则断路连接，从而保护上游的服务。\n如配置对目标服务，最大允许80个连接，这里应用于一般TCP或HTTP1，因为HTTP2对于一个上游实例使用一个连接。最大请求数为800，用来控制HTTP2的请求数。对于HTTP1，通过最大连接数即可以控制。等待连接上发送的最大请求数为1000，应用于HTTP1，因为HTTP2请求可以直接发送。另外，每个连接上最多10个请求。\n这里附了个参照，Istio控制面对于上面的参数的配置和分类和Envoy稍有不同，在实际使用中，有使用者经常搞混。可以看到Istio按照协议分为TCP和HTTP的配置。而Envoy中有些参数是配置上游服务的，有些参数是配置在熔断器上。\nTimeout也是韧性的一个重要手段。设置服务超时来避免服务一直卡在某个请求上，影响业务也影响对资源的一直占用。如在bff服务访问后端业务服务时，配置了一个3s的超时。客户端的 proxy在代理bff发起请求时，会考察超时时间，超过这个时间，则会取消操作。在实际使用中，超时时间的配置需要结合业务。避免过长，导致一直等待一个不会返回应答的请求，也避免过短，导致合理的较慢请求还没返回就超时取消。\n适当的重试是提高系统总体服务质量的一个方便且有效的手段。特别是对暂时网络故障、目标服务内部暂时异常等进行重试可以提高服务总体的访问成功率。网格通过简单配置即可完成重试，无需所有服务调用方自己发起重试。如这里对于满足条件的请求配置了3次重试，则当访问失败时，网格客户端代理会自动发起重试，业务完全无需关注。在实际使用中重试也是结合业务特点谨慎使用，如目标服务过载原因、逻辑错误，重试不能帮助提高总体成功率，反而会对系统产生更大的压力，可能引起重试风暴。\n熔断虽然有各种优点，但因为执行位置在服务调用方，可能会被绕过或者没有使用能力的客户端不生效。限流可以作为另外一种手段来进行保护。限流是保障服务高可用运行的重要手段，特别是对于一些关键的服务，经常需要综合多种因素进行容量规划。当出现瞬时的流量高峰时，通过限流的阈值时则会拒绝服务，从而防止系统过载，进而保障应用的可用性。一般结合业务特点通过一些配置可以做到在负载比较高时先保护核心业务的服务。保证过载的时候能提供基本能力，从而提高系统韧性。网格可以对网格内部任意一个微服务进行非侵入、透明的限流，可以限制四层的连接数，也可以限制七层的请求数。\n提高系统韧性的另外一个重要方面是尽量降低变更引起的风险。灰度是在发布过程中保证业务平稳运行升级的重要手段，在灰度过程中，新老版本同时生成环境在线，新版本只切分少量流量出来，在确认新版本没有问题后，再逐步加大流量比例。当新版本识别到有问题时，只会影响少量流量，并可以快速回退。\n灰度发布的核心部分是配置灰度的流量规则。网格的非侵入流量切分能力，能够灵活配置规则给不同版本分配流量。如这个例子中从生产版本V1中切分30%的流量到V2版本，验证V2版本的运行情况，确定是否在V2稳定运行后，把所有流量都切换到灰度的V2版本上。\n除了支持基于前面流量比例的灰度策略外，网格还支持非常灵活的基于请求内容的灰度策略。请求中的任何字段都可以作为流量标准，从而精准的将只是某种特点的流量分发到灰度版本，其他特征的流量仍然路由到原有版本上，对大部分用户不会产生影响。如实例中只有请求的Header group取值dev的流量分发到灰度V2版本，其他流量还走V1版本。\n通过适当冗余来提高系统可用性是一个非常通用的实践。早期修建桥梁，桥墩底下的钻孔桩个数一般都会要求比力学核算的个数多好几个。在计算机世界中，这个理论更是随处可见。如**微服务部署多实例部署除了负载分担外，另外一个重要的用途就是提供适当的冗余，部分实例故障服务还是可用。**在云原生场景下，基于容器可以非常容易的对负载配置反亲和策略，将不同的实例部署在不同的节点，或者不同可用区的节点上。当某个可用区或者某个节点故障时，不影响最终业务，从而最大化的减少资源故障对服务整体的可用性影响。\n服务可靠性另外一个重要要求是减少升级过程对用户业务的影响，或者尽量保持业务不中断，对用户无感知。在我们提供的云原生解决方案中，Kubernetes滚动升级过程中，根据新版本的镜像先创建一定数据量的实例，然后逐步停用老的实例，滚动的完成新实例替换老实例的过程。在这个示例中，当服务配置了滚动升级策略，则在升级过程中就会滚动的先创建一个新实例，停用一个旧实例，然后再创建一个新实例，停用另外一个旧实例，最终创建3个新的实例，老的实例都逐步停用。\n保证系统韧性的另外一个重要方面是在系统过载时能及时扩容。基于K8s提供的HPA，可以基于观测的指标变化自动的扩展实例。如实例中，当检测到CPU或者PPS高时，会自动的扩展服务实例，保证负载的CPU不高于60%，PPS不超过1200. 对于网格管理的服务间访问，通过非侵入方式收集服务的吞吐和延时，可以支持基于这些请求指标进行自动扩缩容，保证用户服务的访问质量。\n故障注入是一种评估系统可靠性的有效方法，通过向系统注入在实际应用中可能发生的故障，观察系统功能性能变化，故障检测、定位、隔离以及故障恢复情况，发现产品缺陷。\nIstio提供了非侵入的额故障注入能力，无需修改业务代码，而是在网格中对特定的应用层协议进行故障注入，模拟出应用的故障场景。如所示，可以对某种请求注入5秒的延时，验证frontend程序对这种情况的处理。\n另外也可以注入一个特定的故障，测试服务在上游服务故障时能否最大化的保证业务可用，体现系统在业务实现上韧性满足要求。如所示，可以对某种请求注入一个指定的HTTP Code，如此对于访问的客户端来说，就与服务端发生异常一样。\n除了上面介绍的手段外，故障策略中最基础的是发现故障。调用链正是应对和解决大规模复杂的分布式系统在运行中遇到故障定位问题的一个有效手段。不同于早期的调用链，大部分是提供埋点的SDK让用户选择在自己的代码中进行调用链埋点，和网格结合后，这部分复杂的埋点在网格数据面完成。对使用者更简单，只要通过适当配置，就可以生成详细的调用链信息，对服务间的调用进行管理。\n以上就是本次大会分享的主要内容，谢谢大家。\n附： 演讲材料(官方Slides) Video(YouTube) 视频(国内) Sched 主页 ","link":"https://idouba.com/kubecon2021-online-video-upgrades-resilience-from-sc-circuit-breaker-to-service-mesh-kubecon2021/","section":"posts","tags":["KubeCon","Istio","Spring Cloud","微服务","实践"],"title":"KubeCon2021：服务网格替代 Hystrix 提升在线视频服务韧性的生产实践"},{"body":"","link":"https://idouba.com/tags/spring-cloud/","section":"tags","tags":null,"title":"Spring Cloud"},{"body":"","link":"https://idouba.com/tags/%E5%AE%9E%E8%B7%B5/","section":"tags","tags":null,"title":"实践"},{"body":"","link":"https://idouba.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/","section":"tags","tags":null,"title":"微服务"},{"body":"","link":"https://idouba.com/tags/servicemeshcon/","section":"tags","tags":null,"title":"ServiceMeshCon"},{"body":"","link":"https://idouba.com/categories/servicemeshcon/","section":"categories","tags":null,"title":"ServiceMeshCon"},{"body":" 记录在2021年5月4日在欧洲ServiceMeshCon上发表的技术演讲《Kubernetes and Service Mesh Upgrade Automobile Company’s IT Infrastructure》，分享了一个服务网格在一个客户的实践案例。\n议题： Rapid business development brings a great challenge to automobile manufacturing company’s IT platforms. In this presentation, Chaomeng will share a practice of upgrading the traditional IT built microservice platform to cloud native infrastructure. That is gradually transforming the self-developed inner DNS plus ELB for service discovery and load balance, per VM nginx for inbound traffic management, metric, and access log, to Kubernetes and service mesh.\nThe practice solves the problems that every service request crosses too many heterogeneous middleware and proxies, and turns out to provide lightweight resource management, auto-scaling, canary, non-intrusive and transparent traffic management, security and observability on a consistent infrastructure, and thus improve ops efficiency, makes ops work simpler and easier.\n正文： Hello everyone, I'm chaomeng from HUAWEI. My topic today is \u0026quot;Kubernetes and Service mesh upgrade automobile Company IT infrastructure\u0026quot;. I will introduce a service mesh practice in production environment. Since launched in 2018,HUAWEI Application Service Mesh ASM has served a large-number of customers. The practice is about one of them, a leading automobile manufacture in China. I will introduce how cloud native help upgrade their It infrastructure.\nAbout me. I am architect of huawei application service mesh. Currently develop cloud-native projects, such as service mesh, kubernetes, micro service. And also help promote service mesh and cloud native in China, I am author of book \u0026quot;Cloud Native Service Mesh Istio\u0026quot;, help people learn service mesh and istio.\nMy talks include following three parts. The first part is the customer business background and current solution, The second part is about the challenge of current solution, and most importantly target cloud native solution. To solve these challenge And finally, I will introduce how to migrate current solution to target cloud native solution.\nOur customer is a leading automobile manufacture. With the growth of automobile demand in China, the company business developed dramatically, especially their new energy vehicles. And put great pressure on the IT infrastructure. Such as: Increased complexity, as application become complicated, and need to integrate with other platform. Increased Capacity, as number of vehicles increased. And more security requirements, includes access control and service authentication. And heavy ops works and high IT cost are also big problems.\nThis is the current architecture. The customers IT engineer told us: \u0026quot;they say No to the popular micro service framework several years ago. Instead, build their own micro service platform based on DNS, ELB and Nginx. And provide service discovery, load balancing, in an independent platform rather than in application .\nThe total architecture greatly depend on the central ELB, provides load balancing for internal service communication. And DNS is responsible for internal service resolution. ingress Nginx provide TLS termination for ingress TLS traffic, Zuul play the role of L7 application gateway, nginx on each node proxy traffic to local service instance.\nTo better understand the difference of current solution and target cloud native solution, we will compare the abstraction view of two architectures. An abstraction of the current architectures will looks like this. The top is application layer. Applications are developed with Multilanguage. And the third layer is responsible for deploy environment. Currently applications are deployed on Virtual machine and BMS. The central part is the second layer, provide service management, by integrating ELB, DNS and Nginx.\nBecause of second layer micro service platform, the applications are mainly developed in Spring boot, instead of spring cloud.\nAnd this is an abstraction of target solution, compared with the previous one, the main difference is the second and third layer.\nReplace the ELB,DNS,Nginx integrated platform with a unified service mesh infrastructure. And upgrade deployment from VM and BMS to kubernetes.\nThe total architecture does not changed a lot. So our customer engineer proudly called their current solution \u0026quot;a self-developed mesh-like infrastructure before service mesh period\u0026quot; Next we will introduce the challenges of the current solution through several aspects, and focus on how the target cloud native solution solve these challenges respectively.\nFirst, Let see Service discovery and load balance. With current solution DNS and ELB play an important role in service discovery and load balance That is :\nVM service instance bind to ELB Register service name and ELB IP to DNS Consumer call DNS and send request to the resolved ELB IP ELB send traffic to selected VM instance Per node Nginx proxy request to local service instance And with target cloud native solution, by Kubernetes and Istio:\nNo need service registry. Istio automatically retrieve registry data from kubernetes Client side proxy intercept traffic And perform service discovery Then send request to one selected instance Server side proxy intercept traffic, perform server side traffic management Compare two solutions by following views. First, As for Service registry, the former need service name and ELB register to DNS, and the latter No need registry. And As for service discovery, the former is by ELB and DNS, the latter is by mesh data plane and control plane. The former Load balance greatly depend on ELB, and the latter is by client side proxy.When deploy a new service, with current solution, administrator has to manually register new service to DNS. But with target solution, Istio can get the service and its instances automatically from kubernetes. With current solution, Consumer need send request to internal ELB, ELB and Nginx act as static proxy; And with target solution, Consumer send request to target service, request intercepted by mesh data plane, which act as transparent proxy, perform service discovery and load balance.\nAnd next is canary release, Canary is one important part in our customers daily work. With current solution, ELB is responsible for routing traffic to vm instances. And traffic to different version depends on the number of instances.\nAs there are 3 instances of Version one and 2 instances of Version two, so that 60% traffic will be sent to Version one, and 40% will be sent to Version two\nWith target solution, By Istio traffic weight for different version can be specified. Weights of the version control proportion of traffic each version received. The above rule will route 30% of traffic to instances of version two, and the other 70% traffic to version one no matter how many instances each version has.\nWith target solution, Service mesh can do canary release for a group of services. Set route for first service, and the other service in the group just following the route. Makes traffic from version one be sent to Version one, and from version two be sent to version two\nThis table summarize difference of two canary. As for Weighted policy, with current solution, Weight is controlled by instance num. And with target solution, weight can be specified flexibly.\nAnd former does not support Traffic match policy, latter can control route traffic to certain version according to match condition And current solution only support L4 traffic, target solution support both L7 and L4, include TCP, TLS, HTTP, gRPC With current solution, some L7 policy is developed in application code, but with target solution, all is by platform, no need changing application code.\nAs mentioned in application background, there are increasingly security requirement with the development of business. But, with current solution, only TLS termination is provide for the ingress traffic. That is:\nNginx provide TLS termination. All applications have to develop https service, maintain key and certificate by themselves. And Access control is embedded in application, for some confidential interface. And with target solution, Service mesh provide transparent security. : Provide TLS termination by gateway: Operator upload key and certificate in the form of kubernetes secret. : And Istio offers mutual TLS for transport authentication, just enable it without any service code changes. : Include: Provides each service with an identity representing its role. Secures service-to-service communication. Automate key and certificate generation, rotation, and distribution. Provide authority, access control for target service or interface\nThis table shows the difference of two security. Both provide TLS termination and JWT authentication. But the former does not provide any service to service security. Latter provides: Mutual TLS authentication by proxy Transparent TLS encryption. A key management system automate key and certificate generation, distribution, and rotation. Flexible access control by custom condition, such as header, source or target IP And target solution meet our customers security requirement perfectly in this practice.\nWith system becoming complicated, need observability to troubleshoot, and optimize their applications. : With the current solution: Per node nginx generate access log Nginx exporter export metric Tracing agent in each node generate tracing for java application\nAnd With target solution: Istio generates metric, trace, access log, for all service by sidecar. Service developer do not need do any extra work for this. First proxy generates service-oriented metrics. Covers: latency, errors and saturation.\nAnd : proxy generates spans on behalf of the applications Proxy also generates access logs for service traffic.\nCompared two solution. The main difference is that service mesh can help collect all the observability data for any language application, with more extensibility, transparency, flexibility. Proxy generate all kinds of metrics, and metadata or dimensions can be configured Proxy Generate spans for application, applications only need to propagate several request header. And Proxy generate access log, format can be configured Based on access metric, topology can be built, give a global view of the application and services.\nOkay, this is the target cloud native architecture looks like this: A unified infrastructure Kubernets and Istio work together, not only provide application deploy environment, but also provide service managing platform. Covers: canary release, service discovery, load balance, connection management, circuit breaker, fault injection, traffic mirror, retry, redirection, authentication, authority, metric, tracing and so on. Data plane work as a transparent proxy, perform traffic managing, security, observability on behave of application. Ingress Gateway give more flexibility than Nginx, manage traffic together with sidecars. Control plan is responsible for storing and managing the configuration锛宎nd distribute policies to proxies and gateway. And the solution can easily configured to integrate with customers existing canary, chaos platform, and metric, logging, tracing system. By separating all common functions from application code, and offload to infrastructure, help developers focus on the business logic.\nSummary the above aspect by this table. The key difference is architecture and mechanism. The former is an integrated platform provide basic service discovery and load balance. And the latter is an infrastructure designed to handle application communication As for Components , The former platform Consists of different components ELB, DNS,Nginx, and latter is a unified infrastructure, Include control plane and data plane. Both based on proxies, but the former is static proxy. And the latter is transparent proxy intercept and manage service traffic. This is the biggest difference, and the most important characteristic of service mesh. And as for ops work, the former greatly depend on manual operation, the latter mostly works automatically. As for Service management, the former only provide service discovery and loadbalace, Retry is coded in application. Latter Provide powerful service management, covers connection, observability and security. This difference result in both cost and resource reduction, help customer reduce IT investment.\nNext, the final part is how to migrate user's current solution to target cloud native solution safely and gracefully.\nThe main idea is deploy a new environment with target solution, and gradually split the traffic from VM cluster to Kubernetes cluster. To ensure reliability of online service, it is required For each service failover to VM instances when new container instances are unavailable.\nFirst, to failover between VM instance and container instance, it is required VM instance and Container instance bind to one service, share the same service discovery and load balance. And our solution is One service refer to container and VM instances by assigning the same label selector, and route traffic to both containers and VMs.\nWhen consumer service call target service vehicle, traffic can be route to container instance, also can be routed to VM instance.\nNext, and most important, it is required: Retry to VM instances when container instances not work And we set retryRemoteLocalities to specify Retry to VM when container instances are not available. The process will looks like this: Request to container instances fails for some environment or runtime reason Automatically retry to remote VM instances. Retry success, and make consumer request success.\nFinally, It is required load balance traffic to container instances with high priority, failover to VM instances only when container instances are unhealthy. And our solution is Istio locality load balancing, split a small part of traffic to VM instance when containers are unhealthy. Switch all traffic to VMs only when containers are totally unhealthy As show in the table, even half of container instances are unhealthy, they still get seventy percent traffic. It is required to make sure both primary container instances and secondary VM instance meet traffic capacity requirement.\nThat is all of the practice, thank you, for your time.\n附： 演讲材料(scripts) 官方Sched主页 ","link":"https://idouba.com/servicemeshcon2021-kubernetes-and-service-mesh-upgrade-automobile-company%E2%80%99s-it-infrastructure/","section":"posts","tags":["Istio","ServiceMeshCon","演讲","服务网格"],"title":"ServiceMeshCon2021：Kubernetes 和 Service Mesh 升级汽车公司的 IT 基础设施"},{"body":" 记录在北京时间2月23日，在全球首届社区峰会IstioCon 2021中，发表的《Best practice:from Spring Cloud to Istio》技术演讲。回答经常被客户和同事们问到的一个问题，SpringCloud和Istio的关系，如何演进。\n议题： 正文: 大家好，我是来自华为云的工程师。很荣幸有机会和大家分享Istio在生产中使用的实际案例。\n华为云应用服务网格从2018年在公有云上线， 作为全球最早的几个网格服务之一，经历和见证了从早期对网格的了解、尝试到当前大规模使用的过程。服务的客户越来越多，场景也越来越复杂。这其中的通用功能作为feature大都贡献到Istio社区，解决方案层面的实践也希望通过这样的机会和大家交流。\n本次我选取的主题是Spring Cloud to Istio。来自我们客户的Spring cloud的项目和Istio的结合与迁移案例。\n演讲主要包含四部分的内容： 1）背景介绍\n2）使用Spring cloud微服务框架遇到的问题\n3）解决方案\n4）通过示例来描述方案的实践细节\n背景介绍 还是以微服务为切入点，微服务的诸多优势非常明显，但相应给整个系统带来的复杂度也非常显著。单体的系统变成了分布式后，网络问题，服务如何找到并访问到对端的服务发现问题，网络访问的容错保护问题等。连当年最简单的通过日志中的调用栈就能实现的问题定位，微服务化后必须要通过分布式调用链才能支持。怎样解决微服务带来的这些挑战？\n微服务SDK曾经是一个常用的解决方案。将微服务化后通用的能力封装在一个开发框架中，开发者使用这个框架开发写自己的业务代码，生成的微服务自然就内置了这些能力。在很长的一段时间内，这种形态是微服务治理的标配，以至于初学者以为只有这些SDK才是微服务。\n服务网格则通过另一种形态提供治理能力。不同于SDK方式，服务治理的能力在一个独立的代理进程中提供，完全和开发解耦。虽然从图上看两者差异非常小，后面我们将会从架构和实际案例来分析两者在设计理念上的差异，来体会前者是一个开发框架，而后者是一个基础设施。\nSDK形态中Spring cloud是最有影响力的代表项目。Spring cloud提供了构建分布式应用的开发工具集，如列表所示。其中被大部分开发者熟知的是微服务相关项目，如：服务注册发现eureka、配置管理 config、负载均衡ribbon、熔断容错Hystrix、调用链埋点sleuth、网关zuul或Spring cloud gateway等项目。在本次分享中提到的Spring cloud也特指Spring cloud的微服务开发套件。\n而网格形态中，最有影响力的项目当属Istio。Istio的这张架构图在这次演讲中会高频出现。作为本次分享的背景，我们只要知道架构上由控制面和数据面组成，控制面管理网格里面的服务和对服务配置的各种规则。数据面上每个服务间的出流量和入流量都会被和服务同POD的数据面代理拦截和执行流量管理的动作。\n除了架构外，作为背景的另外一个部分，我们挑两个基础功能稍微打开看下两者的设计和实现上的相同和不同。首先是服务发现和负载均衡。\n左边是Spring cloud，所有的微服务都会先注册中心，一般是Eureka进行服务注册，然后在服务访问时，consumer去注册中心进行服务发现得到待访问的目标服务的实例列表，使用客户端负载均衡ribbon选择一个服务实例发起访问。\n右边Istio不需要服务注册的过程，只需要从运行平台k8s中获取服务和实例的关系，在服务访问时，数据面代理Envoy拦截到流量，选择一个目标实例发送请求。可以看到都是基于服务发现数据进行客户端负载均衡，差别是服务发现数据来源不同，负载均衡的执行体不同。\n下面比较下熔断：\n左边为经典的Hystrix的状态迁移图。一段时间内实例连续的错误次数超过阈值则进入熔断开启状态，不接受请求；隔离一段时间后，会从熔断状态迁移到半熔断状态，如果正常则进入熔断关闭状态，可以接收请求；如果不正常则还是进入熔断开启状态。\nIstio中虽然没有显示的提供这样一个状态图，但是大家熟悉Istio规则和行为应该会发现，Istio中OutlierDection的阈值规则也都是这样设计的。两者的不同是Spring cloud的熔断是在SDK中Hystrix执行，Istio中是数据面proxy执行。Hystrix因为在业务代码中，允许用户通过编程做一些控制。\n以上分析可以看到服务发现、负载均衡和熔断，能力和机制都是类似的。如果忽略图上的某些细节，粗的看框图模型都是完全一样的，对比表格中也一般只有一项就是执行位置不同，这一点不同在实际应用中带来非常大的差异。\n使用Spring cloud微服务框架遇到的问题 本次演讲的重点是实践。以下是我们客户找到我们TOP的几个的问题，剖析下用户使用传统微服务框架碰到了哪些问题，这些大部分也是他们选择网格的最大动力。\n1）多语言问题 在企业应用开发下，一个业务使用统一的开发框架是非常合理常见的，很多开发团队为了提升效率，经常还会维护有自己公司或者团队的通用开发框架。当然因为大部分业务系统都是基于Java开发，所以Spring cloud开发框架，或者衍生于Spring cloud的各种开发框架使用的尤其广泛。\n但是在云原生场景下，业务一般更加复杂多样，特别是涉及到很多即存的老系统。我们不能要求为了微服务化将在用的一组成熟服务用Spring cloud重写下。用户非常希望有一种方式不重写原来的系统也能对其进行应用层服务访问管理。\n2）将Spring cloud的微服务运行在K8s上会有很大的概率出现服务发现不及时 前面介绍过Spring cloud服务发现是基于各个微服务先向注册中心进行服务注册的数据来实现的，在传统Spring cloud场景下，当微服务部署在VM上，服务动态变化要求没有那么高，顶多个别实例运行不正常，通过服务发现的健康检查就足够了。但是在k8s场景下，服务实例动态迁移是非常正常场景。如图示，producer的某个Pod已经从一个节点迁移到另外一个节点了，这时需要新的pod2的producer实例向eureka注册，老实例Pod1要去注册。\n如果该情况频繁发生，会出现注册中心数据维护不及时，导致服务发现和负载均衡到旧的实例pod1上，从而引起访问失败的情况。\n3）升级所有应用以应对服务管理需求变化 第三个问题是一个比较典型的问题。客户有一个公共团队专门维护了一套基于Spring cloud的自有开发框架，在每次升级开发框架时，不得不求着业务团队来升级自己的服务。经常会SDK自身修改测试工作量不大，但却要制定很长周期的升级计划，来对上千个基于这个SDK开发的服务分组重新编译，打包，升级，而且经常要陪着业务团队在夜间变更。业务团队因为自身没有什么改动，考虑到这个升级带来的工作量和线上风险，一般也没有什么动力。\n4）从单体式架构向微服务架构迁移 这是一个比较普遍的问题，就是渐进的微服务化。马丁福勒在著名的文章单体到微服务的拆分中（https://martinfowler.com/articles/break-monolith-into-microservices.html ）也提到了对渐进微服务化的倡议，如何能从业务上将一个大的业务分割，解耦，然后逐步微服务化。马丁福勒强调 “解耦的是业务能力不是代码” ，大神将代码的解耦留给了开发者。\n但是站在开发者的角度讲渐进的微服务不是一个容易的事情。以基于Spring cloud框架进行微服务开发为例，为了所有的微服务间进行统一的服务发现、负载均衡，消费和执行同样的治理策略，必须要求所有的微服务基于同样的，甚至是统一版本的SDK来开发。\n当然我们客户在这种情况下也有基于API层面做适配，将原有的未微服务化的和已微服务化的并存，使用这种类似于灰度方式，实际操作非常麻烦。\n曾经有客户问过有没有不用费劲搞两套，是否可以直接有些大的单体微服务化，另外一些单体很长时间内完全不动，直到有时间或者认为安全想动它的时候去动。\n解决方案 对于客户实际碰到的4个典型的微服务框架的问题，我们推荐的解决方案都是服务网格。下面我们分别看下Istio如何解决上面的几个问题。\n首先，多语言问题。基于服务网格，业务和治理的数据面无需运行在同一个进程里，也无需一起编译，因此也没有语言和框架上的绑定。无论什么语言开发的服务，只要有一个对外可以被访问和管理的一定应用协议上的端口，都可以被网格进行管理。通过统一的网格控制面，下发统一的治理规则给统一的网格数据面执行，进行统一的治理动作，包括前面介绍到的灰度、流量、安全、可观察性等等。\n关于Spring cloud服务在Kubernetes运行时，关于原有的服务注册和发现不及时的问题。根本原因是两套服务发现导致的不一致问题，那么解决办法也比较简单，统一服务发现即可。既然K8s已经在Pod调度的同时维护有服务和实例间的数据，那么就没有必要再单独搞一套名字服务的机制，还要费劲的进行服务注册，然后再发现。\n比较之前Spring cloud注册发现那张图，注册中心没了，服务基于注册中心的服务注册和服务发现的动作也不需要了，Istio直接使用k8s的服务发现数据，但从架构上看也简洁很多。\n我们也总结过，大部分碰到这个问题的场景，都是将微服务框架从VM迁移到k8s时候碰到的，有点把容器当作之前的VM使用，只使用了k8s作为容器部署运行的平台，并没有用到k8s的service。\n对于SDK自身升级导致业务全部重新升级的问题，解决办法就是把服务治理的公共能力和业务解耦。在网格中，将治理能力下沉到基础设施后，业务的开发、部署、升级都和服务治理的基础设施解耦了。业务开发者专注自己的业务部分。只要没有修改业务代码，就无需重新编译和上线变更。\n当治理能力升级只需基础设施升级即可，基础设施的升级对用户业务完全没有影响。像华为云ASM这样大部分网格服务的服务提供商都能做到一键升级，用户完全感知不到这个过程。\n关于渐进微服务化的问题，使用Isito服务网格可以非常完美的解决。Istio治理的是服务间的访问，只要一个服务被其他服务访问，就可以通过Istio来进行管理，不管是微服务还是单体。Istio接管了服务的流量后，单体和微服务都可以接收统一的规则进行统一的管理。\n如图中，在微服务化的过程中，可以对某个单体应用svc1根据业务拆分优先进行微服务化，拆分成三个微服务svc11、svc12和svc13，svc1服务依赖的另外一个单体应用svc2不用做任何变更，在网格中运行起来就可以和另外三个微服务一样的被管理。同样在运行一段时间后，svc2服务可以根据自身的业务需要再进行微服务化。从而尽量避免一次大的重构带来的工作量和业务迁移的风险，真正做到马丁富勒倡导的渐进微服务化的实践。\n实践 以上是对实际工作中客户的几个典型问题的解决方案。在实践中，怎么把这些解决方案落地呢？下面基于实际客户案例总结，分享具体的实践。\n我们的主要是思路是解耦和卸载。卸载原有SDK中非开发的功能，SDK只提供代码框架、应用协议等开发功能。涉及到微服务治理的内容都卸载到基础设施去做。\n从图上可以看到开发人员接触到开发框架变薄了，开发人员的学习、使用和维护成本也相应的降低了。而基础设施变得厚重了，除了完成之前需要做的服务运行的基础能力外，还包括非侵入的服务治理能力。即将越来越多的之前认为是业务的能力提炼成通用能力，交给基础设施去做，交给云厂商去做，客户摆脱这些繁琐的非业务的事务，更多的时间和精力投入到业务的创新和开发上。在这种分工下，SDK才真的回归到开发框架的根本职能。\n要使用网格的能力，前提是微服务出来的流量能走到网格的数据面来。主要的迁移工作在微服务的服务调用方。我们推荐3个步骤：\n第一步：废弃原有的微服务注册中心，使用K8S的Service。\n第二步：短路SDK中服务发现和负载均衡等逻辑，直接使用k8s的服务名和端口访问目标服务；\n第三步：结合自身项目需要，逐步使用网格中的治理能力替换原有SDK中提供的对应功能，当然这步是可选的，如调用链埋点等原有功能使用满足要求，也可以作为应用自身功能保留。\n为了达成以上迁移，我们有两种方式，供不同的用户场景采用。\n一种是只修改配置的方式：Spring cloud本身除了支持基于Eureka的服务端的服务发现外，还可以给Ribbon配置静态服务实例地址。利用这种机制给对应微服务的后端实例地址中配置服务的Kubernetes服务名和端口。\n当Spring cloud框架中还是访问原有的服务端微服务名时，会将请求转发到k8s的服务和端口上。这样访问会被网格数据面拦截，从而走到网格数据面上来。服务发现负载均衡和各种流量规则都可以应用网格的能力。\n这种方式其实是用最小的修改将SDK的访问链路和网格数据面的访问链路串接起来。在平台中使用时，可以借助流水线工具辅助，减少直接修改配置文件的工作量和操作错误。可以看到我这个实际项目中，只是修改了项目的application.yaml配置文件，其他代码都是0修改。当然对于基于annotation的方式的配置也是同样的语义修改即可。\n前面一种方式对原有项目的修改比较少，但是Spring cloud的项目依赖都还在。\n我们有些客户选择了另外一种更简单直接的方式，既然原有SDK中服务发现负载均衡包括各种服务治理能力都不需要了，干脆这些依赖也全部干掉。从最终的镜像大小看，整个项目的体量也得到了极大的瘦身。这种方式客户根据自己的实际使用方式，进行各种裁剪，最终大部分是把Spring cloud退化成Spring boot。\n迁移中还有另外一部分比较特殊，就是微服务外部访问的Gateway。\nSpring cloud 有两种功能类似的Gateway，Zuul和Spring cloud Gateway。基于Eureka的服务发现，将内部微服务映射成外部服务，并且在入口处提供安全、分流等能力。在切换到k8s和Istio上来时，和内部服务一样，将入口各个服务的服务发现迁移到k8s上来。\n差别在于对于用户如果在Gateway上开发了很多私有的业务强相关的filter时，这时候Gateway其实是微服务的门面服务，为了业务延续性，方案上可以直接将其当成普通的微服务部署在网格中进行管理。\n但是大多数情况下我们推荐使用Istio的Ingress Gateway直接替换这个微服务网关，以非侵入的方式提供外部TLS终止、限流、流量切分等能力。\n经过以上的简单改造，各种不同语言、各种不同开发框架开发的服务，只要业务协议相通，彼此可以互相访问，访问协议可以被网格管理，就都可以通过Istio进行统一的管理。\n控制面上可以配置统一的服务管理规则。数据面上，统一使用Envoy进行服务发现、负载均衡和其他流量、安全、可观察性等相关能力。数据面上的服务即可以运行在容器里，也可以运行在虚机上。并且可以运行在多个k8s集群中。\n当然在迁移过程中间，我们也支持阶段性的保留原有微服务框架的注册中心，使Istio和其他的服务发现结合使用的中间状态，让网格中的服务可以访问到微服务注册中心的服务。\n这里是一个Spring cloud开发的服务运行在Istio服务网格上进行灰度发布的示例。上面的日志是服务调用方Sidecar的日志，可以看到网格将流量分发到不同的服务后端上。下面的截图是使用了华为云ASM服务的灰度功能，可以看到这个Spring cloud服务通过网格配置的分流策略，将30%的流量分发到灰度版本上。\n下面这个示例是Spring cloud开发的服务使用Istio的熔断功能。这个过程就是就是前面原理一节Hystrix的状态迁移图的实践，不同在于这个实现是基于Istio来实现的。基于服务网格不管这里的服务是什么语言或者框架开发的，都可以对访问进行熔断保护。\n这里的效果截图是来自华为云应用服务网格ASM的应用拓扑，可以非常清新的看到服务级别、服务实例级别流量变化情况，服务和服务实例的健康状态，从而展示故障的Spring cloud实例被隔离的全过程。从拓扑图上可以看到有个实例异常满足熔断阈值，触发了熔断，网格数据面向这个故障实例上分发的流量逐渐减少，直到完全没有流量，即故障实例被隔离。通过这种熔断保护保障服务整体访问的成功率。\n下面三个流量拓扑演示了故障恢复的过程。\n可以看到：\n初始状态这个故障实例被隔离中，没有流量；\n当实例自身正常后，网格数据面在将其隔离配置的间隔后，重新尝试给其分配流量，当满足阈值要求则该实例会被认为是正常实例，可以和其他两个实例一样接收请求。\n最终可以看到三个实例上均衡的处理请求。\n即实现了故障恢复。\n结语 最后，通过微服务、容器、k8s和Istio的关系图来总结今天的内容：\n1）微服务和容器都有轻量和敏捷的共同特点，容器是微服务非常适合的一个运行环境；\n2）在云原生场景下，在微服务场景下，容器从来都不是独立存在的，使用k8s来编排容器已经是一个事实标准；\n3）Istio和k8s在架构和应用场景上的紧密结合，一起提供了一个端到端的微服务运行和治理的平台。\n4）也是我们推荐的方案，使用Istio进行微服务治理正在成为越来越多用户的技术选择。\n以上四个关系顺时针结合在一起为我们的解决方案构造一个完整的闭环。\n完。\n附： 演讲材料(官方Slides) Video(YouTube) 视频(国内) 官方Sched主页 ","link":"https://idouba.com/istiocon2021-best-practice-from-spring-cloud-to-istio/","section":"posts","tags":["IstioCon","SpringCloud","服务网格","演讲","最佳实践"],"title":"IstioCon2021：SpringClod到Istio最佳实践"},{"body":"","link":"https://idouba.com/tags/springcloud/","section":"tags","tags":null,"title":"SpringCloud"},{"body":" 记录在2020年8月1日在KubeCon上发表的技术演讲《Kubernetes \u0026amp; Service Mesh Helps Online Collaboration During Coronavirus Time》，和来自云会议的同事谢飞一起分享了2019年新冠疫情期间Istio在云会议的应用。\n议题： During the period of coronavirus, lots of people required stay at home or different office, use Welink, an online collaboration platform, work together. The exponentially increased online users bring great performance and capacity challenges. In this Session, Chaomeng and Fei will share their technical experience of Kubernetes\u0026amp;Istio in Welink supporting large traffic from large amount of users’ meeting, mailing and other online collaborations. The talk focus on practice in large scale productive environment with heavy traffic. Includes:\nImplement a predictive scaling algorithm to improve the scaling efficiency. Propose a more flexible route chain to decouple configuration complexity (ready to contribute to community) Adopt microservice level canary release, non-intrusive monitoring, interface level rate limiting and transparent service security. 正文： 新冠疫情在2019年底爆发后，远程办公需求指数增长。云会议的业务快速扩展，给下层基础设施带来了强烈的挑战，就包括刚上线不久的服务网格。\n不会忘记和少东、佳青三人组打仗一样长时间高强度支撑会议的同时解决现网问题的哪些日日夜夜。记得当时都隔离在家里办公，早上起床坐在床上没有洗漱就开始了，中午吃饭时少东和佳青好几次是边做饭边在会上看问题。而自悲催的是他们的老大哥我，春节回老家了，初二一起严重紧急返杭。从此就被豆妈隔离在隔壁儿子的小房间，房间里儿子的学习桌也给搬走了。于是这个月里常规的姿势是这样：坐在一个小板凳上，笔记本放在比它面积还小的一个落地小米空气净化器上，带着大耳机，鼠标在大腿上摩擦。更悲催的是，没过几天小房间的灯坏掉了，于是这个画面又增加了一份昏暗的色调，特别是从傍晚到沈阳，就是一个电脑屏幕前趴着一个鬼影，看着滚动的控制台日志，对着耳机喊叫。\n附： 演讲材料(官方Slides) 官方Sched主页 ","link":"https://idouba.com/kubeco2020-kubernetes-and-service-mesh-helps-online-collaboration-during-coronavirus-time/","section":"posts","tags":["Istio","KubeCon","演讲","新冠"],"title":"KubeCon2020：Kubernetes和服务网格在冠状病毒期间助力在线协作"},{"body":"","link":"https://idouba.com/tags/%E6%96%B0%E5%86%A0/","section":"tags","tags":null,"title":"新冠"},{"body":"","link":"https://idouba.com/tags/%E4%BA%91%E5%8E%9F%E7%94%9F%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BCistio%E5%8E%9F%E7%90%86%E5%AE%9E%E8%B7%B5%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","section":"tags","tags":null,"title":"《云原生服务网格Istio:原理,实践,架构与源码解析》"},{"body":"本节书摘来自华为云原生技术丛书《云原生服务网格Istio:原理,实践,架构与源码解析》一书实践篇的第10章灰度发布实践。更多内容参照原书，或者关注容器魔方公众号。作者：star\n目前一些大型的互联网或金融行业的公司，都有自己的发布系统。但是对一些初创公司，从零开始构建这样一套系统并不简单，有一定的门槛。利用Istio提供的流量路由功能可以很方便地构建一个流量分配系统来做灰度发布和AB测试。\n预先准备： 将所有流量都路由到各个服务的v1版本\n在开始本章的实践前，先将frontend、advertisement和forecast服务的v1版本部署到集群中，命名空间是weather，执行如下命令确认Pod成功启动：\n1$ kubectl get pods -n weather 2NAME READY STATUS RESTARTS AGE 3advertisement-v1-6f69c464b8-5xqjv 2/2 Running 0 1m 4forecast-v1-65599b68c7-sw6tx 2/2 Running 0 1m 5frontend-v1-67595b66b8-jxnzv 2/2 Running 0 1m 对每个服务都创建各自的VirtualService和DestinationRule资源，将访问请求路由到所有服务的v1版本：\n1$ kubectl apply -f install/destination-rule-v1.yaml -n weather 2$ kubectl apply -f install/virtual-service-v1.yaml -n weather 查看配置的路由规则，以forecast服务为例：\n1$ kubectl get vs -n weather forecast-route -o yaml 2apiVersion: networking.istio.io/v1alpha3 3kind: VirtualService 4…… 5 name: forecast-route 6 namespace: weather 7…… 8spec: 9 hosts: 10 - forecast 11 http: 12 - route: 13 - destination: 14 host: forecast 15 subset: v1 在浏览器中多次加载前台页面，并查询城市的天气信息，确认显示正常。各个服务之间的调用关系如图10-1所示。\n图10-1 各个服务之间的调用关系\n基于流量比例的路由 Istio能够提供基于百分数比例的流量控制，精确地将不同比例的流量分发给指定的版本。这种基于流量比例的路由策略用于典型的灰度发布场景。\n1．实战目标 用户需要软件能够根据不同的天气状况推荐合适的穿衣和运动信息。于是开发人员增加了recommendation新服务，并升级forecast服务到v2版本来调用recommendation服务。在新特性上线时，运维人员首先部署forecast服务的v2版本和recommendation服务，并对forecast服务的v2版本进行灰度发布。\n2．实战演练 （1）部署recommendation服务和forecast服务的v2版本：\n1$ kubectl apply -f install/recommendation-service/recommendation-all.yaml -n weather 2$ kubectl apply -f install/forecast-service/forecast-v2-deployment.yaml -n weather 执行如下命令确认部署成功：\n1$ kubectl get po -n weather 2NAME READY STATUS RESTARTS AGE 3advertisement-v1-6f69c464b8-5xqjv 2/2 Running 0 33m 4forecast-v1-65599b68c7-sw6tx 2/2 Running 0 33m 5forecast-v2-5475655ff9-zq68g 2/2 Running 0 11s 6frontend-v1-67595b66b8-jxnzv 2/2 Running 0 33m 7recommendation-v1-86f5448b7d-xdc72 2/2 Running 0 23s （2）执行如下命令更新forecast服务的DestinationRule：\n$ kubectl apply -f install/forecast-service/forecast-v2-destination.yaml -n weather 查看下发成功的配置，可以看到增加了v2版本subset的定义：\n1$ kubectl get dr forecast-dr -o yaml -n weather 1…… 2spec: 3 host: forecast 4 subsets: 5 - labels: 6 version: v1 7 name: v1 8 - labels: 9 version: v2 10 name: v2 这时在浏览器中查询天气，不会出现推荐信息，因为所有流量依然都被路由到forecast服务的v1版本，不会调用recommendation服务。\n（3）执行如下命令配置forecast服务的路由规则：\n1$ kubectl apply -f chapter-files/canary-release/vs-forecast-weight-based-50.yaml -n weather 查看forecast服务的VirtualService配置，其中的weight字段显示了相应服务的流量占比：\n1$ kubectl get vs forecast-route -oyaml -n weather 1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4…… 5 name: forecast-route 6 namespace: weather 7…… 8spec: 9 hosts: 10 - forecast 11 http: 12- route: 13 - destination: 14 host: forecast 15 subset: v1 16 weight: 50 17 - destination: 18 host: forecast 19 subset: v2 20 weight: 50 在浏览器中查看配置后的效果：多次刷新页面查询天气，可以发现在大约50%的情况下不显示推荐服务，表示调用了forecast服务的v1版本；在另外50%的情况下显示推荐服务，表示调用了forecast服务的v2版本。我们也可以通过可视化工具来进一步确认流量数据，如图10-2所示。\n图10-2 通过可视化工具进一步确认流量数据\n（4）逐步增加forecast服务的v2版本的流量比例，直到流量全部被路由到v2版本：\n1$ kubectl apply -f chapter-files/canary-release/vs-forecast-weight-based-v2.yaml -n weather 查看forecast服务的VirtualService配置，可以看到v2版本的流量比例被设置为100：\n1$ kubectl get vs forecast-route -oyaml -n weather 1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4…… 5 name: forecast-route 6 namespace: weather 7…… 8spec: 9 hosts: 10 - forecast 11 http: 12 - route: 13 - destination: 14 host: forecast 15 subset: v1 16 weight: 0 17 - destination: 18 host: forecast 19 subset: v2 20 weight: 100 在浏览器中查看配置后的效果：多次刷新页面查询天气，每次都会出现推荐信息，说明访问请求都被路由到了forecast服务的v2版本。可通过可视化工具进一步确认准确的流量数据，如图10-3所示。\n图10-3 通过可视化工具进一步确认流量数据\n（5）保留forecast服务的老版本v1一段时间，在确认v2版本的各性能指标稳定后，删除老版本v1的所有资源，完成灰度发布。\n基于请求内容的路由 Istio可以基于不同的请求内容将流量路由到不同的版本，这种策略一方面被应用于AB测试的场景中，另一方面配合基于流量比例的规则被应用于较复杂的灰度发布场景中，例如组合条件路由。\n1．实战目标 在生产环境中同时上线了forecast服务的v1和v2版本，运维人员期望让不同的终端用户访问不同的版本，例如：让使用Chrome浏览器的用户看到推荐信息，但让使用其他浏览器的用户看不到推荐信息。\n2．实战演练 参照10.2.2节在集群中部署recommendation服务和forecast服务的v2版本，并更新forecast服务的DestinationRule。\n执行如下命令配置forecast服务的路由规则：\n1$ kubectl apply -f chapter-files/canary-release/vs-forecast-header-based.yaml -n weather 在浏览器中查看配置后的效果：用Chrome浏览器多次查询天气信息，发现始终显示推荐信息，说明访问到forecast服务的v2版本；用IE或Firefox浏览器多次查询天气信息，发现始终不显示推荐信息，说明访问到forecast服务的v1版本。\n3．工作原理 使用kubectl命令查看forecast服务的路由配置：\n1$ kubectl get vs forecast-route -oyaml -n weather 1…… 2 hosts: 3 - forecast 4 http: 5 - match: 6 - headers: 7 User-Agent: 8 regex: .*(Chrome/([\\d.]+)).* 9 route: 10 - destination: 11 host: forecast 12 subset: v2 13 - route: 14 - destination: 15 host: forecast 16 subset: v1 在上面的路由规则中，match条件使来自Chrome浏览器的请求被路由到forecast服务的v2版本，使来自其他浏览器的请求被路由到forecast服务的v1版本。\n","link":"https://idouba.com/istio-canary-release-pratice-of-cloudnativeistio-05/","section":"posts","tags":["《云原生服务网格Istio:原理,实践,架构与源码解析》","Istio"],"title":"Istio灰度发布实践 –《云原生服务网格Istio》书摘05"},{"body":"本节书摘来自华为云原生技术丛书《云原生服务网格Istio:原理,实践,架构与源码解析》一书原理篇的第6章透明的Sidecar机制，6.1.1小节Sidecar Injector自动注入的原理。更多内容参照原书，或者关注容器魔方公众号。\nSidecar注入 我们都知道，Istio的流量管理、策略、遥测等功能无须应用程序做任何改动，这种无侵入式的方式全部依赖于Sidecar。应用程序发送或者接收的流量都被Sidecar拦截，并由Sidecar进行认证、鉴权、策略执行及遥测数据上报等众多治理功能。\n如图6-1所示，在Kubernetes中，Sidecar容器与应用容器共存于同一个Pod中，并且共享同一个Network Namespaces，因此Sidecar容器与应用容器共享同一个网络协议栈，这也是Sidecar能够通过iptables拦截应用进出口流量的根本原因。\n图6-1 Istio的Sidecar模式\n在Istio中进行Sidecar注入有两种方式：一种是通过istioctl命令行工具手动注入;另一种是通Istio Sidecar Injector自动注入。\n这两种方式的最终目的都是在应用Pod中注入init容器及istio-proxy容器这两个Sidecar容器。如下所示，通过部署Istio的sleep应用，Sidecar是通过sidecar-injector自动注入的，查看注入的Sidecar容器：\n（1）istio-proxy 容器： 1- args: # istio-proxy 容器命令行参数 2 - proxy 3- sidecar 4 - --domain 5- $(POD_NAMESPACE).svc.cluster.local 6 - --configPath 7- /etc/istio/proxy 8- --binaryPath 9 - /usr/local/bin/envoy 10 - --serviceCluster 11 - sleep.default 12 - --drainDuration 13- 45s 14 - --parentShutdownDuration 15- 1m0s 16 - --discoveryAddress 17 - istio-pilot.istio-system:15011 18 - --zipkinAddress 19 - zipkin.istio-system:9411 20 - --connectTimeout 21 - 10s 22 - --proxyAdminPort 23- \u0026#34;15000\u0026#34; 24 - --controlPlaneAuthPolicy 25 - MUTUAL_TLS 26 - --statusPort 27- \u0026#34;15020\u0026#34; 28 - --applicationPorts 29 - \u0026#34;\u0026#34; 30 env: # istio-proxy 容器环境变量 31 - name: POD_NAME 32 valueFrom: 33 fieldRef: 34 apiVersion: v1 35 fieldPath: metadata.name 36 - name: POD_NAMESPACE 37 valueFrom: 38 fieldRef: 39 apiVersion: v1 40 fieldPath: metadata.namespace 41 - name: INSTANCE_IP 42 valueFrom: 43 fieldRef: 44 apiVersion: v1 45 fieldPath: status.podIP 46 - name: ISTIO_META_POD_NAME 47 valueFrom: 48 fieldRef: 49 apiVersion: v1 50 fieldPath: metadata.name 51- name: ISTIO_META_CONFIG_NAMESPACE 52 valueFrom: 53 fieldRef: 54 apiVersion: v1 55 fieldPath: metadata.namespace 56- name: ISTIO_META_INTERCEPTION_MODE 57 value: REDIRECT 58- name: ISTIO_METAJSON_LABELS 59 value: | 60 {\u0026#34;app\u0026#34;:\u0026#34;sleep\u0026#34;,\u0026#34;pod-template-hash\u0026#34;:\u0026#34;7f59fddf5f\u0026#34;} 61 image: gcr.io/istio-release/proxyv2:release-1.1-20190124-15-51 62 imagePullPolicy: IfNotPresent 63 name: istio-proxy 64 …… 65 volumeMounts: # istio-proxy挂载的证书及配置文件 66- mountPath: /etc/istio/proxy 67 name: istio-envoy 68 - mountPath: /etc/certs/ 69 name: istio-certs 70 readOnly: true 71- mountPath: /var/run/secrets/kubernetes.io/serviceaccount 72 name: sleep-token-266l9 73 readOnly: true （2）istio-init容器： 1initContainers: # istio-init容器，用于初始化Pod网络 2- args: 3 - -p 4- \u0026#34;15001\u0026#34; 5 - -u 6- \u0026#34;1337\u0026#34; 7 - -m 8- REDIRECT 9 - -i 10 - \u0026#39;*\u0026#39; 11 - -x 12 - \u0026#34;\u0026#34; 13 - -b 14 - \u0026#34;\u0026#34; 15 - -d 16- \u0026#34;15020\u0026#34; 17image: gcr.io/istio-release/proxy_init:release-1.1-20190124-15-51 18 imagePullPolicy: IfNotPresent 19 name: istio-init 20 …… 21 securityContext: 22 capabilities: 23 add: 24 - NET_ADMIN 25 procMount: Default Sidecar Injector自动注入的原理 Sidecar Injector是Istio中实现自动注入Sidecar的组件，它是以Kubernetes准入控制器Admission Controller的形式运行的。Admission Controller的基本工作原理是拦截Kube-apiserver的请求，在对象持久化之前、认证鉴权之后进行拦截。Admission Controller有两种：一种是内置的，另一种是用户自定义的。Kubernetes允许用户以Webhook的方式自定义准入控制器，Sidecar Injector就是这样一种特殊的MutatingAdmissionWebhook。\n如图6-2所示，Sidecar Injector只在创建Pod时进行Sidecar容器注入，在Pod的创建请求到达Kube-apiserver后，首先进行认证鉴权，然后在准入控制阶段，Kube-apiserver以REST的方式同步调用Sidecar Injector Webhook服务进行init与istio-proxy容器的注入，最后将Pod对象持久化存储到etcd中。\n图6-2 Sidecar Injector的工作原理\nSidecar Injector可以通过MutatingWebhookConfiguration API动态配置生效，Istio中的MutatingWebhook配置如下：\n1apiVersion: admissionregistration.k8s.io/v1beta1 2kind: MutatingWebhookConfiguration 3metadata: 4 creationTimestamp: \u0026#34;2019-02-12T06:00:51Z\u0026#34; 5 generation: 4 6 labels: 7 app: sidecarInjectorWebhook 8 chart: sidecarInjectorWebhook 9 heritage: Tiller 10 release: istio 11 name: istio-sidecar-injector 12resourceVersion: \u0026#34;2974010\u0026#34; 13 selfLink: /apis/admissionregistration.k8s.io/v1beta1/mutatingwebhookconfigurations/istio-sidecar-injector 14 uid: 8d62addb-2e8b-11e9-b464-fa163ed0737f 15webhooks: 16- clientConfig: 17 caBundle: …… 18 service: 19 name: istio-sidecar-injector 20 namespace: istio-system 21 path: /inject 22 failurePolicy: Fail 23 name: sidecar-injector.istio.io 24 namespaceSelector: 25 matchLabels: 26 istio-injection: enabled 27 rules: 28 - apiGroups: 29 - \u0026#34;\u0026#34; 30apiVersions: 31 - v1 32 operations: 33- CREATE 34 resources: 35 - pods 36 sideEffects: Unknown 从以上配置可知，Sidecar Injector只对标签匹配“istio-injection: enabled”的命名空间下的Pod资源对象的创建生效。Webhook服务的访问路径为“/inject”，地址及访问凭证等都在clientConfig字段下进行配置。\nIstio Sidecar Injector组件是由sidecar-injector进程实现的，本书在之后将二者视为同一概念。Sidecar Injector的实现主要由两部分组成：\n维护MutatingWebhookConfiguration； 启动Webhook Server，为应用工作负载自动注入Sidecar容器。 MutatingWebhookConfiguration对象的维护主要指监听本地证书的变化及Kubernetes MutatingWebhookConfiguration资源的变化，以检查CA证书或者CA数据是否有更新，并且在本地CA证书与MutatingWebhookConfiguration中的CA证书不一致时，自动更新MutatingWebhookConfiguration对象。\n","link":"https://idouba.com/istio-sidecar-injection-of-cloudnativeistio-04/","section":"posts","tags":["《云原生服务网格Istio:原理,实践,架构与源码解析》","Istio"],"title":"Sidecar Injector自动注入的原理 –《云原生服务网格Istio》书摘04"},{"body":"本节书摘来自华为云原生技术丛书《云原生服务网格Istio:原理,实践,架构与源码解析》一书原理篇的第3章非侵入的流量治理，第3.1.4小节灰度发布原理。更多内容参照原书，或者关注容器魔方公众号。\n3.1.4 灰度发布 在新版本上线时，不管是在技术上考虑产品的稳定性等因素，还是在商业上考虑新版本被用户接受的程度，直接将老版本全部升级是非常有风险的。所以一般的做法是，新老版本同时在线，新版本只切分少量流量出来，在确认新版本没有问题后，再逐步加大流量比例。这正是灰度发布要解决的问题。其核心是能配置一定的流量策略，将用户在同一个访问入口的流量导到不同的版本上。有如下几种典型场景。\n1．蓝绿发布 蓝绿发布的主要思路如图3-13所示，让新版本部署在另一套独立的资源上，在新版本可用后将所有流量都从老版本切到新版本上来。当新版本工作正常时，删除老版本；当新版本工作有问题时，快速切回到老版本，因此蓝绿发布看上去更像一种热部署方式。在新老版本都可用时，升级切换和回退的速度都可以非常快，但快速切换的代价是要配置冗余的资源，即有两倍的原有资源，分别部署新老版本。另外，由于流量是全量切换的，所以如果新版本有问题，则所有用户都受影响，但比蛮力发布在一套资源上重新安装新版本导致用户的访问全部中断，效果要好很多。\n图3-13 蓝绿发布\n2．AB测试 AB测试的场景比较明确，就是同时在线上部署A和B两个对等的版本来接收流量，如图3-14所示，按一定的目标选取策略让一部分用户使用A版本，让一部分用户使用B版本，收集这两部分用户的使用反馈，即对用户采样后做相关比较，通过分析数据来最终决定采用哪个版本。 图3-14 AB测试\n对于有一定用户规模的产品，在上线新特性时都比较谨慎，一般都需要经过一轮AB测试。在AB测试里面比较重要的是对评价的规划：要规划什么样的用户访问，采集什么样的访问指标，尤其是，指标的选取是与业务强相关的复杂过程，所以一般都有一个平台在支撑，包括业务指标埋点、收集和评价。\n3．金丝雀发布 金丝雀发布就比较直接，如图3-15所示，上线一个新版本，从老版本中切分一部分线上流量到新版本来判定新版本在生产环境中的实际表现。就像把一个金丝雀塞到瓦斯井里面一样，探测这个新版本在环境中是否可用。先让一小部分用户尝试新版本，在观察到新版本没有问题后再增加切换的比例，直到全部切换完成，是一个渐变、尝试的过程。\n图3-15 金丝雀发布\n蓝绿发布、AB测试和金丝雀发布的差别比较细微，有时只有金丝雀才被称为灰度发布，这里不用太纠缠这些划分，只需关注其共同的需求，就是要支持对流量的管理。能否提供灵活的流量策略是判断基础设施灰度发布支持能力的重要指标。\n灰度发布技术上的核心要求是要提供一种机制满足多不版本同时在线，并能够灵活配置规则给不同的版本分配流量，可以采用以下几种方式。\n1．基于负载均衡器的灰度发布 比较传统的灰度发布方式是在入口的负载均衡器上配置流量策略，这种方式要求负载均衡器必须支持相应的流量策略，并且只能对入口的服务做灰度发布，不支持对后端服务单独做灰度发布。如图3-16所示，可以在负载均衡器上配置流量规则对frontend服务进行灰度发布，但是没有地方给forecast服务配置分流策略，因此无法对forecast服务做灰度发布。\n图3-16 基于负载均衡器的灰度发布\n2．基于Kubernetes的灰度发布 在Kubernetes环境下可以基于Pod的数量比例分配流量。如图3-17所示，forecast服务的两个版本v2和v1分别有两个和3个实例，当流量被均衡地分发到每个实例上时，前者可以得到40%的流量，后者可以得到60%的流量，从而达到流量在两个版本间分配的效果。\n图3-17 基于Pod数量的灰度发布\n给v1和v2版本设置对应比例的Pod数量，依靠Kube-proxy把流量均衡地分发到目标后端，可以解决一个服务的多个版本分配流量的问题，但是限制非常明显：首先，要求分配的流量比例必须和Pod数量成比例，如图3-17所示，在当前的Pod比例下不支持得到3:7的流量比例，试想，基于这种方式支持3:97比例的流量基本上是不可能的；另外，这种方式不支持根据请求的内容来分配流量，比如要求Chrome浏览器发来的请求和IE浏览器发来的请求分别访问不同的版本。\n有没有一种更细粒度的分流方式？答案当然是有，Istio就可以。Istio叠加在Kubernetes之上，从机制上可以提供比Kubernetes更细的服务控制粒度及更强的服务管理能力，该管理能力几乎包括本章的所有内容，对于灰度发布场景，和刚才Kubernetes的用法进行比较会体现得更明显。\n3．基于Istio的灰度发布 不同于前面介绍的熔断、故障注入、负载均衡等功能，Istio本身并没有关于灰度发布的规则定义，灰度发布只是流量治理规则的一种典型应用，在进行灰度发布时，只要写个简单的流量规则配置即可。\nIstio在每个Pod里都注入了一个Envoy，因而只要在控制面配置分流策略，对目标服务发起访问的每个Envoy便都可以执行流量策略，完成灰度发布功能。\n如图3-18所示为对recommendation服务进行灰度发布，配置20%的流量到v2版本，保留80%的流量在v1版本。通过Istio控制面Pilot下发配置到数据面的各个Envoy，调用recommendation服务的两个服务frontend和forecast都会执行同样的策略，对recommendation服务发起的请求会被各自的Envoy拦截并执行同样的分流策略。\n图3-18 Istio基于流量比例的灰度发布\n在Istio中除了支持这种基于流量比例的策略，还支持非常灵活的基于请求内容的灰度策略。比如某个特性是专门为Mac操作系统开发的，则在该版本的流量策略中需要匹配请求方的操作系统。浏览器、请求的Headers等请求内容在Istio中都可以作为灰度发布的特征条件。如图3-19所示为根据Header的内容将请求分发到不同的版本上。\n图3-19 Istio基于请求内容的灰度发布\n","link":"https://idouba.com/istio-canary-release-of-cloudnativeistio-03/","section":"posts","tags":["《云原生服务网格Istio:原理,实践,架构与源码解析》","Istio"],"title":"Istio灰度发布 –《云原生服务网格Istio》书摘03"},{"body":"本节书摘来自华为云原生技术丛书《云原生服务网格Istio:原理,实践,架构与源码解析》一书原理篇的第4章可扩展的策略和遥测中1.4.1小节Prometheus适配器。更多内容参照原书，或者关注容器魔方公众号。\nPrometheus适配器\nPrometheus应该是当前应用最广的开源系统监控和报警平台了，随着以Kubernetes为核心的容器技术的发展，Prometheus强大的多维度数据模型、高效的数据采集能力、灵活的查询语法，以及可扩展性、方便集成的特点，尤其是和云原生生态的结合，使其获得了越来越广泛的应用。Prometheus于2015年正式发布，于2016年加入CNCF，并于2018年成为第2个从CNCF毕业的项目。\n图4-10展示了Prometheus的工作原理。Prometheus的主要工作为抓取数据存储，并提供PromQL语法进行查询或者对接Grafana、Kiali等Dashboard进行显示，还可以根据配置的规则生成告警。\n​ 图4-10 Prometheus的工作原理\n这里重点关注Prometheus工作流程中与Mixer流程相关的数据采集部分，如图4-10所示。不同于常见的数据生成方向后端上报数据的这种Push方式，Prometheus在设计上基于Pull方式来获取数据，即向目标发送HTTP请求来获取数据，并存储获取的数据。这种使用标准格式主动拉取数据的方式使得Prometheus在和其他组件配合时更加主动，这也是其在云原生场景下得到广泛应用的一个重要原因。\n1．Adapter的功能\n我们一般可以使用Prometheus提供的各种语言的SDK在业务代码中添加Metric的生成逻辑，并通过HTTP发布满足格式的Metric接口。更通用的方式是提供Prometheus Exporter的代理，和应用一起部署，收集应用的Metric并将其转换成Prometheus的格式发布出来。\nExporter方式的最大优点不需要修改用户的代码，所以应用非常广泛。Prometheus社区提供了丰富的Exporter实现（https://prometheus.io/docs/instrumenting/exporters/），除了包括我们熟知的Redis、MySQL、TSDB、Elasticsearch、Kafka等数据库、消息中间件，还包括硬件、存储、HTTP服务器、日志监控系统等。\n如图4-11所示，在Istio中通过Adapter收集服务生成的Metric供Prometheus采集，这个Adatper就是Prometheus Exporter的一个实现，把服务的Metric以Prometheus格式发布出来供Prometheus采集。\n图4-11 Prometheus Adapter的工作机制\n结合图4-11可以看到完整的流程，如下所述。\nEnvoy通过Report接口上报数据给Mixer。 Mixer根据配置将请求分发给Prometheus Adapter。 Prometheus Adapter通过HTTP接口发布Metric数据。 Prometheus服务作为Addon在集群中进行安装，并拉取、存储Metric数据，提供Query接口进行检索。 集群内的Dashboard如Grafana通过Prometheus的检索API访问Metric数据。 可以看到，关键步骤和关键角色是作为中介的Prometheus Adapter提供数据。观察“/prometheus/prometheus.yml”的如下配置，可以看到Prometheus数据采集的配置，包括采集目标、间隔、Metric Path等：\n1- job_name: \u0026#39;istio-mesh\u0026#39; 2 # Override the global default and scrape targets from this job every 5 seconds. 3 scrape_interval: 5s 4 5 kubernetes_sd_configs: 6 - role: endpoints 7 namespaces: 8 names: 9 - istio-system 10 relabel_configs: 11 - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] 12 action: keep 13 regex: istio-telemetry;prometheus 在Istio中，Prometheus除了默认可以配置istio-telemetry抓取任务从Prometheus的Adapter上采集业务数据，还可以通过其他多个采集任务分别采集istio-pilot、istio-galley、istio-policy、istio-telemetry对应的内置Metric接口。\n2．Adapter的配置\n将在Adapter配置模型中涉及的三个重要对象Handler、Instance和Rule在Prometheus中分别配置如下。\n㈠ Handler的配置 Handler的标准格式包括name、adapter、compiledAdapter、params等，name、adapter和compiledAdapter都是公用字段，不同的Handler有不同的params定义，这里重点介绍params字段的使用方法。如图4-12所示是Prometheus Handler的参数定义。\n可以看到，Prometheus的Adapter配置比前面示例中的Stdio要复杂得多，其实Prometheus应该是Istio当前支持的多个Adapter中最复杂的一个，也是功能最强大的一个。\n**metricsExpirationPolicy：**配置Metric的老化策略，metricsExpiryDuration定义老化周期，expiryCheckIntervalDuration定义老化的检查间隔。 图4-12 Prometheus Handler的参数定义\n通过以下配置的Prometheus Handler，可清理5分钟未更新的Metric，并且每隔30秒做一次检查，检查周期expiryCheckIntervalDuration是个可选字段，若未配置，则使用老化周期的一半时间：\n1metricsExpirationPolicy: 2 metricsExpiryDuration: \u0026#34;5m\u0026#34; 3 expiryCheckIntervalDuration: \u0026#34;30s\u0026#34; **metrics：**配置在Prometheus中定义的Metric。这里是一个数组，每个元素都是一个MetricInfo类型的结构，分别定义Metric的namespace、name、instanceName、description、kind、buckets、labelNames，这些都是要传给Prometheus的定义。有以下几个注意点:\nMetric的namespace和name会决定Prometheus中的Metric全名。例如requests_total这个请求统计的Metric对应图4-13中Prometheus的Metric：istio_requests_total，即由命名空间istio和Metric名称requests_total拼接而成。\ninstanceName是一个必选字段，表示instance定义的全名。\nkind 表示指标的类型，根据指标的业务特征，请求计数requests_total的类型为COUNTER，请求耗时request_duration_seconds的类型为DISTRIBUTION。对于DISTRIBUTION 类型的指标可以定义其buckets。\n图4-13 Prometheus的Metric查询\n如下所示是Prometheus Handler的一个定义示例，定义了15秒的老化时间及Prometheus中的多个Metric，有的是HTTP的Metric，有的是TCP的Metric：\n1apiVersion: \u0026#34;config.istio.io/v1alpha2\u0026#34; 2kind: handler 3metadata: 4name: prometheus 5 namespace: istio-system 6spec: 7 compiledAdapter: prometheus 8 params: 9 metricsExpirationPolicy: 10 metricsExpiryDuration: 15s 11 metrics: 12 - name: requests_total 13 instance_name: requestcount.metric.istio-system 14 kind: COUNTER 15 label_names: 16 - source_app 17 - source_principal 18 - destination_service_name 19…… 20 - name: request_duration_seconds 21 instance_name: requestduration.metric.istio-system 22 kind: DISTRIBUTION 23…… 24 - name: request_bytes 25 instance_name: requestsize.metric.istio-system 26 kind: DISTRIBUTION 27…… 28 - name: response_bytes 29 instance_name: responsesize.metric.istio-system 30 kind: DISTRIBUTION 31…… 32 - name: tcp_sent_bytes_total 33 instance_name: tcpbytesent.metric.istio-system 34 kind: COUNTER 35…… 36 - name: tcp_received_bytes_total 37 instance_name: tcpbytereceived.metric.istio-system 38 kind: COUNTER ㈡ Instance的配置 Prometheus作为一个处理Metric的监控系统，其对应的模板正是Metric，这也是Mixer中使用最广泛的一种Instance。如图4-14所示是对Metric Instance的定义。\n图4-14 Metric Instance的定义\n在本节配置示例中用到的requests_total这个Metric的定义如下：dimensions记录每个请求上的重要属性信息，可以使用在4.1.3节介绍的属性和属性表达式；value: \u0026quot;1\u0026quot;表示每个请求被记录一次：\n1apiVersion: \u0026#34;config.istio.io/v1alpha2\u0026#34; 2kind: instance 3metadata: 4 name: requestcount 5 namespace: istio-system 6spec: 7 compiledTemplate: metric 8 params: 9 value: \u0026#34;1\u0026#34; # count each request twice 10 dimensions: 11 reporter: conditional((context.reporter.kind | \u0026#34;inbound\u0026#34;) == \u0026#34;outbound\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;destination\u0026#34;) 12 source_workload_namespace: source.workload.namespace | \u0026#34;unknown\u0026#34; 13 source_principal: source.principal | \u0026#34;unknown\u0026#34; 14 source_app: source.labels[\u0026#34;app\u0026#34;] | \u0026#34;unknown\u0026#34; 15 source_version: source.labels[\u0026#34;version\u0026#34;] | \u0026#34;unknown\u0026#34; 16 destination_workload: destination.workload.name | \u0026#34;unknown\u0026#34; 17 destination_workload_namespace: destination.workload.namespace | \u0026#34;unknown\u0026#34; 18 destination_principal: destination.principal | \u0026#34;unknown\u0026#34; 19 destination_app: destination.labels[\u0026#34;app\u0026#34;] | \u0026#34;unknown\u0026#34; 20 destination_version: destination.labels[\u0026#34;version\u0026#34;] | \u0026#34;unknown\u0026#34; 21 destination_service: destination.service.host | \u0026#34;unknown\u0026#34; 22 destination_service_name: destination.service.name | \u0026#34;unknown\u0026#34; 23 destination_service_namespace: destination.service.namespace | \u0026#34;unknown\u0026#34; 24 request_protocol: api.protocol | context.protocol | \u0026#34;unknown\u0026#34; 25 response_code: response.code | 200 26 response_flags: context.proxy_error_code | \u0026#34;-\u0026#34; 27 permissive_response_code: rbac.permissive.response_code | \u0026#34;none\u0026#34; 28 permissive_response_policyid: rbac.permissive.effective_policy_id | \u0026#34;none\u0026#34; 29 connection_security_policy: conditional((context.reporter.kind | \u0026#34;inbound\u0026#34;) == \u0026#34;outbound\u0026#34;, \u0026#34;unknown\u0026#34;, conditional(connection.mtls | false, \u0026#34;mutual_tls\u0026#34;, \u0026#34;none\u0026#34;)) 30 monitored_resource_type: \u0026#39;\u0026#34;UNSPECIFIED\u0026#34;\u0026#39; 使用这种方式可以定义其他多个Metric，例如Istio中常用的requestcount、requestduration、requestsize、responsesize、tcpbytesent、tcpbytereceived等。\n㈢ Rule的配置 通过Rule可以将Handler和Instance建立关系，例如，下面两个Rule可以分别处理HTTP和TCP的Instance：\n1apiVersion: \u0026#34;config.istio.io/v1alpha2\u0026#34; 2kind: rule 3metadata: 4 name: promhttp 5 namespace: istio-system 6spec: 7 match: (context.protocol == \u0026#34;http\u0026#34; || context.protocol == \u0026#34;grpc\u0026#34;) \u0026amp;\u0026amp; (match((request.useragent | \u0026#34;-\u0026#34;), \u0026#34;kube-probe*\u0026#34;) == false) 8 actions: 9 - handler: prometheus 10 instances: 11 - requestcount 12 - requestduration 13 - requestsize 14 - responsesize 1apiVersion: \u0026#34;config.istio.io/v1alpha2\u0026#34; 2kind: rule 3metadata: 4 name: promtcp 5 namespace: istio-system 6spec: 7 match: context.protocol == \u0026#34;tcp\u0026#34; 8 actions: 9 - handler: prometheus 10 instances: 11 - tcpbytesent 12 - tcpbytereceived 只要通过以上配置，我们不用修改任何代码就可以在Prometheus上看到各种Metric，进而对服务的访问吞吐量、延时、上行流量、下行流量等进行管理。\n","link":"https://idouba.com/stio-prometheus-cloudnative-istio-06/","section":"posts","tags":["《云原生服务网格Istio:原理,实践,架构与源码解析》","Istio"],"title":"Istio通过Prometheus收集遥测数据--《云原生服务网格Istio》书摘06"},{"body":"本节书摘来自华为云原生技术丛书《云原生服务网格Istio:原理,实践,架构与源码解析》一书架构篇的第14章司令官Pilot，第4节Pilot的设计亮点。更多内容参照原书，或者关注容器魔方公众号。作者：中虎\n作为Istio数据面的司令官，Pilot控制中枢系统，它的性能好坏直接影响服务网格的大规模可扩展、配置时延等。如果Pilot的性能低，配置生成效率也低，那么它将难以管理大规模服务网格。比如，服务网格拥有成千上万服务及数十万服务实例，配置生成的效率很低，难以满足服务及Config更新带来的配置更新需要，将会造成Pilot负载很高，用户体验很差。Istio社区网络工作组很早就已经意识到这个问题，并在近期的版本中相继做了很多优化工作，本节选取具有代表性的4个优化点进行讲解。\n14.4.1 三级缓存优化 缓存模型是软件系统中最常用的一种性能优化机制，通过缓存一定的资源，减少CPU利用率、网络I/O等，Pilot在设计之初就重复利用缓存来降低系统CPU及网络开销。目前在Pilot层面存在三级资源的缓存，如图14-28所示。\n​ 图14-28 Pilot层面的三级资源的缓存\n以Kubernetes平台为例，所有服务及配置规则的监听都通过Kubernetes Informer实现。我们知道，Informer的LIST-WATCH原理是通过在客户端本地维护资源的缓存实现的。此为Pilot平台适配层的一级缓存。\n平台层的资源（Service、Endpoint、VirtualService、DestinationRule等）都是原始的API模型，对于具体的Sidecar、Gateway配置规则的生成涉及平台层原始资源的选择，以及从原始资源到Istio资源模型的转换。如果在xDS配置生成过程中重复执行原始资源的选择与转换，则非常影响性能。因此Istio在中间层做了Istio资源模型的缓存优化。\n最上面的一层缓存则是xDS配置的缓存。具体来讲，目前在xDS层面有两种配置缓存：Cluster与Endpoint，这两种资源较为通用，很少被Envoy代理的设置所影响。因此在xDS层面对Cluster及Endpoint进行缓存，能极大提高Pilot的性能。\n随着Istio的发展与成熟，越来越多的缓存优化逐渐成型。当然，任何事物都有两面性，缓存技术同样带来了巨大的内存开销，我们同样需要综合权衡利弊。\n14.4.2 去抖动分发 随着集群规模的增大，Config及服务、服务实例的数量成倍增长，任何更新都可能会导致Envoy配置规则的改变，如果每一次的更新都引起Pilot重新计算及分发xDS配置，那么可能导致Pilot过载及Envoy的不稳定。这些都难以支撑大规模服务网格的需求，因此Pilot在内部以牺牲xDS配置的实时性为代价换取了稳定性。\n具体的去抖动优化是通过EnvoyXdsServer的handleUpdates模块完成的，其主要根据最小静默时间及最大延迟时间两个参数控制分发事件的发送来实现。图14-29展示了利用最小静默时间进行去抖动的原理：tN表示在一个推送周期内第N次接收到更新事件的时间，如果从t0到tN不断有更新事件发生，并且在tN时刻之后的最小静默时间段内没有更新事件发生，那么根据最小静默时间原理，EnvoyXdsServer将会在tN+minQuiet时刻发送分发事件到pushChannel。\n​ 图14-29 利用最小静默时间进行去抖动的原理\n图14-30展示了最大延迟的去抖动原理：在很长的时间段内源源不断地产生更新事件，并且事件的出现频率很高，不能满足最小静默时间的要求，如果单纯依赖最小静默时间机制无法产生xDS分发事件，则会导致相当大的延迟，甚至可能影响Envoy的正常工作。根据最大延迟机制，如果当前时刻距离t0时刻超过最大延迟时间，则无论是否满足最小静默时间的要求，EnvoyXdsServer也会分发事件到pushChannel。\n​ 图14-30 最大延迟的去抖动原理\n最小静默时间机制及最大延迟时间机制的结合，充分平衡了Pilot配置生成与分发过程中的时延及Pilot自身的性能损耗，提供了个性化控制微服务网格控制面性能及稳定性的方案。无论如何，Envoy代理的配置具有最终一致性，这也是微服务通信的基本要求。\n14.4.3 增量EDS 我们知道，在集群或者网格中，数量最多、变化最快的必然是服务实例，在Kubernetes平台上，服务实例就是Endpoint（Kubernetes平台的服务实例资源）。尤其是，在应用滚动升级或者故障迁移的过程中会产生非常多的服务实例的更新事件。而单纯的服务实例的变化并不会影响Listener、Route、Cluster等xDS配置，如果仅仅由于服务实例的变化触发全量的xDS配置生成与分发，则会浪费很多计算资源与网络带宽资源，同时影响Envoy代理的稳定性。\nIstio在1.1版本中引入增量EDS特性，专门针对以上场景对Pilot进行优化。首先，服务实例的Event Handler不同于前面提到的通用的事件处理回调函数（直接发送全量更新事件到updateChannel）。增量EDS异步分发的主要流程如图14-31所示。\n可以看到，Kubernetes的Endpoint资源在更新时，首先在平台适配层由updateEDS将其转换为Istio特有的IstioEndpoint模型；然后，EnvoyXdsServer通过对比其缓存的IstioEndpoint资源，检查是否需要全量下发配置，并更新缓存；当仅仅存在Endpoints更新事件时，Pilot只需要进行增量EDS分发；随后，EnvoyXdsServer将增量EDS分发事件发送到updateChannel，后续处理步骤详见14.2.4节。\n​ 图14-31 增量EDS异步分发的主要流程\n为了深入理解增量EDS的特性，这里讲解EnvoyXdsServer是如何判断是否可以进行增量EDS分发的。EnvoyXdsServer全局缓存所有服务的IstioEndpoint及在每个推送周期内发生变化的服务列表。前面已经讲过，EnvoyXdsServer是通过IstioEndpoint缓存判断是否需要全量配置下发的。在每个推送周期内，EnvoyXdsServer都维护了本周期内所有涉及Endpoint变化的服务列表，当增量EDS分发开始时，Pilot将在本次推送周期内更新的服务名称通过pushChannel发送到请求处理模块进行配置分发，这时只需生成与本推送周期变化的服务相关的EDS配置并下发即可。\n14.4.4 资源隔离 随着用户对Istio服务网格的需求越来越旺盛，Istio社区充分认识到服务隔离或者说作用范围的必要性。通过有效定义访问范围及服务的有效作用范围，可以大大消除网格规模增加带来的配置规模几何级的增长，目前在理论上可支持无限大规模的服务网格。\nIstio目前充分利用命名空间隔离的概念，在两方面做了可见范围的优化：用Sidecar API资源定义Envoy代理可以访问的服务；用服务及配置（VirtuslService、DestinationRule）资源定义其有效范围。\nSidecar API资源是Istio 1.1新增的特性，目前支持为同一命名空间下的所有Envoy或者通过标签选择为特定的Envoy定义其对外可访问的服务（支持具体的服务名称或者命名空间的基本服务）。 服务及配置规则的可见范围。目前可定义同一命名空间可见或者全局范围可见。Istio通过其实现服务访问层面的隔离，同Sidecar API资源一起减少xDS配置数量。 ","link":"https://idouba.com/istio-pilot-design-of-cloudnativeistio-02/","section":"posts","tags":["《云原生服务网格Istio:原理,实践,架构与源码解析》","Istio"],"title":"Pilot的设计亮点–《云原生服务网格Istio》书摘02"},{"body":"本节书摘来自华为云原生技术丛书《云原生服务网格Istio:原理,实践,架构与源码解析》一书中的第3章非侵入的流量治理，第3节Istio流量治理的原理3.1.2小节服务熔断。更多内容参照原书，或者关注容器魔方公众号。\n熔断器在生活中一般指可以自动操作的电气开关，用来保护电路不会因为电流过载或者短路而受损，典型的动作是在检测到故障后马上中断电流。“熔断器”这个概念延伸到计算机世界中指的是故障检测和处理逻辑，防止临时故障或意外导致系统整体不可用，最典型的应用场景是防止网络和服务调用故障级联发生，限制故障的影响范围，防止故障蔓延导致系统整体性能下降或雪崩。\n如图3-6所示为级联故障示例，可以看出在4个服务间有调用关系，如果后端服务recommendation由于各种原因导致不可用，则前端服务forecast和frontend都会受影响。在这个过程中，若单个服务的故障蔓延到其他服务，就会影响整个系统的运行，所以需要让故障服务快速失败，让调用方服务forecast和frontend知道后端服务recommendation出现问题，并立即进行故障处理。这时，非常小概率发生的事情对整个系统的影响都足够大\n​ 图3-6 级联故障示例\n在Hystrix官方曾经有这样一个推算：如果一个应用包含30个依赖的服务，每个服务都可以保证99.99%可靠性地正常运行，则从整个应用角度看，可以得到99.9930 =99.7%的正常运行时间，即有0.3%的失败率，在10亿次请求中就会有3 000 000多种失败，每个月就会有两个小时以上的宕机。即使其他服务都是运行良好的，只要其中一个服务有这样0.001%的故障几率，对整个系统就都会产生严重的影响。\n关于熔断的设计，Martin Fowler有一个经典的文章，其中描述的熔断主要应用于微服务场景下的分布式调用中：在远程调用时，请求在超时前一直挂起，会导致请求链路上的级联故障和资源耗尽；熔断器封装了被保护的逻辑，监控调用是否失败，当连续调用失败的数量超过阈值时，熔断器就会跳闸，在跳闸后的一定时间段内，所有调用远程服务的尝试都将立即返回失败；同时，熔断器设置了一个计时器，当计时到期时，允许有限数量的测试请求通过；如果这些请求成功，则熔断器恢复正常操作；如果这些请求失败，则维持断路状态。Martin把这个简单的模型通过一个状态机来表达，我们简单理解下，如图3-7所示。\n​ 图3-7 熔断器状态机\n图3-7上的三个点表示熔断器的状态，下面分别进行解释。\n熔断关闭：熔断器处于关闭状态，服务可以访问。熔断器维护了访问失败的计数器，若服务访问失败则加一。 熔断开启：熔断器处于开启状态，服务不可访问，若有服务访问则立即出错。 熔断半开启：熔断器处于半开启状态，允许对服务尝试请求，若服务访问成功则说明故障已经得到解决，否则说明故障依然存在。 图上状态机上的几条边表示几种状态流转，如表3-1所示。\n​ 表3-1 熔断器的状态流转\nMartin这个状态机成为后面很多系统实现的设计指导，包括最有名的Hystrix，当然，Istio的异常点检测也是按照类似语义工作的，后面会分别进行讲解。\n1．Hystrix熔断\n关于熔断，大家比较熟悉的一个落地产品就是Hystrix。Hystrix是Netflix提供的众多服务治理工具集中的一个，在形态上是一个Java库，在2011年出现，后来多在Spring Cloud中配合其他微服务治理工具集一起使用。\nHystrix的主要功能包括：\n阻断级联失败，防止雪崩； 提供延迟和失败保护； 快速失败并即时恢复； 对每个服务调用都进行隔离； 对每个服务都维护一个连接池，在连接池满时直接拒绝访问； 配置熔断阈值，对服务访问直接走失败处理Fallback逻辑，可以定义失败处理逻辑； 在熔断生效后，在设定的时间后探测是否恢复，若恢复则关闭熔断； 提供实时监控、告警和操作控制。 Hystrix的熔断机制基本上与Martin的熔断机制一致。在实现上，如图3-8所示，Hystrix将要保护的过程封装在一个HystrixCommand中，将熔断功能应用到调用的方法上，并监视对该方法的失败调用，当失败次数达到阈值时，后续调用自动失败并被转到一个Fallback方法上。在HystrixCommand中封装的要保护的方法并不要求是一个对远端服务的请求，可以是任何需要保护的过程。每个HystrixCommand都可以被设置一个Fallback方法，用户可以写代码定义Fallback方法的处理逻辑。\n​ 图3-8 HystrixCommand熔断处理\n在Hystrix的资源隔离方式中除了提供了熔断，还提供了对线程池的管理，减少和限制了单个服务故障对整个系统的影响，提高了整个系统的弹性。在使用上，不管是直接使用Netflix的工具集还是Spring Cloud中的包装，都建议在代码中写熔断处理逻辑，有针对性地进行处理，但侵入了业务代码，这也是与Istio比较大的差别。\n业界一直以Hystrix作为熔断的实现模板，尤其是基于Spring Cloud。但遗憾的是，Hystrix在1.5.18版本后就停止开发和代码合入，转为维护状态，其替代者是不太知名的Resilience4J。\n2．Istio熔断\n云原生场景下的服务调用关系更加复杂，前文提到的若干问题也更加严峻，Istio提供了一套非侵入的熔断能力来应对这种挑战。\n与Hystrix类似，在Istio中也提供了连接池和故障实例隔离的能力，只是概念术语稍有不同：前者在Istio的配置中叫作连接池管理，后者叫作异常点检测，分别对应Envoy的熔断和异常点检测。\nIstio在0.8版本之前使用V1alpha1接口，其中专门有个CircuitBreaker配置，包含对连接池和故障实例隔离的全部配置。在Istio 1.1的V1alpha3接口中，CircuitBreaker功能被拆分成连接池管理（ConnectionPoolSettings）和异常点检查（OutlierDetection）这两种配置，由用户选择搭配使用。\n首先看看解决的问题，如下所述。\n在Istio中通过限制某个客户端对目标服务的连接数、访问请求数等，避免对一个服务的过量访问，如果超过配置的阈值，则快速断路请求。还会限制重试次数，避免重试次数过多导致系统压力变大并加剧故障的传播； 如果某个服务实例频繁超时或者出错，则将该实例隔离，避免影响整个服务。 以上两个应用场景正好对应连接池管理和异常实例隔离功能。\nIstio的连接池管理工作机制对TCP提供了最大连接数、连接超时时间等管理方式，对HTTP提供了最大请求数、最大等待请求数、最大重试次数、每连接最大请求数等管理方式，它控制客户端对目标服务的连接和访问，在超过配置时快速拒绝。\n如图3-9所示，通过Istio的连接池管理可以控制frontend服务对目标服务forecast的请求：\n当frontend服务对目标服务forecast的请求不超过配置的最大连接数时，放行； 当frontend服务对目标服务forecast的请求不超过配置的最大等待请求数时，进入连接池等待； 当frontend服务对目标服务forecast的请求超过配置的最大等待请求数时，直接拒绝。 ​ 图3-9 Istio的连接池管理\nIstio提供的异常点检查机制动态地将异常实例从负载均衡池中移除，如图3-10所示，当连续的错误数超过配置的阈值时，后端实例会被移除。异常点检查在实现上对每个上游服务都进行跟踪，对于HTTP服务，如果有主机返回了连续的5xx，则会被踢出服务池；而对于TCP服务，如果到目标服务的连接超时和失败，则都会被记为出错。\n​ 图3-10 Istio异常点检查\n另外，被移除的实例在一段时间之后，还会被加回来再次尝试访问，如果可以访问成功，则认为实例正常；如果访问不成功，则实例不正常，重新被逐出，后面驱逐的时间等于一个基础时间乘以驱逐的次数。这样，如果一个实例经过以上过程的多次尝试访问一直不可用，则下次会被隔离更久的时间。可以看到，Istio的这个流程也是基于Martin的熔断模型设计和实现的，不同之处在于这里没有熔断半开状态，熔断器要打开多长时间取决于失败的次数。\n另外，在Istio中可以控制驱逐比例，即有多少比例的服务实例在不满足要求时被驱逐。当有太多实例被移除时，就会进入恐慌模式，这时会忽略负载均衡池上实例的健康标记，仍然会向所有实例发送请求，从而保证一个服务的整体可用性。\n下面对Istio与Hystrix的熔断进行简单对比，如表3-2所示。可以看到与Hystrix相比，Istio实现的熔断器其实是一个黑盒，和业务没有耦合，不涉及代码，只要是对服务访问的保护就可以用，配置比较简单、直接。\n​ 表3-2 Istio和Hystrix熔断的简单对比\n熔断功能本来就是叠加上去的服务保护，并不能完全替代代码中的异常处理。业务代码本来也应该做好各种异常处理，在发生异常的时候通知调用方的代码或者最终用户，如下所示：\n1public void callService(String serviceName) throws Exception { 2try { 3// call remote service 4RestTemplate restTemplate = new RestTemplate(); 5String result = restTemplate.getForObject(serviceName, String.class); 6} catch (Exception e) { 7// exception handle 8dealException(e) 9} 10} Istio的熔断能力是对业务透明的，不影响也不关心业务代码的写法。当Hystrix开发的服务运行在Istio环境时，两种熔断机制叠加在一起。在故障场景下，如果Hystrix和Istio两种规则同时存在，则严格的规则先生效。当然，不推荐采用这种做法，建议业务代码处理好业务，把治理的事情交给Istio来做。\n","link":"https://idouba.com/istio-curcuit-break-of-cloudnativeistio/","section":"posts","tags":["《云原生服务网格Istio:原理,实践,架构与源码解析》","Istio"],"title":"Istio服务熔断 –《云原生服务网格Istio》书摘01"},{"body":"2018，我和我的咕咚队友的故事。2019，我和我的咕咚队友的故事，还将继续。整理手机里的文字，告别和纪念逝去的2018，迎来2019，多些勇气和力量。\n一百天，老太太离开我们整整一百天了，可能是过去的几十年里最难过最漫长的一百天。又一次踏上回家路。大清早四点多起床，背了一个空包就钻进了往机场的出租车。再也不用琢磨包里塞点老太太没有吃过的东西。包里空的，心里更空，\n这段时间一直就心里空空的，乱乱的。莫名的会发脾气，也莫名的会忘东西。考勤纪律很严格的我厂里居然一个月里有好几次忘了打卡。\n有几次奇怪的梦里梦到老太太。。。各种有意思的梦，有次还会梦到爷爷。也梦到了又和我这个咕咚队友一起健走。\n真的很巧，马上就是元旦了，眼看着2018就这样过去了。整理手机备忘录里那几天手指头敲进去的一点文字。整理下文字，整理下心情。纪念过去的2018，纪念在2018逝去的祖母。\n您永远在家人的心里。就像从来没有离开过一样。\n十一月份一次出差深圳，一个人爬了南山，感觉就像开着咕咚领着我的那个懒懒的队友一样。感觉非常好，很踏实。我的脚能走多远，我的心就能走多远，装在心里的我这个队友就能陪我走多远。\n农历十月十五，晚上十点多加完班跑回家，8800米，天上的月也很圆，街上人不多，很静，江边上更安静，有些汗，有些泪，很畅快，不累。\n农历十一月十五，早上比平时早起了二十分钟，8800米，比平时多跑了一半路程。但应该没那个一辈子的每一天里天不亮就起来前后屋子扫地的队友起的早。\n农历腊月十五，离过年就剩下半个月了，非常期待的一次健跑。\n2019年，加油！总能感受到队友给予的力量和勇气，教给我的包容、知足、感恩和坦然，当然还有不作恶的叮嘱。感谢2018帮过我的所有人，特别是那个在八月份出差西安给的我几天假的boss哥，得以最后一次陪祖母我的咕咚队友健走了最后的90米。2019，会记住祖母一直唠叨的话，不呵斥孩子，好好说话，每次要发脾气的时候会提醒自己。祝福在新的一年里，家人、朋友和身边所有的人健康顺利。\n2018，8800，发发发，2019，8800，就发发，约起来。。\n","link":"https://idouba.com/the-first-day-of-2019/","section":"posts","tags":["祖母"],"title":"2019年元旦忆我的咕咚队友"},{"body":"","link":"https://idouba.com/tags/%E7%A5%96%E6%AF%8D/","section":"tags","tags":null,"title":"祖母"},{"body":"","link":"https://idouba.com/tags/infoq/","section":"tags","tags":null,"title":"Infoq"},{"body":"发在Infoq上的一篇文章，答疑当前大家工作中碰到的Istio调用链的问题，最终澄清了观点，并推动社区修改了说法，避免误解。\n前言 在 Istio 的实践中最近经常被问到一个问题，使用 Istio 做调用链用户的业务代码是不是完全 0 侵入，到底要不要修改业务代码？\n看官方介绍：\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, without any changes in service code.\n是不用修改任何代码即可做各种治理。实际使用中应用程序不做任何修改，使用 Istio 的调用链输出总是断开的，这到底是什么原因呢？\n对以上问题关注的人比较多，但是貌似说的都不是特别清楚，在最近的 K8S 技术社区的 Meetup 上笔者专门做了主题分享，通过解析 Istio 的架构机制与 Istio 中调用链的工作原理来回答以上问题。在本文中将节选里面的重点内容，基于 Istio 官方典型的示例来展开其中的每个细节和原理。\nIstio 本身的内容在这里不多介绍，作为 Google 继 Kubernetes 之后的又一重要项目，Istio 提供了 Service Mesh 方式服务治理的完整的解决方案。正如其首页介绍，通过非侵入的方式提供了服务的连接、控制、保护和观测能力。包括智能控制服务间的流量和 API 调用；提供授权、认证和通信加密机制自动保护服务安全；通过开放策略来控制调用者对服务的访问；另外提供了可扩展丰富的调用链、监控、日志等手段来对服务与性能进行观测。即用户不用修改代码，就可以实现各种服务治理能力。\n较之其他系统和平台，Istio 比较明显的一个特点是服务运行的监控数据都可以动态获取和输出，提供了强大的调用链、监控和调用日志收集输出的能力。配合可视化工具，运维人员可以方便的看到系统的运行状况，并发现问题进而解决问题。而我们基本上不用在自己的代码里做任何修改来生成数据并对接各种监控、日志、调用链等后端。非常神奇的是只要我们的程序被部署 run 起来，其运行数据就自动收集并在一个面板上展现出来。\n调用链概述 对于分布式系统的运维管理和故障定位来说，调用链当然是第一利器。\n正如 Service Mesh 的诞生是为了解决大规模分布式服务访问的治理问题，调用链的出现也是为了对应于大规模的复杂的分布式系统运行中碰到的故障定位定界问题。大量的服务调用、跨进程、跨服务器，可能还会跨多个物理机房。无论是服务自身问题还是网络环境的问题导致调用上链路上出现问题都比较复杂，如何定位就比单进程的一个服务打印一个异常栈来找出某个方法要困难的多。需要有一个类似的调用链路的跟踪，经一次请求的逻辑规矩完整的表达出来，可以观察到每个阶段的调用关系，并能看到每个阶段的耗时和调用详细情况。\nDapper, a Large-Scale Distributed Systems Tracing Infrastructure 描述了其中的原理和一般性的机制。模型中包含的术语也很多，理解最主要的两个即可：\nTrace：一次完整的分布式调用跟踪链路。 Span：跨服务的一次调用； 多个 Span 组合成一次 Trace 追踪记录。 上图是 Dapper 论文中的经典图示，左表示一个分布式调用关系。前端（A），两个中间层（B 和 C），以及两个后端（D 和 E）。用户发起一个请求时，先到达前端，再发送两个服务 B 和 C。B 直接应答，C 服务调用后端 D 和 E 交互之后给 A 应答，A 进而返回最终应答。要使用调用链跟踪，就是给每次调用添加 TraceId、SpanId 这样的跟踪标识和时间戳。\n右表示对应 Span 的管理关系。每个节点是一个 Span，表示一个调用。至少包含 Span 的名、父 SpanId 和 SpanId。节点间的连线下表示 Span 和父 Span 的关系。所有的 Span 属于一个跟踪，共用一个 TraceId。从图上可以看到对前端 A 的调用 Span 的两个子 Span 分别是对 B 和 C 调用的 Span，D 和 E 两个后端服务调用的 Span 则都是 C 的子 Span。\n调用链系统有很多实现，用的比较多的如zipkin，还有已经加入 CNCF 基金会并且的用的越来越多的Jaeger，满足 Opentracing 语义标准的就有这么多。\n一个完整的调用链跟踪系统，包括调用链埋点，调用链数据收集，调用链数据存储和处理，调用链数据检索（除了提供检索的 APIServer，一般还要包含一个非常酷炫的调用链前端）等若干重要组件。如图是 Jaeger 的一个完整实现。\n这里我们仅关注与应用相关的内容，即调用链埋点的部分，看下在 Istio 中是否能做到”无侵入“的调用链埋点。调用链的埋点是一个比起来记录日志，报个 metric 或者告警要复杂的多。根本原因其数据结构要相对复杂一些，为了能将在多个点上收集的关于一次调用的多个中间请求过程关联起来形成一个链。下面通过详析自带的典型例子来看下这里的细节。\n调用链示例 简单起见，我们以 Istio 最经典的 Bookinfo 为例来说明。Bookinfo 模拟在线书店的一个分类，显示一本书的信息。本身是一个异构应用，几个服务分别由不同的语言编写的。各个服务的模拟作用和调用关系是：\nProductpage ：Productpage 服务会调用 Details 和 Reviews 两个服务，用来生成页面。 Details ：这个微服务包含了书籍的信息。 Reviews ：这个微服务包含了书籍相关的评论。并调用 Ratings 微服务。 Ratings ：Ratings 微服务中包含了由书籍评价组成的评级信息。 在 Istio 上运行这个典型例子，不用做任何的代码修改，自带的 Zipkin 上就能看到如下的调用链输出。\n可以看到 zipkin 上展示给我们的调用链和 Boookinfo 这个场景设计的调用关系一致：Productpage 服务会调用 Details 和 Reviews 两个服务，Reviews 调用了 Ratings 微服务。除了显示调用关系外，还显示了每个中间调用的耗时和调用详情。基于这个视图，服务的运维人员比较直观的定界到慢的或者有问题的服务，并钻取当时的调用细节，进而定位到问题。我们就要关注下调用链埋点到底是在哪里做的，怎么做的？\nIstio 调用链埋点逻辑 在 Istio 中，所有的治理逻辑的执行体都是和业务容器一起部署的 Envoy 这个 Sidecar，不管是负载均衡、熔断、流量路由还是安全、可观察性的数据生成都是在 Envoy 上。Sidecar 拦截了所有的流入和流出业务程序的流量，根据收到的规则执行执行各种动作。实际使用中一般是基于 K8S 提供的 InitContainer 机制，用于在 Pod 中执行一些初始化任务. InitContainer 中执行了一段 Iptables 的脚本。正是通过这些 Iptables 规则拦截 pod 中流量，并发送到 Envoy 上。Envoy 拦截到 Inbound 和 Outbound 的流量会分别作不同操作，执行上面配置的操作，另外再把请求往下发，对于 Outbound 就是根据服务发现找到对应的目标服务后端上；对于 Inbound 流量则直接发到本地的服务实例上。\n所以我们的重点是看下拦截到流量后 Sidecar 在调用链埋点怎么做的。\nEnvoy 的埋点规则和在其他服务调用方和被调用方的对应埋点逻辑没有太大差别，甚至和一般 SDK 方式内置的调用链埋点逻辑也类似。\nInbound 流量：对于经过 Sidecar 流入应用程序的流量，如果经过 Sidecar 时 Header 中没有任何跟踪相关的信息，则会在创建一个根 Span，TraceId 就是这个 SpanId，然后再将请求传递给业务容器的服务；如果请求中包含 Trace 相关的信息，则 Sidecar 从中提取 Trace 的上下文信息并发给应用程序。 Outbound 流量：对于经过 Sidecar 流出的流量，如果经过 Sidecar 时 Header 中没有任何跟踪相关的信息，则会创建根 Span，并将该跟 Span 相关上下文信息放在请求头中传递给下一个调用的服务；当存在 Trace 信息时，Sidecar 从 Header 中提取 Span 相关信息，并基于这个 Span 创建子 Span，并将新的 Span 信息加在请求头中传递。 特别是 Outbound 部分的调用链埋点逻辑，通过一段伪代码描述如下：\n1parentSpan = tracer.getTracer().extract(headers); 2 if(parentSpan == null){ 3 span = tracer.buildSpan(operation).start(); 4 } else { 5 span = tracer.buildSpan(operation).asChildOf(parentSpan).start(); 6 } 如图是对前面 Zipkin 上输出的一个 Trace 一个透视图，观察下每个调用的细节。可以看到每个阶段四个服务与部署在它旁边上的 Sidecar 是怎么配合的。在图上只标记了 Sidecar 生成的 Span 主要信息。下面基于具体的例子我们走一遍流程，类剖析下细节，最终得出我们关系的业务代码要做哪些事情？\n因为 Sidecar 处理 Inbound 和 Outbound 的逻辑有所不同，在图上表也分开两个框图分开表达。如 Productpage，接收外部请求是一个处理，给 Details 发出请求是一个处理，给 Reviews 发出请求是另外一个处理，因此围绕 Productpage 这个 app 有三个黑色的处理块，其实是一个 Sidecar 在做事。\n同时，为了不使的图上箭头太多，最终的 Response 都没有表达出来，其实图上每个请求的箭头都有一个反方向的 Response。在服务发起方的 Sidecar 会收到 Response 时，会记录一个 CR(client Received) 表示收到响应的时间并计算整个 Span 的持续时间。\n下面通过解析下具体数据来找出埋点逻辑：\n首先从调用入口的 Gateway 开始，Gateway 作为一个独立部署在一个 pod 中的 Envoy 进程，当有请求过来时，它会将请求转给入口服务 Productpage。Gateway 这个 Envoy 在发出请求时里面没有 Trace 信息，会生成一个根 Span：SpanId 和 TraceId 都是 f79a31352fe7cae9，因为是第一个调用链上的第一个 Span，也就是一般说的根 Span，所有 ParentId 为空，在这个时候会记录 CS（Client Send）； 请求从入口 Gateway 这个 Envoy 进入 Productpage 的 app 业务进程其 Inbound 流量被 Productpage Pod 内的 Envoy 拦截，Envoy 处理请求头中带着 Trace 信息，记录 SR(Server Received)，并将请求发送给 Productpage 业务容器处理，Productpage 在处理请求的业务方法中在接受调用的参数时，除了接受一般的业务参数外，同时解析请求中的调用链 Header 信息，并把 Header 中的 Trace 信息传递给了调用的 Details 和 Reviews 的微服务。 从 Productpage 出去的请求到达 Reviews 服务前，其 Oubtbound 流量又一次通过同 Pod 的 Envoy，Envoy 埋点逻辑检查 Header 中包含了 Trace 相关信息，在将请求发出前会做客户端的调用链埋点，即以当前 Span 为 parent Span，生成一个子 Span：新的 SpanId cb4c86fb667f3114，TraceId 保持一致 9a31352fe7cae9，ParentId 就是上个 Span 的 Id： f79a31352fe7cae9。 从 Productpage 到 Reviews 的请求经过 Productpage 的 Sidecar 走 LB 后，发给一个 Reviews 的实例。请求在到达 Reviews 业务容器前，同样也被 Reviews 的 Envoy 拦截，Envoy 检查从 Header 中解析出 Trace 信息存在，则发送 Trace 信息给 Reviews。Reviews 处理请求的服务端代码中同样接收和解析出这些包含 Trace 的 Header 信息，发送给下一个 Ratings 服务。 在这里我们只是理了一遍请求从入口 Gateway，访问 Productpage 服务，再访问 Reviews 服务的流程。可以看到期间每个访问阶段，对服务的 Inbound 和 Outbound 流量都会被 Envoy 拦截并执行对应的调用链埋点逻辑。图示的 Reviews 访问 Ratings 和 Productpage 访问 Details 逻辑与以上类似，这里不做复述。\n以上过程也印证了前面我们提出的 Envoy 的埋点逻辑。可以看到过程中除了 Envoy 处理 Inbound 和 Outbound 流量时要执行对应的埋点逻辑外，每一步的调用要串起来，应用程序其实做了些事情。就是在将请求发给下一个服务时，需要将调用链相关的信息同样传下去，尽管这些 Trace 和 Span 的标识并不是它生成的。这样在出流量的 Proxy 向下一跳服务发起请求前才能判断并生成子 Span 并和原 Span 进行关联，进而形成一个完整的调用链。否则，如果在应用容器未处理 Header 中的 Trace，则 Sidecar 在处理请求时会创建根 Span，最终会形成若干个割裂的 Span，并不能被关联到一个 Trace 上，就会出现我们开始提到的问题。\n不断被问到两个问题来试图说明这个业务代码配合修改来实现调用链逻辑可能不必要：\n问题一：既然传入的请求上已经带了这些 Header 信息了，直接往下一直传不就好了吗？Sidecar 请求 APP 的时候带着这些 Header，APP 请求 Sidecar 时也带着这些 Header 不就完了吗？\n问题二：既然 TraceId 和 SpanId 是同一个 Sidecar 生成的，为什么要再费劲让 App 收到请求的时候解析下，发出请求时候再带着发出来传回给 Sidecar 呢？\n回答问题一，只需理解一点，这里的 App 业务代码是处理请求不是转发请求，即图上左边的 Request to Productpage 到 Productpage 中请求就截止了，要怎么处理完全是 Productpage 的服务接口的内容了，可以是调用本地处理逻辑直接返回，也可以是如示例中的场景构造新的请求调用其他的服务。右边的 Request from Productpage 完全是 Productpage 服务构造的发出的另外一个请求。\n回答问题二，需要理解当前 Envoy 是独立的 Listener 来处理 Inbound 和 Outbound 的请求。Inbound 只会处理入的流量并将流量转发到本地的服务实例上。而 Outbound 就是根据服务发现找到对应的目标服务后端上。除了在一个进程里外两个之间可以说没有任何关系。 另外如问题一描述，因为到 Outbound 已经是一个新构造的请求了，使得想维护一个 map 来记录这些 Trace 信息这种方案也变得不可行。\n例子中 Productpage 访问 Reviews 的 Span 详细如下，删减掉一些数据只保留主要信息大致是这样：\n1{ 2\t\u0026#34;traceId\u0026#34;: \u0026#34;f79a31352fe7cae9\u0026#34;, 3\t\u0026#34;id\u0026#34;: \u0026#34;cb4c86fb667f3114\u0026#34;, 4\t\u0026#34;name\u0026#34;: \u0026#34;reviews-route\u0026#34;, 5\t\u0026#34;parentId\u0026#34;: \u0026#34;f79a31352fe7cae9\u0026#34;, 6\t\u0026#34;timestamp\u0026#34;: 1536132571847838, 7\t\u0026#34;duration\u0026#34;: 64849, 8\t\u0026#34;annotations\u0026#34;: [ { 9\t\u0026#34;timestamp\u0026#34;: 1536132571847838, 10\t\u0026#34;value\u0026#34;: \u0026#34;cs\u0026#34;, 11\t\u0026#34;endpoint\u0026#34;: { 12\t\u0026#34;serviceName\u0026#34;: \u0026#34;productpage\u0026#34;, 13\t\u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.33\u0026#34; 14\t} 15\t}, { 16\t\u0026#34;timestamp\u0026#34;: 1536132571848121, 17\t\u0026#34;value\u0026#34;: \u0026#34;sr\u0026#34;, 18\t\u0026#34;endpoint\u0026#34;: { 19\t\u0026#34;serviceName\u0026#34;: \u0026#34;reviews\u0026#34;, 20\t\u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.32\u0026#34; 21\t} 22\t}, { 23\t\u0026#34;timestamp\u0026#34;: 1536132571912403, 24\t\u0026#34;value\u0026#34;: \u0026#34;ss\u0026#34;, 25\t\u0026#34;endpoint\u0026#34;: { 26\t\u0026#34;serviceName\u0026#34;: \u0026#34;reviews\u0026#34;, 27\t\u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.32\u0026#34; 28\t} 29\t}, { 30\t\u0026#34;timestamp\u0026#34;: 1536132571912687, 31\t\u0026#34;value\u0026#34;: \u0026#34;cr\u0026#34;, 32\t\u0026#34;endpoint\u0026#34;: { 33\t\u0026#34;serviceName\u0026#34;: \u0026#34;productpage\u0026#34;, 34\t\u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.33\u0026#34; 35\t} 36\t} ], 37\t\u0026#34;binaryAnnotations\u0026#34;: [ { 38\t\u0026#34;key\u0026#34;: \u0026#34;http.status_code\u0026#34;, 39\t\u0026#34;value\u0026#34;: \u0026#34;200\u0026#34;, 40\t\u0026#34;endpoint\u0026#34;: { 41\t\u0026#34;serviceName\u0026#34;: \u0026#34;reviews\u0026#34;, 42\t\u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.32\u0026#34; 43\t} 44\t}, { 45\t\u0026#34;key\u0026#34;: \u0026#34;http.url\u0026#34;, 46\t\u0026#34;value\u0026#34;: \u0026#34;http://reviews:9080/reviews/0\u0026#34;, 47\t\u0026#34;endpoint\u0026#34;: { 48\t\u0026#34;serviceName\u0026#34;: \u0026#34;productpage\u0026#34;, 49\t\u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.33\u0026#34; 50\t} 51\t}, { 52\t\u0026#34;key\u0026#34;: \u0026#34;response_size\u0026#34;, 53\t\u0026#34;value\u0026#34;: \u0026#34;375\u0026#34;, 54\t\u0026#34;endpoint\u0026#34;: { 55\t\u0026#34;serviceName\u0026#34;: \u0026#34;reviews\u0026#34;, 56\t\u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.32\u0026#34; 57\t} 58\t} ] 59} Productpage 的 Proxy 上报了个 CS，CR, Reviews 的那个 Proxy 上报了个 SS，SR。分别表示 Productpage 作为 client 什么时候发出请求，什么时候最终收到请求，Reviews 的 Proxy 什么时候收到了客户端的请求，什么时候发出了 response。另外还包括这次访问的其他信息如 Response Code、Response Size 等。\n业务代码修改 根据前面的分析我们可以得到结论：埋点逻辑是在 Sidecar 代理中完成，应用程序不用处理复杂的埋点逻辑，但应用程序需要配合在请求头上传递生成的 Trace 相关信息。下面抽取服务代码来印证下结论，并看下业务代码到底是怎么修改的。\nPython 写的 Productpage 在服务端处理请求时，先从 Request 中提取接收到的 Header。然后再构造请求调用 Details 获取服务接口，并将 Header 转发出去。\n首先处理 Productpage 请问的 Restful 方法中从 Request 中提取 Trace 相关的 Header.\n1app.route(\u0026#39;/productpage\u0026#39;) 2 def front(): 3 product_id = 0 # TODO: replace default value 4 Headers = getForwardHeaders(Request) 5 … 6 detailsStatus, details = getProductDetails(product_id, headers) 7 reviewsStatus, reviews = getProductReviews(product_id, headers) 8 return … 1def getForwardHeaders(request): 2 Headers = {} 3 incoming_Headers = [ \u0026#39;x-request-id\u0026#39;, 4 \u0026#39;x-b3-traceid\u0026#39;, 5 \u0026#39;x-b3-spanId\u0026#39;, 6 \u0026#39;x-b3-parentspanid\u0026#39;, 7 \u0026#39;x-b3-sampled\u0026#39;, 8 \u0026#39;x-b3-flags\u0026#39;, 9 \u0026#39;x-ot-span-context\u0026#39; 10 ] 11 12 for ihdr in incoming_Headers: 13 val = request.headers.get(ihdr) 14 if val is not None: 15 headers[ihdr] = val 16 return headers for ihdr in incoming_Headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val return headers\n1def getProductReviews(product_id, headers): 2 url = reviews[\u0026#39;name\u0026#39;] + \u0026#34;/\u0026#34; + reviews[\u0026#39;endpoint\u0026#39;] + \u0026#34;/\u0026#34; + str(product_id) 3 res = requests.get(url, headers=Headers, timeout=3.0) Reviews 服务中 Java 的 Rest 代码类似，在服务端接收请求时，除了接收 Request 中的业务参数外，还要提取 Header 信息，调用 Ratings 服务时再传递下去。其他的 Productpage 调用 Details，Reviews 调用 ratings 逻辑类似。\n1@GET 2@Path(\u0026#34;/reviews/{productId}\u0026#34;) 3public Response bookReviewsById(@PathParam(\u0026#34;productId\u0026#34;) int productId, 4@HeaderParam(\u0026#34;end-user\u0026#34;) String user, 5@HeaderParam(\u0026#34;x-request-id\u0026#34;) String xreq, 6@HeaderParam(\u0026#34;x-b3-traceid\u0026#34;) String xTraceId, 7@HeaderParam(\u0026#34;x-b3-spanid\u0026#34;) String xSpanId, 8@HeaderParam(\u0026#34;x-b3-parentspanid\u0026#34;) String xparentSpanId, 9@HeaderParam(\u0026#34;x-b3-sampled\u0026#34;) String xsampled, 10@HeaderParam(\u0026#34;x-b3-flags\u0026#34;) String xflags, 11@HeaderParam(\u0026#34;x-ot-span-context\u0026#34;) String xotSpan) 当然这里只是个 demo，示意下要在那个环节修改代码。实际项目中我们不会这样在每个业务方法上作这样的修改，这样对代码的侵入，甚至说污染太严重。根据语言的特点会尽力把这段逻辑提取成一段通用逻辑，只要能在接收和发送请求的地方能机械的 forward 这几个 Trace 相关的 Header 即可。如果需要更多的控制，如在在 Span 上加特定的 Tag，或者在应用代码中代码中对某个服务内部方法的调用进详细跟踪需要构造一个 Span，可以使用类似 opentracing 的对应方法来实现。\n社区声明 最近一直在和社区沟通，督促在更显著的位置明确的告诉使用者用 Istio 作治理并不是所有场景下都不需要修改代码，比如调用链，虽然用户不用业务代码埋点，但还是需要修改些代码。尤其是避免首页“without any change”对大家的误导。得到回应是 1.1 中社区首页 what-is-istio 已经修改了这部分说明，不再是 1.0 中说without any changes in service code，而是改为with few or no code changes in service code。提示大家在使用 Isito 进行调用链埋点时，应用程序需要进行适当的修改。当然了解了其中原理，做起来也不会太麻烦。\n改了个程度轻一点的否定词，很少几乎不用修改，还是基本不用改的意思。这也是社区一贯的观点。\n结合对 Istio 调用链的原理的分析和一个典型例子中细节字段、流程包括代码的额解析，再加上和社区沟通的观点。得到以下结论：\nIstio 的绝大多数治理能力都是在 Sidecar 而非应用程序中实现，因此是非侵入的； Istio 的调用链埋点逻辑也是在 Sidecar 代理中完成，对应用程序非侵入，但应用程序需做适当的修改，即配合在请求头上传递生成的 Trace 相关信息。 文中内容比较多，有点啰嗦，原理性的东西不感兴趣也可以跳过，只要知道结论“Istio 调用链埋点业务代码要少量修改”就可以。\n","link":"https://idouba.com/istio-tracing-is-not-zero-code-change/","section":"posts","tags":["Istio","发表","Infoq"],"title":"Istio 调用链埋点原理剖析—是否真的“零修改”？"},{"body":"","link":"https://idouba.com/tags/%E5%8F%91%E8%A1%A8/","section":"tags","tags":null,"title":"发表"},{"body":"","link":"https://idouba.com/categories/%E5%8F%91%E8%A1%A8/","section":"categories","tags":null,"title":"发表"},{"body":"接上文Istio调用链埋点原理剖析—是否真的“零修改”分享实录（上）\nIsito调用链 调用链原理和场景 正如Service Mesh的诞生是为了解决大规模分布式服务访问的治理问题，调用链的出现也是为了对应于大规模的复杂的分布式系统运行中碰到的故障定位定界问题。大量的服务调用、跨进程、跨服务器，可能还会跨多个物理机房。无论是服务自身问题还是网络环境的问题导致调用上链路上出现问题都比较复杂，如何定位就比单进程的一个服务打印一个异常栈来找出某个方法要困难的多。需要有一个类似的调用链路的跟踪，经一次请求的逻辑规矩完整的表达出来，可以观察到每个阶段的调用关系，并能看到每个阶段的耗时和调用详细情况。Dapper, a Large-Scale Distributed Systems Tracing Infrastructure 描述了其中的原理和一般性的机制。模型中包含的术语也很多，理解最主要的两个即可：\nTrace：一次完整的分布式调用跟踪链路。 Span：跨服务的一次调用； 多个Span组合成一次Trace追踪记录。 上图是Dapper论文中的经典图示，左表示一个分布式调用关系。前端（A），两个中间层（B和C），以及两个后端（D和E）。用户发起一个请求时，先到达前端，再发送两个服务B和C。B直接应答，C服务调用后端D和E交互之后给A应答，A进而返回最终应答。要使用调用链跟踪，就是给每次调用添加TraceId、SpanId这样的跟踪标识和时间戳。\n右表示对应Span的管理关系。每个节点是一个Span，表示一个调用。至少包含Span的名、父SpanId和SpanId。节点间的连线下表示Span和父Span的关系。所有的Span属于一个跟踪，共用一个TraceId。从图上可以看到对前端A的调用Span的两个子Span分别是对B和C调用的Span，D和E两个后端服务调用的Span则都是C的子Span。\n调用链系统有很多实现，用的比较多的如zipkin，还有已经加入CNCF基金会并且的用的越来越多的Jaeger，满足Opentracing语义标准的就有这么多。\n一个完整的调用链跟踪系统，包括调用链埋点，调用链数据收集，调用链数据存储和处理，调用链数据检索（除了提供检索的APIServer，一般还要包含一个非常酷炫的调用链前端）等若干重要组件。上图是Jaeger的一个完整实现。这里我们仅关注与应用相关的内容，即调用链埋点的部分，看下在Istio中是否能做到”无侵入“的调用链埋点。当然在最后也会看下Istio机制下提供的不同的调用链数据收集方式。\nIstio标准BookInfo例子 简单期间，我们以Istio最经典的Bookinfo为例来说明。Bookinfo模拟在线书店的一个分类，显示一本书的信息。本身是一个异构应用，几个服务分别由不同的语言编写的。各个服务的模拟作用和调用关系是： productpage ：productpage 服务会调用 details 和 reviews 两个服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。并调用 ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。 调用链输出 在Istio上运行这个典型例子，不用做任何的代码修改，自带的Zipkin上就能看到如下的调用链输出。可以看到展示给我们的调用链和Boookinfo这个场景设计的调用关系一致：productpage 服务会调用 details 和 reviews 两个服务，reviews调用了ratings 微服务。除了显示调用关系外，还显示了每个中间调用的耗时和调用详情。基于这个视图，服务的运维人员比较直观的定界到慢的或者有问题的服务，并钻取当时的调用细节，进而定位到问题。 我们就要关注下调用链埋点到底是在哪里做的，怎么做的？\n在Istio中，所有的治理逻辑的执行体都是和业务容器一起部署的Envoy这个Sidecar，不管是负载均衡、熔断、流量路由还是安全、可观察性的数据生成都是在Envoy上。Sidecar拦截了所有的流入和流出业务程序的流量，根据收到的规则执行执行各种动作。实际使用中一般是基于K8S提供的InitContainer机制，用于在Pod中执行一些初始化任务. InitContainer中执行了一段iptables的脚本。正是通过这些Iptables规则拦截pod中流量，并发送到Envoy上。Envoy拦截到Inbound和Outbound的流量会分别作不同操作，执行上面配置的操作，另外再把请求往下发，对于Outbound就是根据服务发现找到对应的目标服务后端上；对于Inbound流量则直接发到本地的服务实例上。\n我们今天的重点是看下拦截到流量后Sidecar在调用链埋点怎么做的。\nIstio调用链埋点逻辑 Envoy的埋点规则和在其他服务调用方和被调用方的对应埋点逻辑没有太大差别。\nInbound流量：对于经过Sidecar流入应用程序的流量，如果经过Sidecar时Header中没有任何跟踪相关的信息，则会在创建一个根Span，TraceId就是这个SpanId，然后再将请求传递给业务容器的服务；如果请求中包含Trace相关的信息，则Sidecar从中提取Trace的上下文信息并发给应用程序。 Outbound流量：对于经过Sidecar流出的流量，如果经过Sidecar时Header中没有任何跟踪相关的信息，则会创建根Span，并将该跟Span相关上下文信息放在请求头中传递给下一个调用的服务；当存在Trace信息时，Sidecar从Header中提取Span相关信息，并基于这个Span创建子Span，并将新的Span信息加在请求头中传递。 特别是Outbound部分的调用链埋点逻辑，通过一段伪代码描述如图：\n调用链详细解析 如图是对前面Zipkin上输出的一个Trace一个透视图，观察下每个调用的细节。可以看到每个阶段四个服务与部署在它旁边上的Sidecar是怎么配合的。在图上只标记了Sidecar生成的Span主要信息。因为Sidecar 处理 Inbound和Outbound的逻辑有所不同，在图上表也分开两个框图分开表达。如productpage，接收外部请求是一个处理，给details发出请求是一个处理，给reviews发出请求是另外一个处理，因此围绕productpage这个app有三个黑色的处理块，其实是一个Sidecar在做事。\n同时，为了不使的图上箭头太多，最终的Response都没有表达出来，其实图上每个请求的箭头都有一个反方向的Response。在服务发起方的Sidecar会收到Response时，会记录一个CR(client Received)表示收到响应的时间并计算整个Span的持续时间。\n**下面通过解析下具体数据来找出埋点逻辑： **\n首先从调用入口的Gateway开始，Gateway作为一个独立部署在一个pod中的Envoy进程，当有请求过来时，它会将请求转给入口服务productpage。Gateway这个Envoy在发出请求时里面没有Trace信息，会生成一个根Span：SpanId和TraceId都是f79a31352fe7cae9，因为是第一个调用链上的第一个Span，也就是一般说的根Span，所有ParentId为空，在这个时候会记录CS（Client Send）； 请求从入口Gateway这个Envoy进入productpage的app业务进程其Inbound流量被productpage Pod内的Envoy拦截，Envoy处理请求头中带着Trace信息，记录SR(Server Received)，并将请求发送给productpage业务容器处理，productpage在处理请求的业务方法中在接受调用的参数时，除了接受一般的业务参数外，同时解析请求中的调用链Header信息，并把Header中的Trace信息传递给了调用的Details和Reviews的微服务。 从productpage出去的请求到达reviews服务前，其Oubtbound流量又一次通过同Pod的Envoy，Envoy埋点逻辑检查Header中包含了Trace相关信息，在将请求发出前会做客户端的调用链埋点，即以当前Span为parent Span，生成一个子Span：新的SpanId cb4c86fb667f3114，TraceId保持一致9a31352fe7cae9，ParentId就是上个Span的Id： f79a31352fe7cae9。 从prodcutepage到review的请求经过productpage的Sidecar走LB后，发给一个review的实例。请求在到达Review业务容器前，同样也被Review的Envoy拦截，Envoy检查从Header中解析出Trace信息存在，则发送Trace信息给reviews。reviews处理请求的服务端代码中同样接收和解析出这些包含Trace的Header信息，发送给下一个Ratings服务。 在这里我们只是理了一遍请求从入口Gateway，访问productpage服务，再访问reviews服务的流程。可以看到期间每个访问阶段，对服务的Inbound和Outbound流量都会被Envoy拦截并执行对应的调用链埋点逻辑。图示的Reviews访问Ratings和productpage访问Details逻辑与以上类似，这里不做复述。\n以上过程也印证了前面我们提出的Envoy的埋点逻辑。可以看到过程中除了Envoy再处理Inbound和Outbound流量时要执行对应的埋点逻辑外。每一步的调用要串起来，应用程序其实做了些事情。就是在将请求发给下一个服务时，需要将调用链相关的信息同样传下去，尽管这些Trace和Span的标识并不是它生成的。这样在出流量的proxy向下一跳服务发起请求前才能判断并生成子Span并和原Span进行关联，进而形成一个完整的调用链。否则，如果在应用容器未处理Header中的Trace，则Sidecar在处理请求时会创建根Span，最终会形成若干个割裂的Span，并不能被关联到一个Trace上，就会出现我们开始提到的问题。\n不断被问到两个问题来试图说明这个业务代码配合修改来实现调用链逻辑可能不必要：**问题一、**既然传入的请求上已经带了这些Header信息了，直接往下一直传不就好了吗？Sidecar请求APP的时候带着这些Header，APP请求Sidecar时也带着这些Header不就完了吗？**问题二、**既然TraceId和SpanId是Sidecar生成的，为什么要再费劲让App收到请求的时候解析下，发出请求时候再带着发出来传回给Sidecar呢？\n回答问题一，只需理解一点，这里的App业务代码是处理请求不是转发请求，即图上左边的Request to Productpage 到prodcutpage中请求就截止了，要怎么处理完全是productpage的服务接口的内容了，可以是调用本地处理逻辑直接返回，也可以是如示例中的场景构造新的请求调用其他的服务。右边的Request from productpage 完全是服务构造的发出的另外一个请求；**回答问题二，**需要理解当前Envoy是独立的Listener来处理Inbound和Outbound的请求。Inbound只会处理入的流量并将流量转发到本地的服务实例上。而Outbound就是根据服务发现找到对应的目标服务后端上。除了实在一个进程里外两个之间可以说没有任何关系。 另外如问题一描述，因为到Outbound已经是一个新构造的请求了，使得想维护一个map来记录这些Trace信息这种方案也变得不可行。\n这样基于一个例子来打开看一个调用链的主要过程就介绍到这里。附加productpage访问reviews的Span详细，删减掉一些数据只保留主要信息大致是这样：\n1\u0026#34;traceId\u0026#34;: \u0026#34;f79a31352fe7cae9\u0026#34;, 2 \u0026#34;id\u0026#34;: \u0026#34;f79a31352fe7cae9\u0026#34;, 3 \u0026#34;name\u0026#34;: \u0026#34;productpage-route\u0026#34;, 4 \u0026#34;timestamp\u0026#34;: 1536132571838202, 5 \u0026#34;duration\u0026#34;: 77474, 6 \u0026#34;annotations\u0026#34;: [ 7 { 8 \u0026#34;timestamp\u0026#34;: 1536132571838202, 9 \u0026#34;value\u0026#34;: \u0026#34;cs\u0026#34;, 10 \u0026#34;endpoint\u0026#34;: { 11 \u0026#34;serviceName\u0026#34;: \u0026#34;istio-ingressgateway\u0026#34;, 12 \u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.28\u0026#34; 13 } 14 }, 15 { 16 \u0026#34;timestamp\u0026#34;: 1536132571839226, 17 \u0026#34;value\u0026#34;: \u0026#34;sr\u0026#34;, 18 \u0026#34;endpoint\u0026#34;: { 19 \u0026#34;serviceName\u0026#34;: \u0026#34;productpage\u0026#34;, 20 \u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.33\u0026#34; 21 } 22 }, 23 { 24 \u0026#34;timestamp\u0026#34;: 1536132571914652, 25 \u0026#34;value\u0026#34;: \u0026#34;ss\u0026#34;, 26 \u0026#34;endpoint\u0026#34;: { 27 \u0026#34;serviceName\u0026#34;: \u0026#34;productpage\u0026#34;, 28 \u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.33\u0026#34; 29 } 30 }, 31 { 32 \u0026#34;timestamp\u0026#34;: 1536132571915676, 33 \u0026#34;value\u0026#34;: \u0026#34;cr\u0026#34;, 34 \u0026#34;endpoint\u0026#34;: { 35 \u0026#34;serviceName\u0026#34;: \u0026#34;istio-ingressgateway\u0026#34;, 36 \u0026#34;ipv4\u0026#34;: \u0026#34;172.16.0.28\u0026#34; 37 } 38 } 39 ] Productpage的Proxy上报了个CS，CR, reviews的那个proxy上报了个SS，SR。分别表示Productpage作为client什么时候发出请求，什么时候最终收到请求，reviews的proxy什么时候收到了客户端的请求，什么时候发出了response。另外还包括这次访问的其他信息如Response Code、Response Size等。\n根据前面的分析我们可以得到结论：埋点逻辑是在Sidecar代理中完成，应用程序不用处理复杂的埋点逻辑，但应用程序需要配合在请求头上传递生成的Trace相关信息。 服务代码修改示例 前面通过一个典型例子详细解析了Istio的调用链埋点过程中Envoy作为Sidecar和应用程序的配合关系。分析的结论是调用链埋点由Envoy来执行，但是业务程序要有适当修改。下面抽取服务代码来印证下。Python写的 productpage在服务端处理请求时，先从Request中提取接收到的Header。然后再构造请求调用details获取服务接口，并将Header转发出去。\n首先处理productpage请问的rest方法中从Request中提取Trace相关的Header。\n1app.route(\u0026#39;/productpage\u0026#39;) 2 def front(): 3 product_id = 0 # TODO: replace default value 4 headers = getForwardHeaders(request) 5 … 6 detailsStatus, details = getProductDetails(product_id, headers) 7 reviewsStatus, reviews = getProductReviews(product_id, headers) 8 return … 可以看到就是提取几个trace相关的header kv\n1def getForwardHeaders(request): 2 headers = {} 3 incoming_headers = [ \u0026#39;x-request-id\u0026#39;, 4 \u0026#39;x-b3-traceid\u0026#39;, 5 \u0026#39;x-b3-spanid\u0026#39;, 6 \u0026#39;x-b3-parentspanid\u0026#39;, 7 \u0026#39;x-b3-sampled\u0026#39;, 8 \u0026#39;x-b3-flags\u0026#39;, 9 \u0026#39;x-ot-span-context\u0026#39; 10 ] 11 for ihdr in incoming_headers: 12 val = request.headers.get(ihdr) 13 if val is not None: 14 headers[ihdr] = val 15 return headers 然后重新构造一个请求发出去，请求reviews服务接口。可以看到请求中包含收到的Header。 1def getProductReviews(product_id, headers): 2 url = reviews[\u0026#39;name\u0026#39;] + \u0026#34;/\u0026#34; + reviews[\u0026#39;endpoint\u0026#39;] + \u0026#34;/\u0026#34; + str(product_id) 3 res = requests.get(url, headers=headers, timeout=3.0) reviews服务中Java的Rest代码类似，在服务端接收请求时，除了接收Request中的业务参数外，还要提取Header信息，调用Ratings服务时再传递下去。其他的productpage调用details，reviews调用ratings逻辑类似。\n1@GET 2 @Path(\u0026#34;/reviews/{productId}\u0026#34;) 3 public Response bookReviewsById(@PathParam(\u0026#34;productId\u0026#34;) int productId, 4 @HeaderParam(\u0026#34;end-user\u0026#34;) String user, 5 @HeaderParam(\u0026#34;x-request-id\u0026#34;) String xreq, 6 @HeaderParam(\u0026#34;x-b3-traceid\u0026#34;) String xtraceid, 7 @HeaderParam(\u0026#34;x-b3-spanid\u0026#34;) String xspanid, 8 @HeaderParam(\u0026#34;x-b3-parentspanid\u0026#34;) String xparentspanid, 9 @HeaderParam(\u0026#34;x-b3-sampled\u0026#34;) String xsampled, 10 @HeaderParam(\u0026#34;x-b3-flags\u0026#34;) String xflags, 11 @HeaderParam(\u0026#34;x-ot-span-context\u0026#34;) String xotspan) 当然这里只是个demo，示意下要在那个位置修改代码。实际项目中我们不会这样在每个业务方法上作这样的修改，这样对代码的侵入，甚至说污染太严重。根据语言的特点会尽力把这段逻辑提取成一段通用逻辑。\nIstio调用链数据收集：by Envoy 一个完整的埋点过程，除了inject、extract这种处理Span信息，创建Span外，还要将Span report到一个调用链的服务端，进行存储并支持检索。在Isito中这些都是在Envoy这个Sidecar中处理，业务程序不用关心。在proxy自动注入到业务pod时，会自动刷这个后端地址.即Envoy会连接zipkin的服务端上报调用链数据，这些业务容器完全不用关心。当然这个调 用链收集的后端地址配置成jaeger也是ok的，因为Jaeger在接收数据是兼容zipkin格式的。 Istio调用链数据收集：by Mixer 除了直接从Envoy上报调用链到zipkin后端外，Istio提供了Mixer这个统一的面板来对接不同的后端来收集遥测数据，当然Trace数据也可以采用同样的方式。 即如TraceSpan中描述，创建一个TraceSpan的模板，来描述从mixer的一次访问中提取哪些数据，可以看到Trace相关的几个ID从请求的Header中提取。除了基础数据外，基于Mixer和kubernetes的继承能力，有些对象的元数据，如Pod上的相关信息Mixr可以补充，背后其实是Mixer连了kubeapiserver获取对应的pod资源，从而较之直接从Envoy上收集的原始数据，可以有更多的业务上的扩张，如namespace、cluster等信息APM数据要用到，但是Envoy本身不会生成，通过这种方式就可以从Kubernetes中自动补充完整，非常方便。\n这也是Istio的核心组件Mixer在可观察性上的一个优秀实践。\nIstio官方说明更新 最近一直在和社区沟通，督促在更显著的位置明确的告诉使用者用Istio作治理并不是所有场景下都不需要修改代码，比如调用链，虽然用户不用业务代码埋点，但还是需要修改些代码。尤其是避免首页“without any change”对大家的误导。得到回应是1.1中社区首页what-is-istio已经修改了这部分说明，不再是1.0中说without any changes in service code，而是改为with few or no code changes in service code。提示大家在使用Isito进行调用链埋点时，应用程序需要进行适当的修改。当然了解了其中原理，做起来也不会太麻烦。\n改了个程度轻一点的否定词，很少几乎不用修改，还是基本不用改的意思。这也是社区一贯的观点。结合对Istio调用链的原理的分析和一个典型例子中细节字段、流程包括代码的额解析，再加上和社区沟通的观点。得到以下结论：\nIstio的绝大多数治理能力都是在Sidecar而非应用程序中实现，因此是非侵入的； Istio的调用链埋点逻辑也是在Sidecar代理中完成，对应用程序非侵入，但应用程序需做适当的修改，即配合在请求头上传递生成的Trace相关信息。 华为云Istio服务网格公测中 改了个程度轻一点的否定词，很少几乎不用修改，还是基本不用改的意思。这也是社区一贯的观点。结合对Istio调用链的原理的分析和一个典型例子中细节字段、流程包括代码的额解析，再加上和社区沟通的观点。得到以下结论：\n在腾讯的场子上只讲干货的技术，尽量少做广告。在这里只是用一页PPT来简单介绍下华为云当前正在公测的Istio服务网格服务。\n华为云容器引擎CCE的深度集成，一键启用后，即可享受Istio服务网格的全部治理能力；基于应用运行的全景视图配置管理熔断、故障注入、负载均衡等多种智能流量治理功能；内置金丝雀、A/B Testing典型灰度发布流程灰度版本一键部署，流量切换一键生效；配置式基于流量比例、请求内容灰度策略配置，一站式健康、性能、流量监控，实现灰度发布过程量化、智能化、可视化；集成华为云APM，使用调用链、应用拓扑等多种手段对应用运行进行透视、诊断和管理。\n华为云Istio社区贡献 华为作为CNCF 基金会的初创会员、白金会员，CNCF / Kubernetes TOC 成员。在Kubernetes社区贡献国内第一，全球第三，全球贡献3000+ PR，先后贡献了集群联邦、高级调度策略、IPVS负载均衡，容器存储快照等重要项目。随着Istio项目的深入产品化，团队也积极投入到Istio的社区贡献。当前社区贡献国内第一，全球第三。Approver 3席，Member 6席，Contributor 若干。\n通过Pilot agent转发实现HTTP协议的健康检查： 针对mTLS enabled环境，传统的kubernetes http健康检查不能工作，实现 sidecar转发功能，以及injector的自动注入。\nIstioctl debug功能增强：针对istioctl缺失查询sidecar中endpoint的能力，增加proxy-config endpoint、proxy-status endpoint命令，提高debug效率。\nHTTPRetry API增强： 增加HTTPRetry 配置项RetryOn策略，可以通过此控制sidecar重试。\nMCP 配置实现： Pilot支持mesh configuration, 可以与galley等多个实现了MCP协议的server端交互获取配置。以此解耦后端注册中心。\nPilot CPU异常问题解决：1.0.0-snapshot.0 pilot free 状态CPU 利用率超过20%，降低到1%以下。\nPilot 服务数据下发优化：缓存service，避免每次push时进行重复的转换。\nPilot服务实例查询优化：根据label selector查询endpoints（涵盖95%以上的场景），避免遍历所有namespace的endpoints。\nPilot 数据Push性能优化：将原有的串行顺序推送配置，更新为并行push，降低配置下发时延。\n今天分享的只是一个点上的部分内容，Istio和容器技术更多技术分享欢迎大家关注“容器魔方”，容器前沿技术与最佳行业实践的公众号，并可申请加入一个超过2000人的容器技术交流群组，参与容器相关技术的讨论。\n谢谢大家。感谢K8S技术社区提供的这次很棒的活动。感谢腾讯的小伙伴提供的非常棒的场地和现场服务。 ","link":"https://idouba.com/istio-tracing-meetup-02/","section":"posts","tags":["调用链","演讲","Istio"],"title":"Istio调用链埋点原理剖析—是否真的“零修改”分享实录（下）"},{"body":"","link":"https://idouba.com/tags/%E8%B0%83%E7%94%A8%E9%93%BE/","section":"tags","tags":null,"title":"调用链"},{"body":"整理自2018年在K8S技术社区在腾讯大厦关于Istio调用链的分享。\n前言 大家好，我是zhangchaomeng，来自华为Cloud BU，当前在做华为云应用服务网格。今天跟大家分享的主题是Istio调用链相关内容。通过剖析Istio的架构机制与Istio中调用链的工作原理来解答一个大家经常问道的一个问题：Istio是否像其官方文档中宣传的一样，对业务代码完全的无侵入，无需用做任何修改就可以完成所有的治理能力，包括调用链的埋点？\n关于这个问题，可以提前透漏下，答案是让人有点沮丧的，得改点。在Isito中你不用在自己的代码里使用各种埋点的SDK来做埋点的逻辑，但是必须要有适当的配合的修改。\n为什么本来无侵入的Service Mesh形态的技术却要求我们开发者修改些代码，到底要做哪些修改？Istio中调用链到底是怎么工作的？在下面的内容中将逐个回答这些问题。\n本次分享的主题包括两部分: 第一部分作为背景和基础，介绍Istio的架构和机制；第二部分将重点介绍Istio调用链的相关内容，解答前面提出的几个问题。\nIsito的架构和机制 Service Mesh 如官方介绍，Istio是一个用于连接、控制、保护和观测服务的一个开放平台。即：智能控制服务间的流量和API调用；提供授权、认证和通信加密机制自动保护服务安全；并使用各种策略来控制调用者对服务的访问；另外可以扩展丰富的调用链、监控、日志等手段来对服务的与性能进行观测。\nIstio是Google继Kubernetes之后的又一重要项目，提供了Service Mesh方式服务治理的完整的解决方案。2017年5月发布第一个版本 0.1， 2018年6月1日发布了0.8版本，第一个LTS版本，当前在使用的1.0版本是今年7.31发布，对外宣传可用于生产。最新的1.1版本将2018.11中旬最近发布(当时规划实际已延迟，作者注)。\nIstio属于Service Mesh的一种实现。通过一张典型图来了解下Service Mesh。如图示深色是Proxy，浅色的是服务，所有流入流出服务的都通过Proxy。Service Mesh正是由这一组轻量代理组成，和应用程序部署在一起，但是应用程序感知不到他的存在。特别对于云原生应用，服务间的应用访问拓扑都比较复杂，可以通过Service Mesh来保证服务间的调用请求在可靠、安全的传递。在实现上一般会有一个统一的控制面，对这些代理有个统一的管理，所有的代理都接入一个控制面。对代理进行生命期管理和统一的治理规则的配置。 这里是对Service Mesh特点的一个一般性描述，后面结合Isito的架构和机制可以看下在Istio中对应的实现。\n可以看到Service Mesh最核心的特点是在Proxy中实现治理逻辑，从而做到应用程序无感知。其实这个形态也是经过一个演变的过程的：\n最早的治理逻辑直接由业务代码开发人员设计和实现，对服务间的访问进行管理，在代码里其实也不分治理和业务，治理本身就是业务的一部分。这种形态的缺点非常明显就是业务代码和治理的耦合，同时公共的治理逻辑有大量的重复。\n很容易想到封装一个公共库，就是所谓的SDK，使用特定的SDK开发业务，则所有治理能力就内置了。Spring Cloud和Netflix都是此类的工具，使用比较广泛，除了治理能力外，SDK本身是个开发框架，基于一个语言统一、风格统一的开发框架开发新的项目非常好用。但这种形态语言相关，当前Java版本的SDK比较多。另外对于开发人员有一定的学习成本，必须熟悉这个SDK才能基于他开发。最重要的是推动已经在用的成熟的系统使用SDK重写下也不是个容易的事情。比如我们客户中就有用C开发的系统，运行稳定，基本不可能重写。对这类服务的治理就需要一个服务外面的治理方式。\n于是考虑是否可以继续封装，将治理能力提到进程外面来，作为独立进程。即Sidecar方式，也就是广泛关注的Service Mesh 的。真正可以做到对业务代码和进程0侵入，这对于原来的系统完全不用改造，直接使用Sidecar进行治理。\n用一段伪代码来表示以上形态的演变：\n可以看到随着封装越来越加强，从公共库级别，到进程级别。对业务的侵入越来越少，SDK的公共库从业务代码中解耦，Sidecar方式直接从业务进程解耦了。对应的治理位置越来越低，即生效的位置更加基础了。尤其是Service Mesh方式下面访问通过 Proxy执行治理，所以Service Mesh的方式也已被称为一种应用的基础设施层，和TCP/IP的协议栈一样。TCP/IP负责将字节流可靠地在网络节点间传递；而应用基础设施则保证服务间的请求在安全、可靠、可被管控的传递。这也对应了前面Istio作为Service Mesh一种实现的定位。 Istio 关键能力 Istio官方介绍自己的关键能力如上所示，我把它分为两部分：一部分是功能，另有一部分提供的扩展能力。\n功能上包括流量管理、策略执行、安全和可观察性。也正好应对了首页的连接、保护、控制和观测四大功能。\n流量管理：是Istio中最常用的功能。可以通过配置规则和访问路由，来控制服务间的流量和API调用。从而实现负载均衡、熔断、故障注入、重试、重定向等服务治理功能，并且可以通过配置流量规则来对将流量切分到不同版本上从而实现灰度发布的流程。 策略执行：指Istio支持支持访问控制、速率限制、配额管理的能力。这些能力都是通过可动态插入的策略控制后端实现。 安全：Istio提供的底层的安全通道、管理服务通信的认证、授权，使得开发任务只用关注业务代码中的安全相关即可。 可观察性：较之其他系统和平台，Istio比较明显的一个特点是服务运行的监控数据都可以动态获取和输出，提供了强大的调用链、监控和调用日志收集输出的能力。配合可视化工具，运维人员可以方便的看到系统的运行状况，并发现问题进而解决问题。我们这次分享的主题调用链也正是Isito可观察性的一个核心能力。 后面分析可以看到以上四个特性从管理面看，正好对应Istio的三个重要组件。\n扩展性：主要是指Istio从系统设计上对运行平台、交互的相关系统都尽可能的解耦，可扩展。这里列出的特性：\n平台支持：指Istio可以部署在各种环境上，支持Kubernetes、Consul等上部署的服务，在之前版本上还支持注册到Eureka上的Service，新版本对Eureka的支持被干掉了；\n集成和定制：指的Istio可以动态的对接各种如访问控制、配额管理等策略执行的后端和日志监控等客观性的后端。支持用户根据需要按照模板开发自己的后端方便的集成进来。\n其实这两个扩展性的能力正好也对应了Istio的两个核心组件Pilot和Mixer，后面Isito架构时一起看下。\nIstio 总体架构 以上是Isito的总体架构。上面是数据面，下半部分是控制面。 数据面Envoy是一个C++写的轻量代理，可以看到所有流入流出服务的流量都经过Proxy转发和处理，前面Istio中列出的所有的治理逻辑都是在Envoy上执行，正是拦截到服务访问间的流量才能进行各种治理；另外可以看到Sidecar都连到了一个统一的控制面。\nIstio其实专指控制面的几个服务组件：\nPilot：Pilot干两个事情，一个是配置，就是前面功能介绍的智能路由和流量管理功能都是通过Pilot进行配置，并下发到Sidecar上去执行；另外一个是服务发现，可以对接不同的服务发现平台维护服务名和实例地址的关系并动态提供给Sidecar在服务请求时使用。Pilot的详细功能和机制见后面组件介绍。 Mixer：Mixer是Istio中比较特殊，当前甚至有点争议的组件。前面Isito核心功能中介绍的遥测和策略执行两个大特性均是Mixer提供。而Istio官方强调的集成和定制也是Mixer提供。即可以动态的配置和开发策略执行与遥测的后端，来实现对应的功能。Mixer的详细功能和机制见后面组件介绍。 Citadel：主要对应Istio核心功能中的安全部分。配合Pilot和Mixer实现秘钥和证书的管理、管理授权和审计，保证客户端和服务端的安全通信，通过内置的身份和凭证提供服务间的身份验证，并进而该通基于服务表示的策略执行。 Isito主要组件Pilot 如Istio架构中简介，Pilot实现服务发现和配置管理的功能。 作为服务发现，Pilot中定义了一个抽象的服务模型，包括服务、服务实例、版本等。并且只定义的服务发现的接口，并未实现服务发现的功能，而是通过Adapter机制以一种可扩展的方式来集成各种不同的服务发现，并转换成Istio通用的抽象模型。 如在Kubernetes中，Pilot中的Kubernetes适配器通过Kube-APIServer服务器得到Kubernetes中对应的资源信息。而对于像Eureka这种服务注册表，则是使用一个Eureka的HTTP Client去访问Eureka的名字服务的集群，获取服务实例的列表。不管哪种方式最终都转换成Pilot的标准服务发现定义，进而通过标准接口提供给Sidecar使用。\n而配置管理，则是定义并维护各种的流量规则，来实现负载均衡、熔断、故障注入、流量拆分等功能。并转换成Envoy中标准格式推送给Envoy，从而实现治理功能。所有的这些功能用户均不用修改代码接口完成。详细的配置方式可以参照Istio Traffic Routing中的规则定义。重点关注：VirtualService、 DestinationRule、 Gateway等规则定义。如可以使用流量规则来配置各种灰度发布，也可以通过注入一个故障来测试故障场景；可以配置熔断来进行故障恢复；并且可以对HTTP请求根据我们的需要进行重定向、重写，重试等操作。\nIstio主要组件Mixer Mixer是Isito特有的一个组件。主要做两个功能Check和Report，分别对应Istio官方宣传的两个重大特性策略执行和遥测功能。逻辑上理解每次服务间的请求都会通过proxy连接Mixer来进行处理，由Mixer来将请求派发到对应的后端上处理。通过扩展不同的后端来增强Mixer的能力。如可以做访问控制、配额等这样的控制，也可以对接不同的监控后端来做监控数据的收集，进而提供网格运行的可观察性能力。 Mixer通过使用通用插件模型实现的对接不同后端，避免了proxy为了完成不同的功能而去对接各种不同的后端。每个插件都被称为Adapter。对于每个请求Sidecar会从每一次请求中收集相关信息，如请求的路径，时间，源IP，目地服务，tracing头，日志等，并请这些属性上报给Mixer。Mixer和后端服务之间是通过适配器进行连接的，Mixer将Sidecar上报的内容通过适配器发送给后端服务。可以在不停止应用服务的情况下动态切换后台服务。\n除了可以通过adapter机制接入不同的后端，mixer还支持根据需要定义收集的metric，和对metric的处理方式，如样例所示，可以自定义监控指标。\n后面我们会看到Istio中调用链的数据也可以通过Mixer来收集。\nIstio和Kubernetes的天然结合 尽管Isito强调自己的可扩展性的重要一点就是可以适配各种不同的平台，但实际场景上，甚至看Istio当前代码、设计可以发现其所有重要的能力都是基于Kubernetes展开的。Istio与Kubernetes结合之紧密，甚至有描述说看上去是一个团队开发的。即Istio就是基于Kubernetes之上，对Kubernetes能力的补齐。\n从功能场景看，Kubernetes提供了部署、升级和有限的运行流量管理能力；利用Service的机制来做服务注册和发现，转发，通过Kubeproxy有一定的转发和负载均衡能力。但是往上的如熔断、限流降级、调用链等治理能力就没有了。前面的功能介绍可以发现Istio很好的补齐了Kubernetes在服务治理上的这部分能力。即Kubernetes提供了基础服务运行能力，而Istio基于其上提供服务治理能力，对Kubernetes服务的治理能力。\n除了功能互补外，从形态上看Istio也是基于Kubernetes构建的。包括： Sicecar 运行在Kubernetes Pod里，作为一个Proxy和业务容器部署在一起，部署过程对用户透明。Mesh中要求业务程序的运行感知不到Sidecar的存在，基于Kubernetes的pod的设计这部分做的更彻底，对用户更透明，通过Isito的自动注入用户甚至感知不到部署Sidecar的这个过程，和部署一个一般的Deployment没有任何差别。试想如果是通过VM上部署一个Agent，不会有这么方便。\n另外Istio的服务发现也是非常完美基于Kubernetes的域名访问机制构建。Isito中的服务就是Kubernetes的服务，避免了之前使用独立的微服务框架在Kubernetes上运行时两套名字服务的尬尴和困惑。机制上Pilot通过在kubernetes里面注册一个controller来监听事件，从而获取Service和Kubernetes的Endpoint以及Pod的关系，并将这些映射关系转换成为Istio的统一抽象模型下发到Envoy进行转发。\nIstio所有的我们熟悉的路由规则、控制策略都是通过Kubernetes CRD表达，不需要一个单独的APIserver和后端的配置管理。所以Istio APIServer就是Kubernetes的KubeAPIServer，数据也当然的存在了对应Kubernetes的ETCD中。\n就连Istio的命令行工具Istioctl都是类似Kubectl风格的功能，提供基于命令行的配置功能。\n上篇主要是背景的信息，Istio调用链相关请参照：Istio调用链埋点原理剖析—是否真的“零修改”分享实录（下）\n","link":"https://idouba.com/istio-tracing-meetup-01/","section":"posts","tags":["调用链","演讲","Istio"],"title":"Istio调用链埋点原理剖析—是否真的“零修改”分享实录（上）"},{"body":"2018中秋，十多年了第一次在这天回到家里。不是为团聚，而是为离别。\n昨晚九点四十上接到的电话，祖母走了。当时没有难过，可能是因为还没想好接受。前天刚视频过，老太太脸色上看人还胖了点，和我说了许久\n然后，然后人就不行了。实在忍不住捶胸，吓坏了客厅的孩子。\n知道当前这种距离格局下这是迟早的，但没想到会来的这么快这么突然。真到来时还是没法接受。\n悔恨，懊恼，有点扭曲的心里撕裂的疼。\n前天中午视频的时候有点漫不经心，可能是上班中午困了吧。老太太来来回回就是这么几句：娃多大了，上学好好去的吧，不要呵斥娃，有话好好说，娃还小。来回几遍，偶尔会来一遍过年把娃领回来看看啊！\n后悔最近变懒了，事情多了，原来隔天视频最近一周才一次，最近这次两周才打的。\n老太太说她吃了两个菜合煎饼，妈妈说吃了三个。老太太一直胃口都还可以，每天晚上还会喝碗鲜奶。\n但明显今年越来越懒了，不爱动了。坐下就不愿意起来，喜欢坐在那儿犯困发呆。今年回家几次拉着出去散步，走不了一百米就说“咱回，不走了”。而且明显一次比一次懒了。过年的时候能硬拽着走几百米，五月份就只能哄着绕着村子转了，七月份那次真的是从凳子上拉起来都费劲，太懒了。\n七月份一起走路时还给我回忆了她当年当妇女队长的事，对一个已经老年痴呆有点老糊涂的老太太来说真是不容易。\n有人算过在外地一辈子和一生中和家人见面的次数总共有非常可怜的几十次。对我和祖母，我早知道是和零非常接近的一个个位数，今天这个数字终于走到了那个零。\n回想下见面的次数真的能数的过来：\n十五岁去外地念书，才开学二十天，老太太就跟着爸爸和弟弟就坐了一夜的火车到我的学校。宿舍的同学国庆节都回家了，就住在我的宿舍里，半夜老太太又起来给我盖被子，和在家一样。四年中专回过九次家，四年寒暑假外有一次是在洛阳重机厂车间实习，国庆回了趟家。\n后来十九岁毕业了，要去东北铁路工地上修铁路，从渭南铁一局桥梁处的机关培训完中午赶回家，晚上就要坐火车去沈阳工地，老太太下午给缝了一个新的棉花褥子。带着就上了晚上往东北的火车。在工地上第一次收到妈妈的来信，想起祖母，这条新到工地上也算是铁路汉子哭的停不下来，记得工友刘哥他们还以为家里出了什么事，一直过来安慰我。\n再后来从工地回来，回到离家近的西安，干点不像样的工作挣点小钱，自个儿再去看点书学点东西，从修桥的转到了现在的所谓计算机行当，然后考研考到西工大。那几年虽然离家不远，但基本上也是三四个月半年回一次。家人管这个叫上进有事业心，老太太也跟着说好好学习不用惦记她，后来回想只想再多抽自己几个嘴巴子。\n研究生毕业几个offer上选择时，留在西安还是去外地，最终还是被趋势的工作内容打动，选择了先出去工作两年再回西安。清楚记得和一个知心的师姐和他家大哥一起商量，在外地工作和老太太见面就不方便了，那会儿还没有微信，当时商量认为用电脑摄像头视频挺方便的，可以随时至少每天见到。但事实上两年后并没有回去，两年里也没有过一次视频过。反倒是回家的次数越来越少，停留的时间越来越短，和老人电话里的沟通也越来越机械，中午吃的什么，天气冷不冷这些。再后来老太太老年痴呆越来越糊涂了，连中午吃的什么都说开始瞎胡说了。\n后来有了自己的家，老太太那就是一个老家，老家里的一个重要符号，但也只是个符号，有时会打开，有时候忙了就顾不上了，中间有几个过年都没有回去看她。每次从家回来会隔一天在中午吃饭后和老太太微信视频下，但后面忙起来懒起来就一周一次都不一定了。老太太还是重复那几句：娃多大了，上学好好去不？娃小不要呵斥娃，好好和娃说话。老太太后面还学会了一句“还有啥跟婆说不？没了挂了吧，回去歇着吧”，因为有时有点累或者懒我说话的音量和情绪让老太太也不自在，经常“你说啥”问个个不停，甚至答非所问得懒懒的说上一会儿。\n老太太对孙子们的疼爱是出了名的。曾经有过一阵子小学上课搞两大晌，大家上学都是急急忙忙拿个馍装书包就走了，只有我家老太太很早起来做好孙子一人份稀饭，热腾腾的吃完才去，然后她再给家人做第二顿早饭。\n小学初中时候每天中午吃完饭手里拿一个黄灿灿的灶塘里烤好的热腾腾又脆脆的干馍是小伙伴们最羡慕的，比起其他人的一处焦一处白的，手里这个黄干脆脆的东西老太太每天得花多少心思，多少感情在上面。\n记得上小学还是初中时候有个邻居曾经开玩笑以后去外地念书上班把你奶奶带着，老太太就可以一直给你做饭了，老太太也不用整天在家里念叨孙子了。这是玩笑，却也是实情。从很小到长到成年，到现在为人父，还没有做好没有这个老太太的世界是什么样子。不是不够坚强，是真的没有习惯。\n清楚记得初一第一次语文课，题目是那会儿很常见的《我的**》，很多人写《我的妈妈》或者《我的爸爸》，我的一篇《我的奶奶》被特别表扬了，不是这个从小就表现出理工男特质的小屁孩的文采能有多好，只是因为流水帐一样写下老太太的日常就足够感动那个姓史的语文老师。\n从记事开始就和爷爷奶奶一起睡在老房子的土炕上，冬天里点火烧炕的，有时整个屋子都是烟熏的睁不开眼。到后来搬到这个新房子，每次回家也会睡奶奶的大炕上，大炕那头连着厨房的火塘，暖烘烘的，冬天里贴着腰特别舒服，踏实，去乏。当然不多的几次带着老婆孩子回家都会睡楼上的自己的房间。就在奶奶走前的一个月，西安出差周末回家了三天，还睡过奶奶的这个大坑。一般都是奶奶睡炕那头，我睡炕这头。经常半夜老太太又挪到大坑这头，给大孙子把被子角往上拽拽，尽管这个大孙子已经不是两三岁，而是近四十的孩子他爹。曾经非常想领着那个整天蹦哒停不下来的儿子来老祖母的大炕上感受一下，感受下开阔和暖烘烘大炕，感受下老祖母的开阔又有温度的爱，但终没来的及。想到这里，觉得对不起孩子，更对不起那个老太太。\n这次回来，大炕还是同样的温度，但却空荡荡。老太太独自躺在前厅的玻璃柜子里，等待儿孙们送她到西边地里爷爷的身旁。妈妈说天渐变凉，奶奶走的前一天晚上刚从大炕上换下了夏天的薄床单，换上了秋冬的厚毛线床单。\n晚上一个人去了老太太将要安息的地方。苹果园地头的这棵大白杨树下不远就是。通往这里家西边这条路是以前每次和老太太咕咚健走的地方。\n一个人又走了遍队友一起走过的这段路，今天下着小雨静的吓人，一个人但一路走过来路边的核桃树、苹果树、空地的话题都在：“现在这杮子品种不行，没人要了；苹果现在太多了所以不值钱了；你看这家地里草多高，啥也没种，年轻娃都出去打工不种地了”\n后来又领着姨奶奶在这条路上散步，奶奶的妹妹比奶奶小了十多岁，给我讲她们的母亲走的早，奶奶做为老大怎么照顾他们姊妹四个。后来她们成家后又挨个照看二姨奶奶的五个表叔叔，给做针线活，给做吃的养活他们。姨奶奶的外形和神态像极了奶奶，一起走回来就像之前领着奶奶一样，乱乱的心里有了些许慰藉。就像姨奶奶说的，奶奶用他的善良辛劳节俭养活了一大家的人，养活和影响了几辈人。\n早上上楼，后院的核桃树上突然有一大群长尾的鸟飞过来，停下来，叽叽喳喳叫的很急，很快又都飞走了。在老太太眼里这棵核桃树上产的都是最宝贝的东西，每年收下来都在楼上一遍一遍的晾干，然后等孙子回来装成小袋子，塞到他的箱子里。但可恨的是，孙子有多孙子，一年一年的带去，发现去年的还没吃，已经放出虫子了。\n今天家里人很多，过两天要按村里人的习俗有个仪式为老太太送行。人多说话声音很杂，但人群里总是冷不丁感觉到一个熟悉的声音，可能是一个大妈婶婶的声音和老太太很像，或者是我幻听了，一天一直在幻听。中午人少的时候，看看厨房、后院、大炕，总感觉她还在那里，揽柴火、烧锅，或者坐在炕边椅子上发呆。笤帚、水杯、椅子，家里的每个大小物件都是她的日常，总是感觉那个熟悉的身影又慢悠悠的走过来。\n没什么精神，今天大多时间都是扎着脑袋坐在老太太常坐的小椅子上，总感觉有那种轻轻的鞋底在地上塔拉的声音在慢慢走近，然后有人站在前面，笑眯眯的，手里拿着个小吃的。\n老太太自己不太吃饭以外的东西，但给我们很上心。记得之前家里前厅开着小卖部，夏天放假回家，老太太总隔会儿从冰柜里取出个雪糕，“尝尝，今儿很多娃买这种”。遗憾的是我对这些零食也一直不感兴趣，好像没吃过那些雪糕，老太太总是扫兴的自己又放回去了。再后来老太太就开始糊涂了，什么东西多少钱自己乱说开了，收的钱放那儿了自己也说不清了。然后小卖部就关门了。\n每次回家带给她的吃的只有当时打开她才吃，自己从来想不起吃，其实是舍不得变成的一种习惯。今天老太太房间的桌子上，上个月西安出差回家买的那些吃的，几样还没开包装，有几样还是我那天打开两人分着吃剩下的。当时在前厅，吹着过道凉风，分着吃各种干果、水果的感觉和那次一百米的咕咚健步走是最后一次印象深刻的团队运动，遗憾的是，老太太吃几个就说“你吃你吃，婆不吃了”。\n老太太太爱操心，操小家的心，操大家的心，直到湖涂了有些原来操心的事想不起来了，才慢慢消停了。但仍然每天晚上会检查家里大门有没有关好，检查了又忘了，有时回好几遍。祖母走的昨晚，就是自己跑到前屋检查了两遍大门后回去，睡觉前又跑出来，突然发病摔倒就再也没有醒过来。\n老太太走的太急，一家人到现在还不能接受。大家族里一个上年纪的四婶专门过来安慰妈妈：“这都是一辈子积德积来的福份，八十八岁高龄，不影响吃喝，没躺炕上或医院受一天的罪，就这样安静利索的走了。自己不受罪，也不连累子女们。老太太临到最后都是不给人添麻烦。一辈子都是这样，只给人帮忙，从来不给人添麻烦。你看走的这个日子还是八月十五娃放假，都不让孙子们麻烦请假。”。四婶年龄比奶奶小不了几岁，说的是对的。以前这些老太太们的这些有点迷信的道理是不爱听的，但这回是听进去了。老太太真的是这个时候也不想给儿孙们添麻烦，连百日祭的日子都是后面的元旦假期了。\n说德高望重是在整个村里人们对她的评价。后辈的眼里，祖母生活中传递给我们的的品质更值得铭记。勤俭、正直、善良是邻里们对老太太的评价，老太太唱给我们说的是行善不做恶。从来不做亏人的事情。对亲人，邻居，或者陌生人。以前门口要饭的到这家门口，都比较喜欢找这个老太太。\n昨晚在杭州家里从接到电话到四点多赶第一班飞机，基本没睡。今晚到从九点一觉睡到早上，睡的很踏实。心里不平静，一阵一阵的疼，还是那种手机备忘录纪录下思绪。\n今天是中秋节，中秋月满，祖母也在这天圆满的走完了这一生。村里人说连倒下这天也是考虑了儿孙们这两天有假期。因为都说她一辈子只为亲人操心，唯独自己总是凑合。老太太不知道自己的生日，中秋节对于全家人会有另外一种意义。\n之前从来没有想过来世的说法，也第一次自己提起这个词。但此刻真的希望有来世，再当您的孙子，我真的没当够。您养我长大，我陪您健走。如果有来世，不孝孙子一定不会跑的离您这么远了。\n和我的咕咚队友来个约定，每月的月圆的那天会是特别的一天，8800米，纪念我88岁的咕咚队友，让我静静地想您，流着汗，流着泪，我相信您会在我身边一起跟着我慢慢的跑，我会一直跑，一直跑到我跑不动的那一天。\n2018，农历八月十五前夜，月圆满，人圆满。八十八岁，送别我的祖母，送别我的咕咚队友。\n","link":"https://idouba.com/mid-autumn-and-my-grandmother/","section":"posts","tags":["祖母"],"title":"月圆满，人圆满，中秋别祖母"},{"body":"【摘要】 本文基于Pilot服务发现Kubernetes部分源码重点介绍在Istio on Kubernetes环境下，如何基于Pilot的Adapter机制实现Istio管理的服务直接使用Kubernetes service来做统一服务发现，避免了其他微服务框架运行在Kubernetes环境时上下两套服务目录的局面。并以此为入口从架构、场景等方面总结下Istio和Kubernetes的结合关系。\n前言 文章Istio技术与实践01： 源码解析之Pilot多云平台服务发现机制结合Pilot的代码实现介绍了Istio的抽象服务模型和基于该模型的数据结构定义，了解到Istio上只是定义的服务发现的接口，并未实现服务发现的功能，而是通过Adapter机制以一种可扩展的方式来集成各种不同的服务发现。本文重点讲解Adapter机制在Kubernetes平台上的使用。即Istio on Kubernetes如何实现服务发现。\nKubernetes和Istio的结合Kubernetes和Istio的结合 从场景和架构上看Istio和Kubernetes都是非常契合的一种搭配。从场景和架构上看Istio和Kubernetes都是非常契合的一种搭配。\n首先从场景上看Kuberntes为应用负载的部署、运维、扩缩容等提供了强大的支持。通过Service机制提供了负载间访问机制，通过域名结合Kubeproxy提供的转发机制可以方便的访问到对端的服务实例。因此如上图可以认为Kubernetes提供了一定的服务发现和负载均衡能力，但是较深入细致的流量治理能力，因为Kubnernetes所处的基础位置并未提供，而Istio正是补齐了这部分能力，两者的结合提供了一个端到端的容器服务运行和治理的解决方案。\n从架构看Istio和Kubernetes更是深度的结合。 得益于Kuberntes Pod的设计，数据面的Sidecar作为一种高性能轻量的代理自动注入到Pod中和业务容器部署在一起，接管业务容器的inbound和outbound的流量，从而实现对业务容器中服务访问的治理。在控制面上Istio基于其Adapter机制集成Kubernetes的域名，从而避免了两套名字服务的尴尬场景。\n在本文中将结合Pilot的代码实现来重点描述图中上半部分的实现，下半部分的内容Pilot提供的通用的API给Envoy使用可参照上一篇文章的DiscoverServer部分的描述。\n基于Kubernetes的服务发现 理解了Pilot的ServiceDiscovery的Adapter的主流程后，了解这部分内容比较容易。Pilot-discovery在initServiceControllers时，根据服务注册配置的方式，如果是Kubernetes，则会走到这个分支来构造K8sServiceController。\n1case serviceregistry.KubernetesRegistry: 2s.createK8sServiceControllers(serviceControllers, args); err != nil { 3return err 4} 创建controller其实就是创建了一个Kubenernetes的controller，可以看到List/Watch了Service、Endpoints、Node、Pod几个资源对象。\n1// NewController creates a new Kubernetes controller 2func NewController(client kubernetes.Interface, options ControllerOptions) *Controller { 3 out := \u0026amp;Controller{ 4 domainSuffix: options.DomainSuffix, 5 client: client, 6 queue: NewQueue(1 * time.Second), 7 } 8 out.services = out.createInformer(\u0026amp;v1.Service{}, \u0026#34;Service\u0026#34;, options.ResyncPeriod, 9 func(opts meta_v1.ListOptions) (runtime.Object, error) { 10 return client.CoreV1().Services(options.WatchedNamespace).List(opts) 11 }, 12 func(opts meta_v1.ListOptions) (watch.Interface, error) { 13 return client.CoreV1().Services(options.WatchedNamespace).Watch(opts) 14 }) 15 out.endpoints = out.createInformer(\u0026amp;v1.Endpoints{}, \u0026#34;Endpoints\u0026#34;, options.ResyncPeriod, 16 func(opts meta_v1.ListOptions) (runtime.Object, error) { 17 return client.CoreV1().Endpoints(options.WatchedNamespace).List(opts) 18 }, 19 func(opts meta_v1.ListOptions) (watch.Interface, error) { 20 return client.CoreV1().Endpoints(options.WatchedNamespace).Watch(opts) 21 }) 22 out.nodes = out.createInformer(\u0026amp;v1.Node{}, \u0026#34;Node\u0026#34;, options.ResyncPeriod, 23 func(opts meta_v1.ListOptions) (runtime.Object, error) { 24 return client.CoreV1().Nodes().List(opts) 25 }, 26 func(opts meta_v1.ListOptions) (watch.Interface, error) { 27 return client.CoreV1().Nodes().Watch(opts) 28 }) 29 out.pods = newPodCache(out.createInformer(\u0026amp;v1.Pod{}, \u0026#34;Pod\u0026#34;, options.ResyncPeriod, 30 func(opts meta_v1.ListOptions) (runtime.Object, error) { 31 return client.CoreV1().Pods(options.WatchedNamespace).List(opts) 32 }, 33 func(opts meta_v1.ListOptions) (watch.Interface, error) { 34 return client.CoreV1().Pods(options.WatchedNamespace).Watch(opts) 35 })) 36return out }\n在 createInformer 中其实就是创建了SharedIndexInformer。这种方式在Kubernetes的各种Controller中广泛使用。Informer调用 APIserver的 List 和 Watch 两种类型的 API。在初始化的时，先调用 List API 获得全部资源对象，缓存在内存中; 然后，调用 Watch API 去 Watch这种这种资源对象，维护缓存。\n1Service informer := cache.NewSharedIndexInformer( 2 \u0026amp;cache.ListWatch{ListFunc: lf, WatchFunc: wf}, o, 3 resyncPeriod, cache.Indexers{}) 下面看下Kubernetes场景下对ServiceDiscovery接口的实现。我们看下Kubernetes下提供的服务发现的接口，包括获取服务列表和服务实例列表。\n1func (c *Controller) GetService(hostname model.Hostname) (*model.Service, error) { 2 name, namespace, err := parseHostname(hostname) 3 item, exists := c.serviceByKey(name, namespace) 4 svc := convertService(*item, c.domainSuffix) 5 return svc, nil 6 } 最终是从infromer的缓存中获取Service资源对象。\n1func (c *Controller) serviceByKey(name, namespace string) (*v1.Service, bool) { 2 item, exists, err := c.services.informer.GetStore().GetByKey(KeyFunc(name, namespace)) 3 return item.(*v1.Service), true 4} 获取服务实例列表也是类似，也是从Informer的缓存中获取对应资源，只是涉及的对象和处理过程比Service要复杂一些。\n1func (c *Controller) InstancesByPort(hostname model.Hostname, reqSvcPort int, 2 labelsList model.LabelsCollection) ([]*model.ServiceInstance, error) { 3 // Get actual service by name 4 name, namespace, err := parseHostname(hostname) 5 item, exists := c.serviceByKey(name, namespace) 6 svc := convertService(*item, c.domainSuffix) 7 svcPortEntry, exists := svc.Ports.GetByPort(reqSvcPort) 8 for _, item := range c.endpoints.informer.GetStore().List() { 9 ep := *item.(*v1.Endpoints) 10 … 11 } 12 … } 13 } 14return nil, nil 15 } 可以看到就是做了如下的转换,将Kubernetes的对一个服务发现的数据结构转换成Istio的抽象模型对应的数据结构。\n其实在conversion.go中提供了多个convert的方法将Kubernetes的数据对象转换成Istio的标准格式。除了上面的对Service、Instance的convert外，还包含对port，label、protocol的convert。如下面protocol的convert就值得一看。\n1func ConvertProtocol(name string, proto v1.Protocol) model.Protocol { 2 out := model.ProtocolTCP 3 switch proto { 4 case v1.ProtocolUDP: 5 out = model.ProtocolUDP 6 case v1.ProtocolTCP: 7 prefix := name 8 i := strings.Index(name, \u0026#34;-\u0026#34;) 9 if i \u0026amp;gt;= 0 { 10 prefix = name[:i] 11 } 12 protocol := model.ParseProtocol(prefix) 13 if protocol != model.ProtocolUDP \u0026amp;\u0026amp; protocol != model.ProtocolUnsupported { 14 out = protocol 15 } 16 } 17 return out 18 } 看过Istio文档的都知道在使用Istio和Kuberntes结合的场景下创建Pod时要求满足4个约束。其中重要的一个是Port必须要有名，且Port的名字名字的格式有严格要求：Service 的端口必须命名，且端口的名字必须满足格式 \u0026lt;protocol\u0026gt;[-\u0026lt;suffix\u0026gt;]，例如name: http2-foo 。在K8s场景下这部分我们一般可以不对Pod命名的，看这段解析的代码可以看服务的Protocol是从name中解析出来的。如果Service的protocol是UDP的，则协议UDP；如果是TCP的，则会从名字中继续解析协议。如果名称是不可识别的前缀或者端口上的流量就会作为普通的 TCP 流量来处理。\n另外同时在Informer 中添加对add、delete、和update事件的回调，分别对应 informer 监听到创建、更新和删除这三种事件类型。可以看到这里是将待执行的回调操作包装成一个task，再压到Queue中，然后在Queue的run()方法中拿出去挨个执行，这部分不细看了。\n到这里Kuberntes特有的服务发现能力就介绍完了。即kube\\controller也实现了ServiceDiscovery中规定的服务发现的接口中定义的全部发方法。除了初始化了一个kube controller来从Kubeapiserver中获取和维护服务发现数据外，在pilot server初始化的时候，还有一个重要的initDiscoveryService初始化DiscoveryServer，这个discoveryserver使用contrller，其实是ServiceDiscovery上的服务发现供。发布成通用协议的接口，V1是rest，V2是gRPC,进而提供服务发现的能力给Envoy调用，这部分是Pilot服务发现的通用机制，在上篇文章的adapter机制中有详细描述，这里不再赘述。\n总结 以上介绍了istio基于Kubernetes的名字服务实现服务发现的机制和流程。整个调用关系如下，可以看到和其他的Adapter实现其实类似。\nDiscoveryserver基于Controller上维护的服务发现数据，发布成gRPC协议的服务供Envoy使用。\n前面只是提到了服务发现的数据维护，可以看到在Kubernetes场景下，Istio只是是使用了kubeAPIServer中service这种资源对象。在执行层面，说到Service就不得不说Kuberproxy，因为Service只是一个逻辑的描述，真正执行转发动作的是Kubeproxy，他运行在集群的每个节点上，把对Service的访问转发到后端pod上。在引入Istio后，将不再使用Kubeproxy做转发了，而是将这些映射关系转换成为pilot的转发模型，下发到envoy进行转发，执行更复杂的控制。这些在后面分析Discoveryserver和Sidecar的交互时再详细分析。\n在和Kubnernetes结合的场景下强调下几个概念：\nIstio的治理Service就是Kubernetes的Service。不管是服务描述的manifest还是存在于服务注册表中服务的定义都一样。 Istio治理的是服务间的通信。这里的服务并不一定是所谓的微服务，并不在乎是否做了微服务化。只要有服务间的访问，需要治理就可以用。一个大的单体服务打成一个镜像在Kuberntes里部署起来被其他负载访问和分拆成微服务后被访问，在治理看来没有任何差别。\n本文只是描述了在服务发现上两者的结合，随着分析的深入，会发现Istio和Kubernetes的更多契合。K8s编排容器服务已经成为一种事实上的标准；微服务与容器在轻量、快速部署运维等特征的匹配，微服务运行在容器中也正成为一种标准实践；随着istio的成熟和ServiceMesh技术的流行，使用Istio进行微服务治理的实践也正越来越多；而istio和k8s的这种天然融合使得上面形成了一个完美的闭环。对于云原生应用，采用kubernetes构建微服务部署和集群管理能力，采用Istio构建服务治理能力，也将成为微服务真正落地的一个最可能的途径。有幸参与其中让我们一起去见证和经历这个趋势吧。\n注：文中代码基于commit：505af9a54033c52137becca1149744b15aebd4ba\n","link":"https://idouba.com/istio-01-code-pilot-service-discovery-upon-k8s/","section":"posts","tags":["Pilot","Istio"],"title":"Istio技术与实践02：源码解析之Istio on Kubernetes 统一服务发现"},{"body":"","link":"https://idouba.com/tags/pilot/","section":"tags","tags":null,"title":"Pilot"},{"body":"前言 本文结合Pilot中的关键代码来说明下Istio的服务发现的机制、原理和流程。并以Eureka为例看下Adapter的机制如何支持多云环境下的服务发现。可以了解到： 1. Istio的服务模型; 2. Istio发现的机制和原理; 3. Istio服务发现的adpater机制。 基于以上了解可以根据需开发集成自有的服务注册表，完成服务发现的功能。\n服务模型 首先，Istio作为一个（微）服务治理的平台，和其他的微服务模型一样也提供了Service，ServiceInstance这样抽象服务模型。如Service的定义中所表达的，一个服务有一个全域名，可以有一个或多个侦听端口。\n1type Service struct { 2 // Hostname of the service, e.g. \u0026#34;catalog.mystore.com\u0026#34; 3 Hostname Hostname `json:\u0026#34;hostname\u0026#34;` 4 Address string `json:\u0026#34;address,omitempty\u0026#34;` 5 Addresses map[string]string `json:\u0026#34;addresses,omitempty\u0026#34;` 6 // Ports is the set of network ports where the service is listening for connections 7 Ports PortList `json:\u0026#34;ports,omitempty\u0026#34;` 8 ExternalName Hostname `json:\u0026#34;external\u0026#34;` 9 ... 10 } 当然这里的Service不只是mesh里定义的service，还可以是通过serviceEntry接入的外部服务。每个port的定义在这里：\n1type Port struct { 2 Name string `json:\u0026#34;name,omitempty\u0026#34;` 3 Port int `json:\u0026#34;port\u0026#34;` 4 Protocol Protocol `json:\u0026#34;protocol,omitempty\u0026#34;` 5 } 除了port外，还有 一个name和protocol。可以看到支持如下几个Protocol ：\n1const ( 2 ProtocolGRPC Protocol = \u0026#34;GRPC\u0026#34; 3 ProtocolHTTPS Protocol = \u0026#34;HTTPS\u0026#34; 4 ProtocolHTTP2 Protocol = \u0026#34;HTTP2\u0026#34; 5 ProtocolHTTP Protocol = \u0026#34;HTTP\u0026#34; 6 ProtocolTCP Protocol = \u0026#34;TCP\u0026#34; 7 ProtocolUDP Protocol = \u0026#34;UDP\u0026#34; 8 ProtocolMongo Protocol = \u0026#34;Mongo\u0026#34; 9 ProtocolRedis Protocol = \u0026#34;Redis\u0026#34; 10 ProtocolUnsupported Protocol = \u0026#34;UnsupportedProtocol\u0026#34; 11 ) 每个服务实例ServiceInstance的定义如下\n1type ServiceInstance struct { 2 Endpoint NetworkEndpoint `json:\u0026#34;endpoint,omitempty\u0026#34;` 3 Service *Service `json:\u0026#34;service,omitempty\u0026#34;` 4 Labels Labels `json:\u0026#34;labels,omitempty\u0026#34;` 5 AvailabilityZone string `json:\u0026#34;az,omitempty\u0026#34;` 6 ServiceAccount string `json:\u0026#34;serviceaccount,omitempty\u0026#34;` 7 } 熟悉SpringCloud的朋友对比下SpringCloud中对应interface，可以看到主要字段基本完全一样。\n1public interface ServiceInstance { 2 String getServiceId(); 3 String getHost(); 4 int getPort(); 5 boolean isSecure(); 6 URI getUri(); 7 Map\u0026amp;lt;String, String\u0026amp;gt; getMetadata(); 8 } 以上的服务定义的代码分析，结合官方spec可以非常清楚的定义了服务发现的数据模型。但是，Istio本身没有提供服务发现注册和服务发现的能力，翻遍代码目录也找不到一个存储服务注册表的服务。Discovery部分的文档是这样来描述的：\n对于服务注册，Istio认为已经存在一个服务注册表来维护应用程序的服务实例(Pod、VM)，包括服务实例会自动注册这个服务注册表上；不健康的实例从目录中删除。而服务发现的功能是Pilot提供了通用的服务发现接口，供数据面调用动态更新实例。\n即Istio本身不提供服务发现能力，而是提供了一种adapter的机制来适配各种不同的平台。\n多平台支持的Adpater机制 具体讲，Istio的服务发现在Pilot中完成，通过以下框图可以看到，Pilot提供了一种平台Adapter，可以对接多种不同的平台获取服务注册信息，并转换成Istio通用的抽象模型。\n\u0015\r从pilot的代码目录也可以清楚看到，至少支持consul、k8s、eureka、cloudfoundry等平台。\n服务发现的主要行为定义 服务发现的几重要方法方法和前面看到的Service的抽象模型一起定义在service中。，可以认为是Istio服务发现的几个主要行为。\n1// ServiceDiscovery enumerates Istio service instances. 2 type ServiceDiscovery interface { 3 // 服务列表 4 Services() ([]*Service, error) 5 // 根据域名的得到服务 6 GetService(hostname Hostname) (*Service, error) 7 // 被InstancesByPort代替 8 Instances(hostname Hostname, ports []string, labels LabelsCollection) ([]*ServiceInstance, error) 9 //根据端口和标签检索服务实例，最重要的以方法。 10 InstancesByPort(hostname Hostname, servicePort int, labels LabelsCollection) ([]*ServiceInstance, error) 11 //根据proxy查询服务实例，如果是sidecar和pod装在一起，则返回该服务实例，如果只是装了sidecar，类似gateway，则返回空 12 GetProxyServiceInstances(*Proxy) ([]*ServiceInstance, error) 13 ManagementPorts(addr string) PortList 14 } 下面选择其中最简单也可能是大家最熟悉的Eureka的实现来看下这个adapter机制的工作过程\n主要流程分析 1. 服务发现服务入口 Pilot有三个独立的服务分别是agent，discovery和sidecar-injector。分别提供sidecar的管理，服务发现和策略管理，sidecar自动注入的功能。Discovery的入口都是pilot的pilot-discovery。 在service初始化时候，初始化ServiceController 和 DiscoveryService。\n1if err := s.initServiceControllers(\u0026amp;args); err != nil { 2 return nil, err 3 } 4 if err := s.initDiscoveryService(\u0026amp;args); err != nil { 5 return nil, err 6 } 前者是构造一个controller来构造服务发现数据，后者是提供一个DiscoveryService，发布服务发现数据，后面的分析可以看到这个DiscoveryService向Envoy提供的服务发现数据正是来自Controller构造的数据。我们分开来看。\n2. Controller对接不同平台维护服务发现数据 首先看Controller。在initServiceControllers根据不同的registry类型构造不同的conteroller实现。如对于Eureka的注册类型，构造了一个Eurkea的controller。\n1case serviceregistry.EurekaRegistry: 2 eurekaClient := eureka.NewClient(args.Service.Eureka.ServerURL) 3 serviceControllers.AddRegistry( 4 aggregate.Registry{ 5 Name: serviceregistry.ServiceRegistry(r), 6 ClusterID: string(serviceregistry.EurekaRegistry), 7 Controller: eureka.NewController(eurekaClient, args.Service.Eureka.Interval), 8 ServiceDiscovery: eureka.NewServiceDiscovery(eurekaClient), 9 ServiceAccounts: eureka.NewServiceAccounts(), 10 }) 可以看到controller里包装了Eureka的client作为句柄，不难猜到服务发现的逻辑正式这个client连Eureka的名字服务的server获取到。\n1func NewController(client Client, interval time.Duration) model.Controller { 2 return \u0026amp;controller{ 3 interval: interval, 4 serviceHandlers: make([]serviceHandler, 0), 5 instanceHandlers: make([]instanceHandler, 0), 6 client: client, 7 } 8 } 可以看到就是使用EurekaClient去连EurekaServer去获取服务发现数据，然后转换成Istio通用的Service和ServiceInstance的数据结构。分别要转换convertServices convertServiceInstances, convertPorts, convertProtocol等。\n1// InstancesByPort implements a service catalog operation 2 func (sd *serviceDiscovery) InstancesByPort(hostname model.Hostname, port int, 3 tagsList model.LabelsCollection) ([]*model.ServiceInstance, error) { 4 apps, err := sd.client.Applications() 5services := convertServices(apps, map[model.Hostname]bool{hostname: true}) 6 7out := make([]*model.ServiceInstance, 0) 8for _, instance := range convertServiceInstances(services, apps) { 9 out = append(out, instance) 10} 11return out, nil 12} Eureka client或服务发现数据看一眼，其实就是通过Rest方式访问/eureka/v2/apps连Eureka集群来获取服务实例的列表。\n1func (c *client) Applications() ([]*application, error) { 2 req, err := http.NewRequest(\u0026#34;GET\u0026#34;, c.url+appsPath, nil) 3 req.Header.Set(\u0026#34;Accept\u0026#34;, \u0026#34;application/json\u0026#34;) 4 resp, err := c.client.Do(req) 5 data, err := ioutil.ReadAll(resp.Body) 6 var apps getApplications 7 if err = json.Unmarshal(data, \u0026amp;apps); err != nil { 8 return nil, err 9 } 10return apps.Applications.Applications, nil 11} Application是本地对Instinstance对象的包装。\n1type application struct { 2 Name string `json:\u0026#34;name\u0026#34;` 3 Instances []*instance `json:\u0026#34;instance\u0026#34;` 4 } 又看到了eureka熟悉的ServiceInstance的定义。当年有个同志提到一个方案是往metadata这个map里塞租户信息，在eureka上做多租。\n1type instance struct { // nolint: maligned 2 Hostname string `json:\u0026#34;hostName\u0026#34;` 3 IPAddress string `json:\u0026#34;ipAddr\u0026#34;` 4 Status string `json:\u0026#34;status\u0026#34;` 5 Port port `json:\u0026#34;port\u0026#34;` 6 SecurePort port `json:\u0026#34;securePort\u0026#34;` 7 Metadata metadata `json:\u0026#34;metadata,omitempty\u0026#34;` 8 } 以上我们就看完了服务发现数据生成的过程。对接名字服务的服务发现接口，获取数据，转换成Istio抽象模型中定义的标准格式。下面看下这些服务发现数据怎么提供出去被Envoy使用的。\n3. DiscoveryService 发布服务发现数据 在pilot server初始化的时候，除了前面初始化了一个controller外，还有一个重要的initDiscoveryService初始化Discoveryservice\n1environment := model.Environment{ 2 Mesh: s.mesh, 3 IstioConfigStore: model.MakeIstioStore(s.configController), 4 ServiceDiscovery: s.ServiceController, 5 .. 6 } 7 … 8 s.EnvoyXdsServer = envoyv2.NewDiscoveryServer(environment, v1alpha3.NewConfigGenerator(registry.NewPlugins())) 9 s.EnvoyXdsServer.Register(s.GRPCServer) 10 .. 即构造gRPC server提供了对外的服务发现接口。DiscoveryServer定义如下\n1//Pilot支持Evnoy V2的xds的API 2 type DiscoveryServer struct { 3 // env is the model environment. 4 env model.Environment 5 ConfigGenerator *v1alpha3.ConfigGeneratorImpl 6 modelMutex sync.RWMutex 7 services []*model.Service 8 virtualServices []*networking.VirtualService 9 virtualServiceConfigs []model.Config 10 } 即提供了这个grpc的服务发现Server，sidecar通过这个server获取服务发现的数据，而server使用到的各个服务发现的功能通过Environment中的ServiceDiscovery句柄来完成.从前面environment的构造可以看到这个ServiceDiscovery正是上一个init构造的controller。\n1// Environment provides an aggregate environmental API for Pilot 2 type Environment struct { 3 // Discovery interface for listing services and instances. 4 ServiceDiscovery DiscoveryServer在如下文件中开发了对应的接口，即所谓的XDS API，可以看到这些API都定义在envoyproxy/go-control-plane/envoy/service/discovery/v2 下面，即对应数据面服务发现的标准API。Pilot和很Envoy这套API的通信方式，包括接口定义我们在后面详细展开。\n这样几个功能组件的交互会是这个样子:\nController使用EurekaClient来获取服务列表，提供转换后的标准的服务发现接口和数据结构； Discoveryserver基于Controller上维护的服务发现数据，发布成gRPC协议的服务供Envoy使用。 非常不幸的是，码完这篇文字码完的时候，收到社区里merge了这个PR ：因为Eureka v2.0 has been discontinued，Istio服务发现里removed eureka adapter 。即1.0版本后再也看不到Istio对Eureka的支持了。这里描述的例子真的就成为一个例子了。\n总结 我们以官方文档上这张经典的图来端到端的串下整个服务发现的逻辑：\nPilot中定义了Istio通用的服务发现模型，即开始分析到的几个数据结构；\nPilot使用adapter方式对接不同的(云平台的)的服务目录，提取服务注册信息；\nPilot使用将2中服务注册信息转换成1中定义的自定义的数据结构。\nPilot提供标准的服务发现接口供数据面调用。\n数据面获取服务服务发现数据，并基于这些数据更新sidecar后端的LB实例列表，进而根据相应的负载均衡策略将请求转发到对应的目标实例上。\n文中着重描述以上的通用模板流程和一般机制，很多细节忽略掉了。后续根据需要对于以上点上的重要功能会展开。如以上2和3步骤在Kubernetes中如何支持将在后面一篇文章《Istio源码分析 Istio+Kubernetes的服务发现》中重点描述，将了解到在Kubernetes环境下，Istio如何使用Pilot的服务发现的Adapter方式集成Kubernetes的Service资源，从而解决长久以来在Kunbernetes上运行微服务使用两套名字服务的尴尬局面。\n注：文中代码基于commit：505af9a54033c52137becca1149744b15aebd4ba\n","link":"https://idouba.com/istio-01-code-pilot-service-discovery-adapter/","section":"posts","tags":["Pilot","Istio"],"title":"Istio技术与实践01： 源码解析之Pilot多云平台服务发现机制"},{"body":"","link":"https://idouba.com/tags/doudou/","section":"tags","tags":null,"title":"doudou"},{"body":"宝贝的幼儿园老师都是漂亮活泼又富有爱心的小天使一样的人物，非常神奇的教会了宝贝们很多我们家长们都搞不定的事情，非常有办法的完全不用发火的将这些淘气的小家伙们修理的服服贴贴，在小宝贝们眼里简直就是神一般的存在，当然在家长眼里也是。\n如果你以为这就是全部，那就大错特错了。就在这两天发现了，她们居然也是深谙各种计算机中的算法。不得不偷着怀疑这些白天在学校里的陪孩子们玩的小姑娘们下班后是不是在菊厂或者其他公司写代码。\n这不这两天在家长群里发了消息，就小露了一手。这个案例的背景是要收集到每个宝贝的家长对一个重要通知的确认。上级的重要通知，每个家的家长都要确认。如果你说请在群里回复“收到”那简直弱爆了。最终怎么确认每个都收到了。\n看看我们美女老师设计的算法。如右图所示，每个家长的回复一个序号+宝贝名+收到，序号是根据前面的回复次序i++上来，并且每个回复要求追加在前面一个回复的后面。\n这样：\n第一个回复是：1 苹果收到； 第二个回复是：1 苹果收到 2 桃子收到； 最后一个回复是：1 苹果收到 2 桃子收到……32 橘子收到。 同一个时间段内家长回复非常密集（我们都非常积极的支持我们老师们的工作的），那发之前就要检查下是否符合这个规则。比如douba发之前，先拷贝前面的串，发现doudou轮到10号了，那我们该回复的串就是：1苹果收到 2 桃子收到……10豆豆收到。如果在发出去前发现有个家长先于我们发到群里了：1 苹果收到 2 桃子收到……10香蕉收到，那么douba这个提交就和香蕉冲突了，需要发：1 苹果收到 2 桃子收到……10香蕉收到收到 11 豆豆收到。\n并发场景下，在更新同一个变量，先比较你读过来的值和当前值是否一致，一致表示在更新前这个值未被别人（其他线程）更新过，则允许替换；否则不一致说明别人（其他线程）已经更新过了，则不允许更新。是不是非常眼熟，不就是大名鼎鼎的的Compare and Swap的应用。\n1public boolean cas(StringBuffer targetValue, String mySegment, String myOldValue) { 2if(targetValue.equals(myOldValue)){ 3targetValue.append(mySegment); 4return true;} 5else 6{ 7return false} 8} 9} 当然这个伪代码中的set操作不是机器指令上的原子操作，但也demo了CAS的思路。算法执行读-修改-写操作，而无需害怕其他线程同时修改变量，因为如果其他线程修改变量， 会检测它并失败，算法可以对该操作重新计算，因此是一种无锁的并发算法。\n用在这个微信点名没法加锁的场景下，简单高效。设想后面老师要收集最终数据，只要检查最后的那个串上，所有的她所有的宝贝按顺序排列在上面，就像每天早上老师当队头排成长队去教室外面活动一样，保证一个都不会少。\n","link":"https://idouba.com/cas-in-kids-garden/","section":"posts","tags":["随笔","doudou"],"title":"论CAS在幼儿园点名中的应用"},{"body":"1 概述 kubernetes提供了的Probe可以进行健康检查。\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n对pod中的每个容器通过配置liveness或者readiness。\n当liveness probe failed后，该container会杀掉，并重新创建；而readinessProbe失败，则该pod ip 会从service的endpoints列表中删除，即隔离到该后端的请求。\n如liveness 配置如下：\n1livenessProbe: 2 httpGet: 3 port: 10252 4 path: “/healthz” 5 initialDelaySeconds: 15 6 timeoutSeconds: 15 文中尝试端到端的看下整个过程有哪些组件参与进来，怎么配合工作的。\n2 配置 pkg/api/types.go#Probe结构描述了Probe的一个定义。其中Handler是执行的动作，initialDelaySeconds表示容器启动后延迟多少秒初始化probe，考虑一般应用启动需要一定时间。periodSeconds 表示周期检查的间隔，默认10秒，最小1秒。timeoutSeconds会告诉健康检查等待多长时间，默认1秒，最小1秒。successThreshold表示连续探测多少次成功才算成功。failureThreshold表示连续探测多少次失败才算失败。默认是3.\n1type Probe struct { 2Handler json:\u0026#34;,inline\u0026#34; 3InitialDelaySeconds int32 json:\u0026#34;initialDelaySeconds,omitempty\u0026#34; 4TimeoutSeconds int32 json:\u0026#34;timeoutSeconds,omitempty\u0026#34; 5PeriodSeconds int32 json:\u0026#34;periodSeconds,omitempty\u0026#34; 6SuccessThreshold int32 json:\u0026#34;successThreshold,omitempty\u0026#34; 7FailureThreshold int32 json:\u0026#34;failureThreshold,omitempty\u0026#34; 8} 探测动作Handler支持httpget tcd 和exec三种动作。\nhttpGet对应一个http请求，主要是配置http端口和path；TCPSocket对应一个TCP请求，主要是配置一个TCP端口，EXEC表示执行一个命令。各个handler详细的定义不看了。\n1type Handler struct { 2Exec *ExecAction json:\u0026#34;exec,omitempty\u0026#34; 3// HTTPGet specifies the http request to perform. 4// +optional 5HTTPGet *HTTPGetAction json:\u0026#34;httpGet,omitempty\u0026#34; 6// TCPSocket specifies an action involving a TCP port. 7// TODO: implement a realistic TCP lifecycle hook 8// +optional 9TCPSocket *TCPSocketAction json:\u0026#34;tcpSocket,omitempty\u0026#34; 10} 下面看下probe的探测是怎么被执行的，探测结果又是如何处理的。\n在kubelet启动的时候根据配置定期执行配置的Probe，写状态。\npkg/kubelet/kubelet.go# NewMainKubelet\n1klet.probeManager = prober.NewManager( 2klet.statusManager, 3klet.livenessManager, 4klet.runner, 5containerRefManager, 6kubeDeps.Recorder) kubelete创建时会创建一个prober_manager. 负责pod 的probe。主要工作是：对每个容器创建一个worker，worker周期的执行探测并保存探测结果。\npkg/kubelet/prober/prober_manager.go\nkubelet的addpod的操作HandlePodAdditions会触发prober_manager的AddPod，对pod中每个容器检查是否配置了readiness或者liveness的probe，并创建启动对应的worker。\nkubelete创建时会创建一个prober_manager. 负责pod 的probe。主要工作是：对每个容器创建一个worker，worker周期的执行探测并保存探测结果。\npkg/kubelet/prober/prober_manager.go\n1func (m *manager) AddPod(pod *api.Pod) { 2for _, c := range pod.Spec.Containers { 3if c.ReadinessProbe != nil { 4key.probeType = readiness 5w := newWorker(m, readiness, pod, c) 6m.workers[key] = w 7go w.run() 8} 9if c.LivenessProbe != nil { 10key.probeType = liveness 11w := newWorker(m, liveness, pod, c) 12m.workers[key] = w 13go w.run() 14} 根据probe中配置的PeriodSeconds周期执行probe动作。\npkg/kubelet/prober/worker.go # doProbe\n1func (w *worker) doProbe() (keepGoing bool) { 2result, err := w.probeManager.prober.probe(w.probeType, w.pod, status, w.container, w.containerID, prev) 3w.resultsManager.Set(w.containerID, result, w.pod) 执行prober中的探测动作，并将探测结果保持到resultsManager中。\n忽略细节，调用过程是; pkg/kubelet/prober/prober.go#probe –\u0026gt; pkg/kubelet/prober/prober.go#runProbeWithRetries –\u0026gt; pkg/kubelet/prober/prober.go#runProbe\n根据配置的是HTTPGET、Exec还是TCPSocket执行不同的探测。\n1func (pb *prober) runProbe(p *api.Probe, pod *api.Pod, status api.PodStatus, container api.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) { 2if p.Exec != nil { 3return pb.exec.Probe(pb.newExecInContainer(container, containerID, p.Exec.Command, timeout)) 4} 5if p.HTTPGet != nil { 6return pb.http.Probe(url, headers, timeout) 7} 8if p.TCPSocket != nil { 9return pb.tcp.Probe(status.PodIP, port, timeout) 10} 11}\u0026lt; 分别对应pkg/probe/http/http.go，pkg/probe/tcp/tcp.go，pkg/probe/exec/exec.go 三种探测方式。各种方式也就是检查执行结果是否有错。\nTcp探测看连接后是否error\n1func DoTCPProbe(addr string, timeout time.Duration) (probe.Result, string, error) { 2conn, err := net.DialTimeout(“tcp”, addr, timeout) 3if err != nil { 4// Convert errors to failures to handle timeouts. 5return probe.Failure, err.Error(), nil 6} http探测看StatusCode\n1func DoHTTPProbe(url *url.URL, headers http.Header, client HTTPGetInterface) (probe.Result, string, error) { 2req, err := http.NewRequest(“GET”, url.String(), nil) 3if err != nil { 4// Convert errors into failures to catch timeouts. 5return probe.Failure, err.Error(), nil 6} 7res, err := client.Do(req) 8if res.StatusCode \u0026amp;gt;= http.StatusOK \u0026amp;\u0026amp; res.StatusCode \u0026amp;lt; http.StatusBadRequest { 9return probe.Success, body, nil 10} exec探测看执行返回码\n1func (pr execProber) Probe(e exec.Cmd) (probe.Result, string, error) { 2data, err := e.CombinedOutput() 3if err != nil { 4exit, ok := err.(exec.ExitError) 5if ok { 6if exit.ExitStatus() == 0 { 7return probe.Success, string(data), nil 8} else { 9return probe.Failure, string(data), nil 10} 11} 12return probe.Unknown, “”, err 至此探测流程很简单的看完了，关注下探测的结果的保存。\n在prober_manager中，根据不同的探测类型，构造不同的worker，结果会保存到对应的prober_manager的readinessManager或者livenessManger中。\n4 根据状态 action pkg/kubelet/kubelet.go#NewMainKubelet\nkubelet启动时候会构造podWorker\n1klet.podWorkers = newPodWorkers(klet.syncPod, klet.syncPodWithProcess, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache, klet.processCache) syncPod是kubelet一个比较核心的方法，做了包括更新为static pod创建mirrorpod，为pod创建数据目录等操作，还有就是调用容器runtime的SyncPod的回调。\n1result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) 2kl.reasonCache.Update(pod.UID, result) pkg/kubelet/container/runtime.go\n定义了对pod的各种操作。是容器运行时需要实现的接口。\n1type Runtime interface { 2Type() string 3Status() (*RuntimeStatus, error) 4GetPods(all bool) ([]*Pod, error) 5GarbageCollect(gcPolicy ContainerGCPolicy, allSourcesReady bool) error 6SyncPod(pod *api.Pod, apiPodStatus api.PodStatus, podStatus *PodStatus, pullSecrets []api.Secret, backOff *flowcontrol.Backoff) PodSyncResult 7KillPod(pod *api.Pod, runningPod Pod, gracePeriodOverride *int64) error 8GetPodStatus(uid types.UID, name, namespace string) (*PodStatus, error) 9GetNetNS(containerID ContainerID) (string, error) 10GetPodContainerID(*Pod) (ContainerID, error) 11GetContainerLogs(pod *api.Pod, containerID ContainerID, logOptions *api.PodLogOptions, stdout, stderr io.Writer) (err error) 12DeleteContainer(containerID ContainerID) error 13UpdatePodCIDR(podCIDR string) error 14} 两种实现pkg/kubelet/dockertools/docker_manager.go 中的DockerManager 和\npkg/kubelet/rkt/rkt.go中Runtime\n如pkg/kubelet/dockertools/docker_manager.go # SyncPod\n1containerChanges, err := dm.computePodContainerChanges(pod, podStatus) 2for _, containerStatus := range runningContainerStatues { 3_, keep := containerChanges.ContainersToKeep[kubecontainer.DockerID(containerStatus.ID.ID)] 4if !keep \u0026amp;\u0026amp; !keepInit { 5if err := dm.KillContainerInPod(containerStatus.ID, podContainer, pod, killMessage, nil); } 6} pkg/kubelet/dockertools/docker_manager.go# computePodContainerChanges\n1liveness, found := dm.livenessManager.Get(containerStatus.ID) 2if !found || liveness == proberesults.Success { 3containersToKeep[containerID] = index 4continue 5} 6if pod.Spec.RestartPolicy != api.RestartPolicyNever { 7message := fmt.Sprintf(“pod %q container %q is unhealthy, it will be killed and re-created.”, format.Pod(pod), container.Name) 8glog.Info(message) 9containersToStart[index] = message 10} 从livenessmanger获取对应container的状态，状态不是Success的都会被kill掉，在这里就是不在containersToKeep和initContainersToKeep的 container都会被杀掉\npkg/kubelet/rkt/rkt.go 中的Runtime的处理也是类似，从livenessmanager中获取状态，并执行杀pod的动作。\n1liveness, found := r.livenessManager.Get(c.ID) 2if found \u0026amp;\u0026amp; liveness != proberesults.Success \u0026amp;\u0026amp; pod.Spec.RestartPolicy != api.RestartPolicyNever { 3glog.Infof(“Pod %q container %q is unhealthy, it will be killed and re-created.”, format.Pod(pod), container.Name) 4restartPod = true 5} 6delete(unidentifiedContainers, c.ID) 7} 8if restartPod { 9if err = r.KillPod(pod, runningPod, nil); } 5 总结 对于文章前probe的处理过程，加上执行的主体，整个过程应该描述如下：\nKubelet 在延迟initialDelaySeconds后以periodSeconds为周期，执行定义的探测（包括httpGet，TcpSocket或者exec），并保存探测结果。对于liveness，kubelet会读取该探测结果，杀掉对应容器。\n","link":"https://idouba.com/kubernetes-liveness-probe/","section":"posts","tags":["Kubernetes"],"title":"kubernetes liveness probe 流程"},{"body":"","link":"https://idouba.com/tags/golang/","section":"tags","tags":null,"title":"Golang"},{"body":"1 前言 希望通过本文最简单的方式向熟悉k8s的人说明白其上的federation是干什么的，如何工作的。\n关于federation，比较官方的说法是：集群联邦可以让租户/企业根据需要扩展或伸缩所需要的集群；可以让租户/企业在跨地区甚至跨地域的多个集群上部署、调度、监测管理应用。通过集群联邦，租户/企业可以在指定集群上部署应用，可以拉通私有云和公有云建立混合云(hybrid cloud)。\n如在design-proposal 中描述的federation提供了cross-cluster scheduling, cross-cluster service discovery, cross-cluster migration, cross-cluster**ing and auditing, cross-cluster load balancing。\n简单讲就一句话。能调用一个api，向操作一个k8s集群一样操作多个k8s集群。主要是拉通其下的k8s集群在上部署应用，发布服务，并且可以让其互相访问。\n那么是怎么做到的呢？熟悉了kubernetes代码和主要的工作逻辑会发现非常简单。简单看下这部分代码会就会发现federation有如下特点：\n复用了kubernets的机制 复用kubernetes的代码 扩展了kubernetes的对象（的定义和功能） 2. 架构 Federation 层的主要组件包括Federation-API Server，Controller-Manager和ETCD。根据Decoupled 的设计的目标和kubernetes 共用类库，而不是共用一个紧密的结构结构。在结构上解耦可以保证，Federation层故障，其下的每个个kubernetes集群不受影响。另外FederationAPI接口和kube-api接口完全兼容，用户可以像之前操作单个kubernetes集群一样操作联邦。\n和Kubernetes类似，用户通过kubectl或者API调用向FederationAPI server的接口创建和维护各种对象，数据对象被持久化存储在Federation的ETCD中。联邦只是维护了规划，真正干活还是在其下的各个Cluster上（现实生活中其实也总是这样，你见过在联邦的川普干过什么正经事情）。真正关键的联邦如何通过一个统一的入口来接收请求，在各个cluster上调度。具体到（代码）功能就是联邦中指令如何在cluster上被落实执行。\n联邦和其下k8s 集群的调用关系。调用细节下面描述。\n3. 主要流程 关键就在于Federation的组件Controller-manager。和K8s其他的controller作用和工作机制类似，通过watch api-server 执行动作来维护集群状态。Federation的Controller-manager的处理逻辑和kubernetes略有不同，在于它一般都要连两个API server，watch 3个API 对象\n对于每种Resource对象，都对应一个Controller，在Federation的Controller-manager启动时，启动这些Controller。\n以ConfigMap为例，ConfigMapController启动后会watch如下三类接口：\nFederation API server的Cluster接口federation/v1beta1/cluster； Federation API server的ConfigMap接口v1/configmap； Federation 管理的 N 个kubernetes cluster的Kube-API server 的ConfigMap的接口：v1/ configmap 当ConfigMapController watch到有户通过Federation API 创建（或者更新删除）一个ConfigMap，则会调用对应的每个cluster 的kube-apiserver创建（或者更新删除）对应的ConfigMap。\n当ConfigMapController Watch到有新的Cluster加入进来时，调用新的Cluster的kube-api接口创建ConfigMap。Configmap、Secret等对象都是依照以上逻辑，从上向下Sync。\n以ConfigMap 的controller为例，其他的都是遵从同一个模板流程。在NewConfigMapController场景controller时对watch 3个api。\nFederation API server的ConfigMap接口v1/configmap；\n1configmapcontroller.configmapInformerStore, configmapcontroller.configmapInformerController = cache.NewInformer( 2\u0026amp;cache.ListWatch{ 3ListFunc: func(options api.ListOptions) (pkg_runtime.Object, error) { 4versionedOptions := util.VersionizeV1ListOptions(options) 5return client.Core().ConfigMaps(api_v1.NamespaceAll).List(versionedOptions) 6}, 7WatchFunc: func(options api.ListOptions) (watch.Interface, error) { 8versionedOptions := util.VersionizeV1ListOptions(options) 9return client.Core().ConfigMaps(api_v1.NamespaceAll).Watch(versionedOptions) 10}, Federation 管理的 N 个kubernetes cluster的Kube-API server 的ConfigMap的接口：v1/ configmap\n1/ Federated informer on configmaps in members of federation. 2\u0026amp;cache.ListWatch{ 3ListFunc: func(options api.ListOptions) (pkg_runtime.Object, error) { versionedOptions := util.VersionizeV1ListOptions(options) 4return targetClient.Core().ConfigMaps(api_v1.NamespaceAll).List(versionedOptions)}, 5WatchFunc: func(options api.ListOptions) (watch.Interface, error) { 6versionedOptions := util.VersionizeV1ListOptions(options) 7return targetClient.Core().ConfigMaps(api_v1.NamespaceAll).Watch(versionedOptions) 8} Federation API server的Cluster接口：federation/v1beta1/cluster；\n1\u0026amp;cache.ListWatch{ 2ListFunc: func(options api.ListOptions) (pkg_runtime.Object, error) { 3versionedOptions := VersionizeV1ListOptions(options) 4return federationClient.Federation().Clusters().List(versionedOptions) 5}, 6WatchFunc: func(options api.ListOptions) (watch.Interface, error) { 7versionedOptions := VersionizeV1ListOptions(options) 8return federationClient.Federation().Clusters().Watch(versionedOptions) 9} 在controller的run中执行reconcile时一般启动两个Hander处理，一个是调用reconcileConfigMap处理一个特地给的configmap，另外一个是调用reconcileConfigMapsOnClusterChange，处理federation的所有configmap。\n1func (configmapcontroller *ConfigMapController) Run(stopChan \u0026amp;lt;-chan struct{}) { 2… 3configmapcontroller.configmapDeliverer.StartWithHandler(func(item *util.DelayingDelivererItem) { 4configmap := item.Value.(*types.NamespacedName) 5configmapcontroller.reconcileConfigMap(*configmap) 6}) 7configmapcontroller.clusterDeliverer.StartWithHandler(func(_ *util.DelayingDelivererItem) { 8configmapcontroller.reconcileConfigMapsOnClusterChange() 9}) 10…} federation/pkg/federation-controller/configmap/configmap_controller.go# reconcileConfigMap\n即获取每个cluster上这个configmap的是否存在，如果不存在则构造一个Add的operation，存在则构造一个Update的operation，最终会调用对应cluster的kube-api，在对应的cluster上创建。\n1func (configmapcontroller *ConfigMapController) reconcileConfigMap(configmap types.NamespacedName) { 2baseConfigMapObj, exist, err := configmapcontroller.configmapInformerStore.GetByKey(key) 3clusters, err := configmapcontroller.configmapFederatedInformer.GetReadyClusters() 4operations := make([]util.FederatedOperation, 0) 5for _, cluster := range clusters { 6clusterConfigMapObj, found, err := configmapcontroller.configmapFederatedInformer.GetTargetStore().GetByKey(cluster.Name, key) 7operations = append(operations, util.FederatedOperation{ 8Type: util.OperationTypeAdd, 9Obj: desiredConfigMap, 10ClusterName: cluster.Name, 11}) 12} 13err = configmapcontroller.federatedUpdater.UpdateWithOnError(operations, configmapcontroller.updateTimeout, 14func(op util.FederatedOperation, operror error) Operation中包括数据对象、集群名、操作动作等参数。\n1type FederatedOperation struct { 2Type FederatedOperationType 3ClusterName string 4Obj pkg_runtime.Object 5} 即会根据cluster名选择对应的Clientset，连接对应cluster的kube-apiserver上执行对应的操作。而对应的Updater操作在controller被创建的时候定义。这里分别是执行Configmap的创建更新和删除。\n1util.NewFederatedUpdater(configmapcontroller.configmapFederatedInformer, 2func(client kubeclientset.Interface, obj pkg_runtime.Object) error { 3_, err := client.Core().ConfigMaps(configmap.Namespace).Create(configmap) 4}, 5func(client kubeclientset.Interface, obj pkg_runtime.Object) error { 6_, err := client.Core().ConfigMaps(configmap.Namespace).Update(configmap) 7}, 8func(client kubeclientset.Interface, obj pkg_runtime.Object) error { 9err := client.Core().ConfigMaps(configmap.Namespace).Delete(configmap.Name, \u0026amp;api_v1.DeleteOptions{}) 10}) 而对所有configmap的检查是通过调用reconcileConfigMapsOnClusterChange\n其实就是当一个新的cluster可用时，触发所有的configmap对象的 reconciliation。其实就是加到deliver的队列中，等待在due的时间被执行。\n1func (configmapcontroller *ConfigMapController) reconcileConfigMapsOnClusterChange() { 2for _, obj := range configmapcontroller.configmapInformerStore.List() { 3configmap := obj.(*api_v1.ConfigMap) 4configmapcontroller.deliverConfigMap(types.NamespacedName{Namespace: configmap.Namespace, Name: configmap.Name}, configmapcontroller.smallDelay, false) 5} 6} 这里示例中是通过Configmap的configmap_controller来看这个过程，打开代码结构federation-controller下每个对象都有一个controller。\n打开看代码发现主要的结构都差不多。对比下secret_controller.go和configmap_controller的一个代码片段，可以看到简直是拷代码了。\n在当前的upstream上，以上逻辑都抽象成一个模板流程在Sync中。可以看到configmap、secrete等大部分对象（甚至包括Deployment）都被模板化了。\n4 关于应用部署和服务发现 Deployment（RS）和 Service作为两个比较特殊的资源对象，主要流程也是参照了上面的流程。差别在于Deployment在Federation层在应用上加了个annotation，来描述在cluster如何部署应用。在其reconcile的方法中会调用一个schedule的逻辑来解析配置的策略，进而决定在不同的cluster创建多少个实例，然后调用对应cluster的接口来创建实例。这个后面作为一个专题专门描述下规则和解析下这部分的实现。\n而Service的逻辑可能更要略微复杂一些，上面描述的controller主流程中从Federation层向下的Sync来维护状态外，还包括从其下的cluster集群向federation层同步数据的过程。这个主要是因为Federation跨集群的服务注册和服务发现需要。主要思路是在Federation的Service扩展了k8s service的定义，即在Federation层创建一个Service，则会在对应的集群里创建同样的service，每个集群的service要求有一个可以被外面访问的地址，作为在federation层service的endpoint，并且会在全局的DNS中注册对应的域名。当下面某个cluster的service的所有endpoint都不可用或该集群故障时，federation层service和域名都要被更新。 这部分后面也会专门描述下。\n","link":"https://idouba.com/kubernetes-federation-sync-accross-cluster/","section":"posts","tags":["Golang","Kubernetes"],"title":"kubernetes federation 工作机制之资源对象同步"},{"body":"吃完晚饭与淋波\u0026amp;宝强在园区周边溜达一圈，不知怎么的扯到了小偷和被盗这个悲催的话题。三个人各自讲了自己经历的这方面的糗事，居然洋洋洒洒的扯了咕咚3000步。发现无论是数量还是质量还是精彩指数都完爆那两位，而这些素材都得益于在家乡古城的那几年经历，然后晚上回来虽然很晚了还是把这些归纳整理了下。\n记忆最深刻的是在解放路上图书大厦的那次遭遇。那会儿刚从东北的铁路工地上回到家乡的这个城市，好久找不到工作，最后在和平门里一个老民居的公司里跑业务送东西。无意中撞到附近解放路上图书大厦里，后来就每天上午在这里看半天书，下午接着去跑业务。到现在能从搞工地的工程跨界到另外一个工程，也是这阵子开的头。\n和那位君子（后文中都君子，是简称，不是尊称。灵感于小时候小人书里有梁上君子的说法）的偶遇也就发生在书城下面的存包处。那天正在存包时突的感觉裤兜里有点动静，伸手一看，一个大镊子正夹手机玩外拽。有点惊慌，这可是当时唯一的家用电器啊，有点愤怒，想大喊小偷，说实话还有点害怕，对面那个肥头大耳留着板寸的君子哥面相有点凶，而且高我半头。总之，几秒钟里气氛有点奇怪。没想对面的板寸哥却却异常镇定，像个大哥哥一样的非常和蔼的的拍拍我的肩膀说“小兄弟，不好意思，搞错了，搞错了。。。”，然后就消失在大街上。留下一个原地里发愣的傻逼青年在想“搞错了”是啥意思，当时要是顺利被偷走了，就没搞错这茬了吧。后面好几年里多次坐车经过这里，往这边瞄下，总能看到那个平头哥在这里晃悠。\n另外一次是虽然没有直接交互，但却非常有意思的来了个亲密接触。这次遭遇的不是一个君子哥，而是一群。故事发生在古城隔壁的渭城。和我一样，同事刘哥在东北工地完事后，也不去后面的青藏工地上班了，在家乡开了个卖小工艺品的小店。那年考完研入学前要转档案到NPU，就到修桥的原单位处机关里办手续，被拖了好几天。机关在刘哥店所在的渭城，那几天没事就在刘哥店里帮忙，有时还拿着老俞的红宝书准备GRE翻翻。每到晚上七八点吃完晚饭的时候，是店里生意最好的时候，几平米的小店里就挤满了人，主要都是些小姑娘。店里实在太挤了，有时候就蹲在店门口的台阶上打发时间，偶尔还好翻两下红宝书。一天傍晚正蹲着，不知什么时候身边整整齐齐的又蹲了仨哥们儿，一字儿排开蹲在我的隔壁抽烟，每人手里拿着一个硬纸板撑着的一套秋衣。刘哥之前早就说过这边有股这种神秘的势力，这帮人都是拿着一个秋衣盒子做拆挡，然后还另外一只手用镊子夹手机。于是就比较好奇的偷偷瞄一眼蹲在边上的这三位仁兄，碰巧三个人也整整齐齐的从上打量我，目光尤其落在我手上的这本书上。当时心里就乐了“人家一定当我是这片新来的吧，装备不太本地化啊”。难怪人家看我是那种同行冤家的那种感觉，《天下无贼》里葛优碰上刘德华啦。我也挑衅的瞪了他们一眼，这几个哥哥会了下眼神就离开了。\n除了直接近距离直接交手这么两次，坐车骑车目击的就更多了。在南大街见过一直跟着行人走手伸到人家挎包里的；上班骑车路过含光路见过几个小朋友跟着前面的自行车跑，去伸手到骑车人斜跨的包里；在火车站还看到过一个一条腿的君子头目拄着拐杖指挥者兄弟们一起挤起点站的14路公交车的。\n当然更近距离经历的是和宿舍的两个舍友三个男人第一次逛东大街，结果那个小名华华的东北三好小伙，手机上插着耳机正听着歌时被人拿走了。真为那个下手的君子哥的手法和胆识所折服。想这个动作是多么需要技巧和胆量，从裤兜里轻轻的掏出手机，快速拔掉耳机，然后离开现场或者伪装成路人。总之这一行还真是像葛优演的梨叔说的讲究技术含量的，也是业务素质要求很高的一个行业。\n因为地理或者一些其他什么原因，在这个古城的这种行业尤其发达。在这个城市生活的人们估计很少有人能幸免不遭遇一点。除了上面热闹的几个场景，本人几轮交手下来的负向战利品是一个电脑台式机和4+辆自行车。在这里丢自行车电瓶车几乎是每个人都经历过的。印象最深刻的是最后一辆，不对，是最后两辆。这段经历在当时项目组里被广为流传：因为这个倒霉蛋连着两天丢了两辆。也就是今天丢了，重新买一辆，第二天早上又没了。汗！\n如果说在古城方几十里圆范围内随处可以碰到这些君子们不算啥的话，在他乡遇故人的感觉就非常特别了。事情那发生在南方一个城市火车站。那是一个下午，正走着路，就听到前面两个人用熟悉的乡音在交流。虽然这么远的地方碰到老乡不至于激动上去搭腔，但是多听几声乡音的冲动还是有的。但是多听了几声，发现味道就不对了。一个用带着脏字的土话教育另外一个，“咋这么笨，那个背包在后面背着还下不了手”。妈呀，您们这是千里之外拓展业务来了，一种滑稽的自豪感油然而生。真是应了牛群给冯巩说的那句“tou出亚洲，tou向世界”。还真保不齐人家是这边办事处编制的。老家竞争太激烈了，南下开拓市场了。\n当然，谁也不会抹黑自己的家乡。咱更是时时以家乡古城悠久的历史和文化而自豪。戏虐的写下这些文字也只是archive一个很有意思的主题记忆。没想到拎起来居然能扯这么多。\n很多年了，也没有那种丢了东西的伤感，也没有当时的愤怒和对这个群体的仇恨。本来想取名“古城贼影”，觉得有几分惊悚和，又有点武侠的庸俗，联系下古城的特色，觉得“古城贼文化”似乎更契合这个城市的气质和闷骚的底蕴。只是不小心又揭了我们华华伤疤。那天回到宿舍大家都很愤怒，都忙着安慰你，没好意思问个问题“到底当时发生了什么，耳机在耳朵里插着手机也能被拿走了？”\n","link":"https://idouba.com/thief-culture-of-the-city/","section":"posts","tags":["西安"],"title":"古城贼文化"},{"body":"","link":"https://idouba.com/tags/%E8%A5%BF%E5%AE%89/","section":"tags","tags":null,"title":"西安"},{"body":"","link":"https://idouba.com/tags/g20/","section":"tags","tags":null,"title":"G20"},{"body":"注：这是第一篇用手机敲出来的文字，每天下午五点半开始在医院挂那瓶红色肺炎专用药的那四个小时。\n熟悉的时间又来到这个最近开始熟悉的地方。这两天杭州g20，今天周末，临出门看了眼电视里习主席G20开幕布上的演讲。\n右手上扎的针眼太有点多，今天换左手。腾出右手来闲着想记点东西。\n这周过得和平时很不同。和很多生活在这个城市的人感觉一样，路上很空，街道人很少。连输液这边的护士都说这里现在的人都比平日少了好几成。与现实周围形成反差的是朋友圈里很热闹，名种晒图，大江南北。\n对不住大家，对不住这个盛世的气氛。这周一个小人物过得异常的。。。稀松！\n低烧了几乎整个八月，在月底了没扛住还是来到了这个地方。\n有多稀松！\n开始一天，单位里，翘班跑医院抽血拍片，幸亏和咱单位在一条街上，打车几分钟就过来了，抽血～回单位～拍片～回单位～取结果。稀松，折腾，狼狈。然后一个多礼拜里每天下班后要来这里挂水至少四个小时。\n回家睡觉早早就被隔离在小房间，虽然窃喜过终于逃脱家务了，看到那娘俩忙乱的样子，心里也着急上火还有点惭愧。因为只能戴着口罩面对儿子，他妈问儿子：“是不是都不记得你爹长啥样了？”\n当然最明显的稀和松是咱这点小身板。月底十天，减了5公斤。十天减十斤，对此良方有意者可留言咨询。\n一只病手在另一只手还在滴滴答答的时候，手机敲这么多字进来，应该不是自己玩悲情，而是想记录下感受，只有在这种地方这种感受才会有稍微饱满那么一点。能想起拍片那下被机器把人送进去的那种紧张，那会儿就想娃和娃他妈。咱这个阶段确实很微妙，已经不是咱一个人的事儿了，倒不是咱自己怕啥。\n所以立帖为证（除了欧洲杯那会儿赛前装逼的发个帖子预测下结果很少干这种事）：\n等这阵过去后，每天一奶一水果，和娃他妈坚持过半个月。每周最少在楼下的塑胶跑道上跑三次，两次吧先尝尝，这么好的设备在家门口。搞俩二手自行车，骑车上班。每天步行15000+。\n临座那个尿结石的小伙正电话给家人汇报自己的健身计划呢。这个时候都是这种心态吧，但大部分人应该是好了伤疤忘了疼。但不影响咱继续表态。\n还有，吃饭排队不看手机，关注的每场比赛只看一篇报道，被一群没有足球底蕴只会炒作卖弄粗糙文字的小编骗流量和时间早就证明是一件极其傻逼的事情。回到家里尽量少想工作上的事情，三十多岁的老码农在几天里老纠结看到的那点代码长的难看总想着怎么重写下还是挺装逼的。\n算不上死里逃生的大场面的人生感悟，天天坐在这个输液室里三四个小时还是难得大片空闲。记点东西下来总比意思贝斯和唐老师的扯淡有点意思。\n顺便记几个三八事：那个过去几天一直值班的男护士 今天没来，他总是非常细心的摸胳膊上吊水打出的红疹问痒不痒，今天的女护士扎针有点疼，应该是新人，边上那个被她扎了好几次才扎上。医院里的G20绿色通道不是摆设，今天碰到了一群执勤的女警察通过G20专用通道把她们一个同伴用轮骑推进来。G20也是保障也真辛苦了一大群人！好长时间都是晴空万里的，偏偏今天G20开幕天有点灰蒙蒙的。朋友圈里有人说是奥黑的那几辆大排量的车闹的，我觉得有道理。\n快滴完了，护士要来拔针了，好了好了。。。\n","link":"https://idouba.com/weekly-xisong-g20/","section":"posts","tags":["G20"],"title":"稀松！对不住杭州，对不住G20"},{"body":"","link":"https://idouba.com/categories/golang/","section":"categories","tags":null,"title":"Golang"},{"body":"","link":"https://idouba.com/tags/java/","section":"tags","tags":null,"title":"java"},{"body":"前面两部分分别描述了Go和java两种语言对sort的使用方式和源码实现。作为go初学者，最想做的是通过例子和源码来对新的语言有个理解。这部分就结合自己的理解整理下，可以看到，是非常主观。\n4 语言语法比较\n可以看到，两种语言的思路基本上是一样的，用户必须定义比较规则。在排序过程中都要考察集合长度，使用用户定义的比较规则，然后调整元素位置来达到排序的效果，对应go的interface要求的三个方法。但是还是有挺多不同。\n首先从使用方式上看，go的规则（通过方法来体现）定义在集合上，并且定义了三个方法，分别获取集合的状况，元素比较的规则，元素位置操作的方法；而java的规则定义在元素上，用户只用一个元素比较的规则compareTo就够了。看起来java要求用户定义少，因为其封装的多，这也是其一贯的风格。对用户要似乎更友好一点。比较而言go更直接，简单。留给程序员能干预的更多一点。但还是觉得Len和Swap方法留给用户定义的意义不是很大，就像源码中自带的好几个例子，Len除了返回集合长度， Swap(i,j)除了{ a[i], a[j] = a[j], a[i] }，真想不好我们还能赋予它其他的使用方法。\n从package的组织，能看到Go中更面向功能（或者说行为），功能更简单直接，而java更是面向对象组织的，封装更多。看到go sort的package下面的内容如图，而java一个Collections下面这么多功能方法，而看所在的的uitl包下上内容（只能截一屏还有一大半显示不全）。\n看到了java的uitl包，下面除了Collections和Arrays这样几个工具类外更多的是List、Map、Set这样的数据结构定义（通过接口），以及常用的ArrayList，LinedList，HashMap，HashTable，TreeSet这样的实现。同样的这样键值对的容器，即有HashMap这样非同步的，又有Hashtable这样同步的，同步个容器上嫌Hashtable上加一把锁影响并发性能，后来大师Doug Lea，又发明了ConcurrentMap接口，以及现在已成经典的实现ConcurrentHashMap，在一个容器上分开加多把锁。\n而比较而言，go的结构就简单很多，语言内置容器数组和map提供了几乎所有我们要的功能，并且其操作非常简单，在源码中费劲的想找其他基于其上的结构居然没有找见。Go的简的风格在此体现的真是明显，只提供基础的数据结构，和对这些结构的好用的操作。稍微用了下切片操作真的非常赞，非要对照下和java的ArrayList挺像，但是[:]这样操作起来真的很简单直接。另外看sort源码时发现两个语言在各自package中，和sort 相伴的也都是search之类的操作。\n当然，看例子中还有源码中两个语言的语法差别那就更多了。尤其要称赞下go的几个非常酷的语法。如快排中找中间点的方法\n1func doPivot(data Interface, lo, hi int) (midlo, midhi int){.. 2return lo + b - a, hi - (d - c) 3} 居然可以返回多个返回值，不用再像原来那样，要么一个返回值作为参数传进来，另外一个从return上取，要么就不得不构造一个结构来做组织多个返回值，费劲又别扭。真是解决程序员的真正问题。\n还有，这个swap的方法实现，\n1func (a ByAge) Swap(i, j int) { a[i], a[j] = a[j], a[i] } 这样一句就解决问题。而不是java这样费劲。\n一下就把我大java比下去了。有没有想到很多年前，我们用c来数据结构的时候，老师反复强调，这时候要声明一个局部变量temp类做数据交换，非常神秘非常高级的知识点哈。在go这里居然真的可以直接交换。看上去有点像SQL这样的非过程语言了，又有点像我们描述代码功能的伪代码，只要告诉我你想干啥描述清楚，过程的能省就省了。非常非常简单，就这一小点，让咱这个新人产生了无数好感。\n1private static void swap(Object[] x, int a, int b) { 2Object t = x[a]; 3x[a] = x[b]; 4x[b] = t; 5} 当然还有左大括号的使用，声明未使用的变量编译报错等新语法，看着蛮横，但却是简单。有一点点不习惯的是居然用方法名首字母的大小写来区分方法的访问权限，不像java中public protected private这样关键字来修饰，但说实话，写大写字母大头的方法还是有点瞅着怪怪的，老想给改成小写，虽然都是所谓的驼峰式的命名。\n语法上最有感触的还是interface上的差别，在怀疑go是不是在刻意淡化interface或者压根儿人家就不承认那是你们原来认识的interface。\n除了语法上像例子中描述的必须要implements一个接口，然实现接口中规定的方法。在java中我们一般会写一个interface，然后在框架中使用这个interface，来调用interface中定义的方法，模板一样的构造业务逻辑，后面的子类只要实现了这个接口，就可以走这个模板里的流程。几乎所有号称framework的牛叉框架都是这么干的，尤其像spring struts这些已经在深入人心了，所有搞web编程的小朋友们不管理解不理解，首先就会配置**.xml 在里面写**.Action之类，然后在自己的代码里implements一下这个Action的interace（或者中间再搞一层抽象类），就可以来堆积自己的业务（在简历上就可以写熟练掌握**框架）。\n在我们的项目中确实也经常写一堆的Abstract Class 和Interface，定义这些顶层对象的行为，以一定抽象和设计的角度来编码这些顶层对象在整个业务中的角色地位和使用。这也体现了一种约束，不只是代码上的约束，也是开发过程的一种约束，一种契约和协议，项目中其他角色的人这部分实现必须要实现这些接口，也必须要按照规定的行为来实现。多符合我们项目开发过程中，老大制定规则，小弟负责遵守实施的思路。把文档文档中的设计变成interface，实现不好就编译错，这个约束力老大们多喜欢啊。\n说实话，个人是很喜欢，也很习惯这么干。但每次这么干的时候，在子类上面必须要加个implements Interface1，表示我实现了这个接口，我会实现这个接口规定的行为，比较复杂的类往往会实现多个接口。如Hadoop的JobTracker这样的重要类，真是越重要的角色越要具备各种能力，更要有担当。很像我们项目中的所谓矩阵式管理，听多个领导的安排。 每次要听命一个新领导，就不得不在声明中把新领导implements在后面。\n曾经有思考过，又没没有可能，不用这么侵入式的来做，我实现我的行为，我满足了哪个领导的规定，我就是哪个领导的兵。我只管实现自己的方法，不用在class头上必须implements。Go居然做到了，感觉在go中是先构建对象，然后再归纳出接口，而java中是先定义好接口，再让对象去实现。看上去约束少了，所以go要自然洒脱很多。\n实践中这种先有接口还是现有实现其实也区分不清楚的，如这个例子中，实际上go和java的思路是没有差别的，你让我排序的东西起码得有个排序的规则（go的Less，java的copareTo），我才能有办法给你排个序，如例子中的按年龄还是按姓名。虽然初次见识，还是被其这样简洁的特征却征服。感觉就是你有这样的行为我就让你做这样的事情，不用大声说出来！\n如例子中，把ByeAge把Len方法注释掉，编译同样能检查出编译错ByAge does not implement sort.Interface (missing Len method)，而不是我在这个实验前想的，编译不报错，运行报错。\n整体看下来。Go无处不在秀气简洁之道，包的组成，结构的定义，看源码涉及的代码行，大致也能看出来。当然也不会为了夸强调go的简洁，就使劲埋汰java。个人觉得，与语言的发展阶段可能有关系。这里用的java的源码为了简单期间是jdk1.6版本，能看到用的是mergesort，在新的jdk1.8中改为引入一个TimSort的排序方法的 TimSort类，原来的mergesort现在重命名为legacyMergeSort，并且声明在未来会去掉。\nGo作为一个比较新的语言，简单是其风格。但相信随着发展，也会逐步扩展些功能出来。）但就这接触这几天，有些地方还是比较肤浅的要吐槽下，比如上面看到sort.go的源码中，似乎真的再使劲淡化Interface吗？也太低调了吧，在子类实现的时候不用implements提名下也就算了，这里这么重要的的一个Interface居然连个像样的名字都没有，藏在package sort中，取名就叫Interface，这名字还不如没名字呢。哪像我大java中对应的接口名曰Comparable接口这么响亮霸气，最主要是能告诉人家我是什么东西。\n还有，看源码中最核心的快速排序方法：func quickSort(data Interface, a, b, maxDepth int)。方法声明上怎么还a、b都上来啦，也太信手啦，最起码也得start，end这样的吧。尤其看到方法中递归调用quickSort时候，a，b两个最重要的变量像小蝌蚪一样游弋在这段核心代码段中\n1quickSort(data, a, mlo, maxDepth) 2a = mhi // i.e., quickSort(data, mhi, b) 3} else { 4quickSort(data, mhi, b, maxDepth) 5b = mlo // i.e., quickSort(data, a, mlo) 6} 作者是觉得是a、b是洒脱，还是孤独。呵呵，也是说到这里，调侃下。\n5 后记 总的，这几天接触下来的感受，似乎能体会到语言的作者应该是一个老程序员吧，起码是非常了解程序员的大师。很多细节都是在为程序员考虑，对程序员非常友好。 基于为程序员提供一个更方便的语言，把挺多咱们这种老（其实不老哈）程序员以前只是敢想想嚷嚷两下的东西真的给做出来了。非常酷，非常NB，非常惊艳。上手非常容易，但是稍微思考下，会发现其设计非常讲究，尤其挺多和其他习惯用的语言比较打破比较多的开始会不习惯，慢慢也能体会到作者的良苦用心。希望在慢慢使用中，慢慢体会。\n尤其是读到一点源码的时候，以前就有这样的体会，读这个语言相关源码的时候，能感受到一些文档中表达不到的东西，有些我们看了源码会比文档理解更深刻；有些我们会发现人传人的所谓官方文档吹的很NB，代码中也就这样，换咱做不了那么高明，大致也能做个差不多的样，而有些时候冷不丁看到有些源码中出现类似变量声明未使用（这个在go中居然要编译报错了，彪悍！），变量类型使用错、明显逻辑错、循环不正确、几个相关包中方法各自重复定义，也会调侃大师代码覆盖测试不够，不得不感叹大师也不是神，也会犯咱们每天都在犯的小错。不管是其上的那种体会，都会顿时觉得和作者亲近很多。就像以前念书的时候，老师说的读一篇巨著能感受大师的思考，就像在和大师跨时空对话一样。\n总体来说，感觉很好。包括sublime很轻便，代码补全提示都很不错，用着很舒服，写代码效率很高。另外，API可以联网看，居然还可以在自己机器上起个godocserver，而且自己gopath里面的代码和注释都会被load进来，虽然想想实现也不难，但真的是非常酷。访问package，非常方便。\n当然啰嗦这么多，也只是一个新手个人零碎的一点体会。流水账一样的列举了一些体会，其中观点个人背景和个人主观色彩非常明显。不是一个面向主题的有深度的“深入论述java和go若干特性。。。”之类的专题文档。很多还是停留在体会，如go的interface，好像能感觉到其设计，想描述，但又说不完整，原因还是实践不够，体会不深，理解可能会有偏差。\n","link":"https://idouba.com/compare-go-and-java-style-by-sort-implement-3/","section":"posts","tags":["Golang","Java"],"title":"Sort源码实现比较Go\u0026Java语言风格(3)"},{"body":"接上篇博文，工作中用到了go排序的功能，作为go新手，在参照着例子，并读了go的sort.go部分涉及的源码，了解了go中排序的细节和实现算法，这些在上篇博文中有介绍。作为一个java ZHONGDU*2的用户，不由自主的想到了java中对应实现的样子，在这里也非常简要的贴出来，描述下java中排序的用法，和java源码中对应部分的实现，比较好奇java是和go一样，也用的时候快速排序吗？\n3 Java 排序源码解析 主要代码Looks like this： 3.1 使用例子\n1public class Person implements Comparable\u0026amp;lt;Person\u0026amp;gt; { 2private String name; 3private int age; 4 5 6@Override 7public int compareTo(Person o) { 8return this.age - o.age; 9} 10 11public static void main(String args[]) 12{ 13List\u0026amp;lt;Person\u0026amp;gt; persons = new ArrayList\u0026amp;lt;Person\u0026amp;gt;(); 14Collections.sort(persons); 15} 16} 和go中不同，必须要在class的第一行implements Comparable这个接口，然后在实现这个接口中定义的compareTo方法，告诉接口到底元素是怎么比较大小的。于是这也追踪下Collections.sort()方法中是如何使用这个compareTo规则来进行排序的。\n3.2 源码解析 Collections是一个工具类，调用的是另外一个工具类Arrays中提供的静态方法。java中很少有class名用复数的，和Collection这也一个单数表达的是interface不同，这两个有悠久历史的类下面提供了很多static的工具方法。\n1public static \u0026amp;lt;T extends Comparable\u0026amp;lt;? super T\u0026amp;gt;\u0026amp;gt; void sort(List\u0026amp;lt;T\u0026amp;gt; list) { 2Object[] a = list.toArray(); 3Arrays.sort(a); 4ListIterator\u0026amp;lt;T\u0026amp;gt; i = list.listIterator(); 5for (int j=0; j\u0026amp;lt;a.length; j++) { 6i.next(); 7i.set((T)a[j]); 8} 9} 看到Arrays中其实调用的是归并排序，clone一个是待排序的数组，排序的结果放在原来的数组上。\n1public static void sort(Object[] a) { 2Object[] aux = (Object[])a.clone(); 3mergeSort(aux, a, 0, a.length, 0); 4} 再看mergesort的实现，也是看少于一个阈值INSERTIONSORT_THRESHOLD = 7则直接进行插入排序（为什么和go一样阈值都是7！）。否则就是递归的分而治之的的从中间把待排序的数据二分成两部分，对每部分进行归并排序，直到一部分里面数据时有序为止。然后对有序的子集进行merge，最终达到大的集合整体有序。是我们想象中的归并排序 。\n1/** 2* Src is the source array that starts at index 0 3* Dest is the (possibly larger) array destination with a possible offset 4* low is the index in dest to start sorting 5* high is the end index in dest to end sorting 6* off is the offset to generate corresponding low, high in src 7*/ 8private static void mergeSort(Object[] src, 9Object[] dest, 10int low, 11int high, 12int off) { 13int length = high - low; 14 15 16// Insertion sort on smallest arrays 17if (length \u0026amp;lt; INSERTIONSORT_THRESHOLD) { 18for (int i=low; i\u0026amp;lt;high; i++) 19for (int j=i; j\u0026amp;gt;low \u0026amp;\u0026amp; 20((Comparable) dest[j-1]).compareTo(dest[j])\u0026amp;gt;0; j--) 21swap(dest, j, j-1); 22return; 23} 24 25// Recursively sort halves of dest into src 26int destLow = low; 27int destHigh = high; 28low += off; 29high += off; 30int mid = (low + high) \u0026amp;gt;\u0026amp;gt;\u0026amp;gt; 1; 31mergeSort(dest, src, low, mid, -off); 32mergeSort(dest, src, mid, high, -off); 33 34// If list is already sorted, just copy from src to dest. This is an 35// optimization that results in faster sorts for nearly ordered lists. 36if (((Comparable)src[mid-1]).compareTo(src[mid]) \u0026amp;lt;= 0) { 37System.arraycopy(src, low, dest, destLow, length); 38return; 39} 40 41// Merge sorted halves (now in src) into dest 42for(int i = destLow, p = low, q = mid; i \u0026amp;lt; destHigh; i++) { 43if (q \u0026amp;gt;= high || p \u0026amp;lt; mid \u0026amp;\u0026amp; ((Comparable)src[p]).compareTo(src[q])\u0026amp;lt;=0) 44dest[i] = src[p++]; 45else 46dest[i] = src[q++]; 47} 48} 交互两个元素位置的方法不出所料长得是这个样子。\n1/** 2* Swaps x[a] with x[b]. 3*/ 4private static void swap(Object[] x, int a, int b) { 5Object t = x[a]; 6x[a] = x[b]; 7x[b] = t; 8} 看两者的源码实现，第一印象，从使用的排序算法看，两者都采用了总体时间复杂度较好的算法，复杂度一般都是n*logn。go使用的是快排 ，尽管不是一个典型快排，用的是快排的思路。java使用的是归并排序 。算法决定了go中的排序不是staable，而java中采用的是stable的 。\n","link":"https://idouba.com/compare-go-and-java-style-by-sort-implement-2/","section":"posts","tags":["Golang","Java"],"title":"Sort源码实现比较Go\u0026Java语言风格(2)"},{"body":"1 前言 刚开始拥抱go，非常新鲜！才使用没多久，属于没有经验，有一点感受的那种。具体也说不了特别清楚，就是觉得：简单，直接，灵活，方便。作为一个 java 重度中毒（ZHANGDU*2）用户，过程中还是习惯对照着思考，至少在这个阶段。也好，发现对照着想，似乎更容易融会贯通。 对资深的go程序员来说，应该都是非常基础基本的问题，但也挡不住咱这个小白要发表下感想。\n第一篇文章首先结合最近做一个小feature时用到go中元素排序的功能，顺便了解下两种语言中排序功能的使用方式，各自源码中对排序功能的实现。当然最主要的是在这个过程中，作为go初学者对语言的体会和理解。\n2 Go排序源码解析 现在一般不太会自己写排序算法的实现了，就去搜go的package， 如愿找到了package sort?，和猜想的接口差不多，有一个func Sort(data Interface) 。只要把待排序的对象传到一个sort方法中就可以了。\n只是对这个Interface?对象还有点疑惑，这个Interface是个啥东西呀？带着这个疑问，瞄见了package前面这个这个example（裁剪了部分非关键代码）。\n2.1 go排序例子 1type Person struct { 2Name string 3Age int 4} 5// ByAge implements sort.Interface for []Person based on the Age field. 6type ByAge []Person 7func (a ByAge) Len() int { return len(a) } 8func (a ByAge) Swap(i, j int) { a[i], a[j] = a[j], a[i] } 9func (a ByAge) Less(i, j int) bool { return a[i].Age \u0026amp;lt; a[j].Age } 10func main() { 11sort.Sort(ByAge(people)) 12}type Person struct { 13Name string 14Age int 15} 16// ByAge implements sort.Interface for []Person based on the Age field. 17type ByAge []Person 18func (a ByAge) Len() int { return len(a) } 19func (a ByAge) Swap(i, j int) { a[i], a[j] = a[j], a[i] } 20func (a ByAge) Less(i, j int) bool { return a[i].Age \u0026amp;lt; a[j].Age } 21func main() { 22sort.Sort(ByAge(people)) 23} 这个例子看上去挺简单，对一个Person的对象数组ByAge，调用sort.Sort(ByAge(people))按照Age属性进行排序。在关注sort方法之前，首先留意到集合上定义了三个方法：Len、Swap、Less。为什么要在数组上要定义这三个方法，字面意思有一点点暗示我们这三个子操作是与排序有关，尤其是Less方法，与java中熟知的compareTo(T)从字面上理解是何其相似。\n2.2 go排序源码解析 Interface是何物，和这三个方法之间有什么关系，带着疑问去看下sort.go的源码。果然在文件的顶头就定义了这样一个interface（只是interface的名字叫做Interface着实刚才有点误导人了）：\n1/ A type, typically a collection, that satisfies sort.Interface can be 2// sorted by the routines in this package. The methods require that the 3// elements of the collection be enumerated by an integer index. 4type Interface interface { 5// Len is the number of elements in the collection. 6Len() int 7// Less reports whether the element with 8// index i should sort before the element with index j. 9Less(i, j int) bool 10// Swap swaps the elements with indexes i and j. 11Swap(i, j int) 12} 注释也明确的告诉我们，只有实现了这个interface的集合才可以被排序。哦，说错了，人家注释中说的不是implements，而是satisfy。似乎感觉到了的go的Interface和java的Interface的一丝丝淡淡的不同。\n但是在example的ByAge定义中，只看到了对三个方法的定义，没有对这个接口实现的声明。难道实现了这三个方法方法就自然而然的实现了这个Interface！这也也忒高级了吧！只要定义了这三个方法ByAge对象就满足了sort定义可排序的的集合要求，可以被作为sort.Sort方法的参数被使用的， sort.Sort(ByAge(people))这样被排序了。\n那看下一个对象具备一个定义了上面有三种能力，如何进行排序。也就是sort(Interface)方法中，如何使用Interface的这三个方法来完成排序的功能。\n首先看sort方法的实现。\n1// Sort sorts data. 2// It makes one call to data.Len to determine n, and O(n*log(n)) calls to 3// data.Less and data.Swap. The sort is not guaranteed to be stable. 4func Sort(data Interface) { 5// Switch to heapsort if depth of 2*ceil(lg(n+1)) is reached. 6n := data.Len() 7maxDepth := 0 8for i := n; i \u0026amp;gt; 0; i \u0026amp;gt;\u0026amp;gt;= 1 { 9maxDepth++ 10} 11maxDepth *= 2 12quickSort(data, 0, n, maxDepth) 13} 这个方法里，只是用到了取接口定义的Len的方法，来获取集合元素个数，根据元素个数估算下如果是被平衡分布的树的深度。该深度作为一个参数传给quikSort方法用，进行真正的排序。看到到这里知道go的sort用的是快排！然后看下这个快排到底是怎么做的，虽然大致都知道快排的算法是怎么回事，还是非常想知道go里面是怎么实现的。\n1func quickSort(data Interface, a, b, maxDepth int) { 2for b-a \u0026amp;gt; 7 { 3if maxDepth == 0 { 4heapSort(data, a, b) 5return 6} 7maxDepth-- 8mlo, mhi := doPivot(data, a, b) 9// Avoiding recursion on the larger subproblem guarantees 10// a stack depth of at most lg(b-a). 11if mlo-a \u0026amp;lt; b-mhi { 12quickSort(data, a, mlo, maxDepth) 13a = mhi // i.e., quickSort(data, mhi, b) 14} else { 15quickSort(data, mhi, b, maxDepth) 16b = mlo // i.e., quickSort(data, a, mlo) 17} 18} 19if b-a \u0026amp;gt; 1 { 20insertionSort(data, a, b) 21} 22} 先找好中心点，然后两边递归调用分别作快排，熟悉的思路（熟悉的配方，熟悉的味道！JDB）。但显然这个quickSort是一个改良的快排，在每次执行快排时，如果发现depth已经递减为0，而元素还很多，说明待排序的数据分布不均衡，则使用堆排序，如果子集元素少于7个，则直接进行插入排序。\n这个所谓的quickSort并不是我们数据结构中熟悉的经典快排，看上去只是一个递归的代码框架，找了中间点，然后两边递归调。并没有涉及的具体的元素操作，没有我们期待中的两边元素比较，交换位置，检查点移动这样的操作。当然也就看不到我们期望的元素比较Less方法，也没有看到元素位置调整的Swap。为了了解下实际排序的操作怎么做的，需要再往下看一点。\n堆排序方法heapSort 和插入排序方法insertionSort才是真正执行元素排序的方法，选择考察其中比较简单的insertionSort方法。\n1// Insertion sort 2func insertionSort(data Interface, a, b int) { 3for i := a + 1; i \u0026amp;lt; b; i++ { 4for j := i; j \u0026amp;gt; a \u0026amp;\u0026amp; data.Less(j, j-1); j-- { 5data.Swap(j, j-1) 6} 7} 8} 终于，我们期待的三个重要方法都看到了。Len方法负责获取集合状态（除了常规的集合长度外，这里还有根据集合长度算出的一个深度），这个集合即可以是我们原始传给Sort方法的那个集合，其实也是中间每次递归的那个小集合。Less()是负责比较元素大小的规则。而Swap方法负责实现元素位置的置换，即执行排序的动作。\n至此，结合这个例子，和一点源码理解了go中排序的使用方法。写自己的代码让待排序的集合实现以上三个方法Len、Less、Swap，调用sort.go的Sort方法，完成排序功能，顺利完成手里的feature开发。\n故事到这里本该结束了，非常不受控制的，脑子里半秒钟就闪过这个例子在java中的样子，于是就有了后面的故事。。。\n","link":"https://idouba.com/compare-go-and-java-style-by-sort-implement-1/","section":"posts","tags":["Golang","Java"],"title":"Sort源码实现比较Go\u0026Java语言风格(1)"},{"body":"","link":"https://idouba.com/tags/executor/","section":"tags","tags":null,"title":"Executor"},{"body":"","link":"https://idouba.com/categories/%E5%B9%B6%E5%8F%91/","section":"categories","tags":null,"title":"并发"},{"body":"归档下发表于infoq.com 2015年6月的两篇文章。本来是一篇文章，经过Infoq编辑Alice Ding建议，拆分为\u0026lt;上\u0026gt;和\u0026lt;下\u0026gt;两部分。谢谢Alice对文章的细心校对，帮我发下了其中的很多问题。添加下infoq要求的声明：本文为InfoQ中文站特供稿件，首发地址为：http://www.infoq.com/cn/articles/executor-framework-thread-pool-task-execution-part-02 。如需转载，请与InfoQ中文站联系。\n内容综述 基于Executor接口中将任务提交和任务执行解耦的设计，ExecutorService和其各种功能强大的实现类提供了非常简便方式来提交任务并获取任务执行结果，封装了任务执行的全部过程。本文尝试通过对j.u.c.下该部分源码的解析以ThreadPoolExecutor为例来追踪任务提交、执行、获取执行结果的整个过程。为了避免陷入枯燥的源码解释，将该过程和过程中涉及的角色与我们工作中的场景和场景中涉及的角色进行映射，力图生动和深入浅出。\n上一篇文章中通过引入的一个例子介绍了在Executor框架下，提交一个任务的过程，这个过程就像我们老大的老大要找个老大来执行一个任务那样简单。并通过剖析ExecutorService的一种经典实现ThreadPoolExecutor来分析接收任务的主要逻辑，发现ThreadPoolExecutor的工作思路和我们带项目的老大的工作思路完全一致。在本文中我们将继续后面的步骤，着重描述下任务执行的过程和任务执行结果获取的过程。会很容易发现，这个过程我们更加熟悉，因为正是每天我们工作的过程。除了ThreadPoolExecutor的内部类Worker外，对执行内容和执行结果封装的FutureTask的表现是这部分着重需要了解的。\n为了连贯期间，内容的编号延续上篇。\n2. 任务执行 其实应该说是任务被执行，任务是宾语。动宾结构：execute the task，执行任务，无论写成英文还是中文似乎都是这样。那么主语是是who呢？明显不是调用submit的那位（线程），那是哪位呢？上篇介绍ThreadPoolExecutor主要属性时提到其中有一个HashSet workers的集合，我们有说明这里存储的就是线程池的工作队列的集合，队列的对象是Worker类型的工作线程，是ThreadPoolExecutor的一个内部类，实现了Runnable接口：\n1private final class Worker implements Runnable 8)看作业线程干什么当然是看它的run方法在干什么。如我们所料，作业线程就是在一直调用getTask方法获取任务，然后调用 runTask(task)方法执行任务。看到没有，是在while循环里面，就是不干完不罢休的意思！在加班干活的苦逼的朋友们，有没有遇见战友的亲切感觉？\n1public void run() { 2 try { 3 Runnable task = firstTask; 4 //循环从线程池的任务队列获取任务 5 while (task != null || (task = getTask()) != null) { 6 //执行任务 7 runTask(task); 8 task = null; 9 } 10 } finally { 11 workerDone(this); 12 } 13 } 然后简单看下getTask和runTask(task)方法的内容。\ngetTask方法是ThreadPoolExecutor提供给其内部类Worker的的方法。作用就是一个，从任务队列中取任务，源源不断地输出任务。有没有想到老大手里拿的总是满满当当的project，也是源源不断的。 1Runnable getTask() { 2 for (;;) { 3 //从任务队列的头部取任务 4 r = workQueue.take(); 5 return r; 6 } 7 } runTask(Runnable task)是工作线程Worker真正处理拿到的每个具体任务。看到这里才可用确认我们的猜想，之前提到的“执行任务”这个动宾结构前面的主语正是这些Worker呀。唠叨了半天（看主要方法都看到了整整第10个了），前面都是派活，这里才是干活。和我们的工作何其相似！老大（LD），老大的老大（LD^2），老大的老大（LD^n） 非常辛苦，花了很多时间、精力在会议室、在project上想着怎么生成和安排任务，然而真的轮到咱哥们干活，可能花了不少时间，但看看流程就是这么简单。三个大字：“Just do it”。 1private void runTask(Runnable task) { 2 //调用任务的run方法，即在Worker线程中执行Task内定义内容。 3 task.run(); 4 } 需要注意的地方出现了，调用的其实是task的run方法。看下FutureTask的run方法做了什么事情。\n这里插入一个FutureTask的类图。可以看到FutureTask实现了RunnableFuture接口，所以FutureTask即有Runnable接口的run方法来定义任务内容，也有Future接口中定义的get、cancel等方法来控制任务执行和获取执行结果。Runnable接口自不用说，Future接口的伟大设计，就是使得实现该接口的对象可以阻塞线程直到任务执行完毕，也可以取消任务执行，检测任务是执行完毕还是被取消了。想想在之前我们使用Thread.join()或者Thread.join(long millis)等待任务结束是多么苦涩啊。\nFutureTask内部定义了一个Sync的内部类，继承自AQS，来维护任务状态。关于AQS的设计思路，可以参照参考Doug Lea大师的原著The java.util.concurrent Synchronizer Framework。\n和其他的同步工具类一样，FutureTask的主要工作内容也是委托给其定义的内部类Sync来完成。 1public void run() { 2 //调用Sync的对应方法 3 sync.innerRun(); 4 } FutureTask.Sync.innerRun()，这样做的目的就是为了维护任务执行的状态，只有当执行完后才能够获得任务执行结果。在该方法中，首先设置执行状态为RUNNING只有判断任务的状态是运行状态，才调用任务内封装的回调，并且在执行完成后设置回调的返回值到FutureTask的result变量上。在FutureTask中，innerRun等每个“写”方法都会首先修改状态位，在后续会看到innerGet等“读”方法会先判断状态，然后才能决定后续的操作是否可以继续。下图是FutureTask.Sync中几个重要状态的流转情况，和其他的同步工具类一样，状态位使用的也是父类AQS的state属性。 1void innerRun() { 2 //通过对AQS的状态位state的判断来判断任务的状态是运行状态，则调用任务内封装的回调，并且设置回调的返回值 3 if (getState() == RUNNING) 4 innerSet(callable.call()); 5 } 6 7void innerSet(V v) { 8 for (;;) { 9\tint s = getState(); 10 //设置运行状态为完成，并且把回调额执行结果设置给result变量 11\tif (compareAndSetState(s, RAN)) { 12 result = v; 13 releaseShared(0); 14 done(); 15\treturn; 16 } 17 } 至此工作线程执行Task就结束了。提交的任务是由Worker工作线程执行，正是在该线程上调用Task中定义的任务内容，即封装的Callable回调，并设置执行结果。下面就是最重要的部分：调用者如何获取执行的结果。让你加班那么久，总得把成果交出来吧。老大在等，因为老大的老大在等！\n3. 获取执行结果 前面说过，对于老大的老大这样的使用者来说，获取执行结果这个过程总是最容易的事情，只需调用FutureTask的get()方法即可。该方法是在Future接口中就定义的。get方法的作用就是等待执行结果。（Waits if necessary for the computation to complete, and then retrieves its result.）Future这个接口命名得真好，虽然是在未来，但是定义有一个get()方法，总是“可以掌控的未来，总是有收获的未来！”实现该接口的FutureTask也应该是这个意思，在未来要完成的任务，但是一样要有结果哦。\nFutureTask的get方法同样委托给Sync来执行。和该方法类似，还有一个V get(long timeout, TimeUnit unit)，可以配置超时时间。 1public V get() throws InterruptedException, ExecutionException { 2 return sync.innerGet(); 3 } 在Sync的 innerGet方法中，调用AQS父类定义的获取共享锁的方法acquireSharedInterruptibly来等待执行完成。如果执行完成了则可以继续执行后面的代码，返回result结果，否则如果还未完成，则阻塞线程等待执行完成。再大的老大要想获得结果也得等老子干完了才行！可以看到调用FutureTask的get方法，进而调用到该方法的一定是想要执行结果的线程，一般应该就是提交Task的线程，而这个任务的执行是在Worker的工作线程上，通过AQS来保证执行完毕才能获取执行结果。该方法中acquireSharedInterruptibly是AQS父类中定义的获取共享锁的方法，但是到底满足什么条件可以成功获取共享锁，这是Sync的tryAcquireShared方法内定义的。具体说来，innerIsDone用来判断是否执行完毕，如果执行完毕则向下执行，返回result即可；如果判断未完成，则调用AQS的doAcquireSharedInterruptibly来挂起当前线程，一直到满足条件。这种思路在其他的几种同步工具类Semaphore、CountDownLatch、ReentrantLock、ReentrantReadWriteLock也广泛使用。借助AQS框架，在获取锁时，先判断当前状态是否允许获取锁，若是允许则获取锁，否则获取不成功。获取不成功则会阻塞，进入阻塞队列。而释放锁时，一般会修改状态位，唤醒队列中的阻塞线程。每个同步工具类的自定义同步器都继承自AQS父类，是否可以获取锁根据同步类自身的功能要求覆盖AQS对应的try前缀方法，这些方法在AQS父类中都是只有定义没有内容。可以参照《源码剖析AQS在几个同步工具类中的使用》来详细了解。 突然想到想想那些被称为老大的，是不是整个career流程就是只干两件事情：submit a task， then wait and get the result。不对，还有一件事情，不是等待，而是催。“完了没，完了没？schedule很紧的，抓点紧啊，要不要适当加点班啊……”\n1 V innerGet() throws InterruptedException, ExecutionException { 2 //获得锁，表示执行完毕，才能获得后执行结果，否则阻塞等待执行完成再获取执行结果 3 acquireSharedInterruptibly(0); 4 return result; 5 } 6 protected int tryAcquireShared(int ignore) { 7 return innerIsDone()? 1 : -1; 8 } 至此，获得执行结果，圆满完成任务！\n老大的老大，拍着咱们老大的肩膀（或者深情的抚摸着咱们老大唏嘘胡茬的脸庞）说：“亲，你这活干的漂亮！”而隔壁桌座位的几个兄弟，刚熬了几个晚上加班交付完这波task后，发现任务队列里又有新任务了，俺们老大又从他的另外一个老大手里接来的任务了。每个人都按照这样的角色进行着，依照这样的角色安排和谐愉快地进行着。。。\n角色名 任务用户 任务管理者 任务执行者 角色属性 任务的甲方 任务的乙方 乙方的工具 角色说明 选择合适的任务执行服务，如可以根据需要选择ThreadPoolExecutor还是ScheduledThreadPoolExecutor，并定制ExecutorService的配置。 定义好任务的工作内容和结果类型，提交任务，等待任务的执行结果 接收提交的任务； 维护执行服务内部管理； 配置工作线程执行任务 每个工作线程一直从任务执行服务获取待执行的任务，保证任务完成后返回执行结果。 Executor****中对应 创建获取ExecutorService、并提交Task的外部接口 ExecutorService的各种实现。如经典的ThreadPoolExecutor，ScheduledThreadPoolExecutor 执行服务内定义的配套的Worker线程。如ThreadPoolExecutor.Worker 主要接口方法 submit(Callable task) execute(Runnable command) runTask(Runnable task) 现实角色映射 手里有活的大老大 领人干活的老大 真正干活的码农 主要工作伪代码 taskService = createService() future=taskService.submitTask() future.get() executeTask() { addTask() createThread() } while(ture) { getTask() runTask() } 四、 总结 从时序图上看主要的几个角色是这样配合完成任务提交、任务执行、获取执行结果这几个步骤的。\n外面需要提交任务的角色（如例子中老大的老大），首先创建一个任务执行服务ExecutorService，一般使用工具类Executors的若干个工厂方法 创建不同特征的线程池ThreadPoolExecutor，例子中是使用newFixedThreadPool方法创建有n个固定工作线程的线程池。 线程池是专门负责从外面接活的老大。把任务封装成一个FutureTask对象，并根据输入定义好要获得结果的类型，就可以submit任务了。 线程池就像我们团队里管人管项目的老大，各个都有一套娴熟、有效的办法来对付输入的任务和手下干活的兄弟一样，内部有一套比较完整、细致的任务管理办法，工作线程管理办法，以便应付输入的任务。这些逻辑全部在其execute方法中体现。 线程池接收输入的task，根据需要创建工作线程，启动工作线程来执行task。 工作线程在其run方法中一直循环，从线程池领取可以执行的task，调用task的run方法执行task内定义的任务。 FutureTask的run方法中调用其内部类Sync的innerRun方法来执行封装的具体任务，并把任务的执行结果返回给FutureTask的result变量。 当提及任务的角色调用FutureTask的get方法获取执行结果时，Sync的innerGet方法被调用。根据任务的执行状态判断，任务执行完毕则返回执行结果；未执行完毕则等待。 还记得我们费了半天劲试图找出任务执行时那个动宾结构的主语吗？从示例上看更像是线程池在向外提供任务执行的服务。就像我们的老大在代表我们接收任务、执行任务、提交执行结果。明显我们这些真正的Worker成了延伸，有点搞不懂到底我们是主语，还是主语延伸的工具，就像定义ThreadPoolExecutor的内部类Worker一样。我们只是工具，不是主语，是状语： execute the task by workers。突然想到毛主席当年的“数风流人物，还看今朝”，说的应该是这些Worker的劳苦大众吧，怎么都今朝这么久了，俺们这些Woker们还是风流不起来呢？风骚的作者居然在上面严肃的时序图上加了个风骚的小星星，向同行的Worker们致敬！\n","link":"https://idouba.com/executor-framework-thread-pool-task-execution-part-02/","section":"posts","tags":["Infoq","Executor","Java"],"title":"戏（细）说Executor框架线程池任务执行全过程（下）"},{"body":"归档下发表于infoq.com 2015年6月的两篇文章。本来是一篇文章，经过Infoq编辑Alice Ding建议，拆分为\u0026lt;上\u0026gt;和\u0026lt;下\u0026gt;两部分。谢谢Alice对文章的细心校对，帮我发下了其中的很多问题。添加下infoq要求的声明：本文为InfoQ中文站特供稿件，首发地址为：http://www.infoq.com/cn/articles/executor-framework-thread-pool-task-execution-part-01。如需转载，请与InfoQ中文站联系。\n内容综述 基于Executor接口中将任务提交和任务执行解耦的设计，ExecutorService和其各种功能强大的实现类提供了非常简便方式来提交任务并获取任务执行结果，封装了任务执行的全部过程。本文尝试通过对j.u.c.下该部分源码的解析以ThreadPoolExecutor为例来追踪任务提交、执行、获取执行结果的整个过程。为了避免陷入枯燥的源码解释，将该过程和过程中涉及的角色与我们工作中的场景和场景中涉及的角色进行映射，力图生动和深入浅出。\n一、前言 1.5后引入的Executor框架的最大优点是把任务的提交和执行解耦。要执行任务的人只需把Task描述清楚，然后提交即可。这个Task是怎么被执行的，被谁执行的，什么时候执行的，提交的人就不用关心了。具体点讲，提交一个Callable对象给ExecutorService（如最常用的线程池ThreadPoolExecutor），将得到一个Future对象，调用Future对象的get方法等待执行结果就好了。\n经过这样的封装，对于使用者来说，提交任务获取结果的过程大大简化，调用者直接从提交的地方就可以等待获取执行结果。而封装最大的效果是使得真正执行任务的线程们变得不为人知。有没有觉得这个场景似曾相识？我们工作中当老大的老大（且称作LD^2）把一个任务交给我们老大（LD）的时候，到底是LD自己干，还是转过身来拉来一帮苦逼的兄弟加班加点干，那LD^2是不管的。LD^2只用把人描述清楚提及给LD，然后喝着咖啡等着收LD的report即可。等LD一封邮件非常优雅地报告LD^2report结果时，实际操作中是码农A和码农B干了一个月，还是码农ABCDE加班干了一个礼拜，大多是不用体现的。这套机制的优点就是LD^2找个合适的LD出来提交任务即可，接口友好有效，不用为具体怎么干费神费力。\n二、 一个最简单的例子 看上去这个执行过程是这个样子。调用这段代码的是老大的老大了，他所需要干的所有事情就是找到一个合适的老大（如下面例子中laodaA就荣幸地被选中了），提交任务就好了。\n1\t// 一个有7个作业线程的线程池，老大的老大找到一个管7个人的小团队的老大 2 ExecutorService laodaA = Executors.newFixedThreadPool(7); 3\t//提交作业给老大，作业内容封装在Callable中，约定好了输出的类型是String。 4\tString outputs = laoda.submit( 5\tnew Callable\u0026amp;lt;String\u0026amp;gt;() { 6\tpublic String call() throws Exception 7\t{\t8\treturn \u0026#34;I am a task, which submited by the so called laoda, and run by those anonymous workers\u0026#34;; 9\t} 10\t//提交后就等着结果吧，到底是手下7个作业中谁领到任务了，老大是不关心的。 11\t}).get(); 12\tSystem.out.println(outputs); 使用上非常简单，其实只有两行语句来完成所有功能：创建一个线程池，提交任务并等待获取执行结果。\n例子中生成线程池采用了工具类Executors的静态方法。除了newFixedThreadPool可以生成固定大小的线程池，newCachedThreadPool可以生成一个无界、可以自动回收的线程池，newSingleThreadScheduledExecutor可以生成一个单个线程的线程池。newScheduledThreadPool还可以生成支持周期任务的线程池。一般用户场景下各种不同设置要求的线程池都可以这样生成，不用自己new一个线程池出来。\n三、代码剖析 这套机制怎么用，上面两句语句就做到了，非常方便和友好。但是submit的task是怎么被执行的？是谁执行的？如何做到在调用的时候只有等待执行结束才能get到结果。这些都是1.5之后Executor接口下的线程池、Future接口下的可获得执行结果的的任务，配合AQS和原有的Runnable来做到的。在下文中我们尝试通过剖析每部分的代码来了解Task提交，Task执行，获取Task执行结果等几个主要步骤。为了控制篇幅，突出主要逻辑，文章中引用的代码片段去掉了异常捕获、非主要条件判断、非主要操作。文中只是以最常用的ThreadPoolExecutor线程池举例，其实ExecutorService接口下定义了很多功能丰富的其他类型，有各自的特点，但风格类似。本文重点是介绍任务提交的过程，过程中涉及的ExecutorService、ThreadPoolExecutor、AQS、Future、FutureTask等只会介绍该过程中用到的内容，不会对每个类都详细展开。\n1、 任务提交 从类图上可以看到，接口ExecutorService继承自Executor。不像Executor中只定义了一个方法来执行任务，在ExecutorService中，正如其名字暗示的一样，定义了一个服务，定义了完整的线程池的行为，可以接受提交任务、执行任务、关闭服务。抽象类AbstractExecutorService类实现了ExecutorService接口，也实现了接口定义的默认行为。\nAbstractExecutorService任务提交的submit方法有三个实现。第一个接收一个Runnable的Task，没有执行结果；第二个是两个参数：一个任务，一个执行结果；第三个一个Callable，本身就包含执任务内容和执行结果。 submit方法的返回结果是Future类型，调用该接口定义的get方法即可获得执行结果。V get() 方法的返回值类型V是在提交任务时就约定好了的。\n除了submit任务的方法外，作为对服务的管理，在ExecutorService接口中还定义了服务的关闭方法shutdown和shutdownNow方法，可以平缓或者立即关闭执行服务，实现该方法的子类根据自身特征支持该定义。在ThreadPoolExecutor中，维护了RUNNING、SHUTDOWN、STOP、TERMINATED四种状态来实现对线程池的管理。线程池的完整运行机制不是本文的重点，重点还是关注submit过程中的逻辑。\n看AbstractExecutorService中代码提交部分，构造好一个FutureTask对象后，调用execute()方法执行任务。我们知道这个方法是顶级接口Executor中定义的最重要的方法。。FutureTask类型实现了Runnable接口，因此满足Executor中execute()方法的约定。同时比较有意思的是，该对象在execute执行后，就又作为submit方法的返回值返回，因为FutureTask同时又实现了Future接口，满足Future接口的约定。 1\tpublic \u0026lt;T\u0026gt; Future\u0026lt;T\u0026gt; submit(Callable\u0026lt;T\u0026gt; task) { 2 if (task == null) throw new NullPointerException(); 3 RunnableFuture\u0026lt;T\u0026gt; ftask = newTaskFor(task); 4 execute(ftask); 5 return ftask; 6 } Submit传入的参数都被封装成了FutureTask类型来execute的，对应前面三个不同的参数类型都会封装成FutureTask。 1protected \u0026lt;T\u0026gt; RunnableFuture\u0026lt;T\u0026gt; newTaskFor(Callable\u0026lt;T\u0026gt; callable) { 2 return new FutureTask\u0026lt;T\u0026gt;(callable); 3 } Executor接口中定义的execute方法的作用就是执行提交的任务，该方法在抽象类AbstractExecutorService中没有实现，留到子类中实现。我们观察下子类ThreadPoolExecutor，使用最广泛的线程池如何来execute那些submit的任务的。这个方法看着比较简单，但是线程池什么时候创建新的作业线程来处理任务，什么时候只接收任务不创建作业线程，另外什么时候拒绝任务。线程池的接收任务、维护工作线程的策略都要在其中体现。 1//任务队列 2private final BlockingQueue\u0026lt;Runnable\u0026gt; workQueue; 3//作业线程集合 4private final HashSet\u0026lt;Worker\u0026gt; workers = new HashSet\u0026lt;Worker\u0026gt;(); 其中阻塞队列workQueue是来存储待执行的任务的，在构造线程池时可以选择满足该BlockingQueue 接口定义的SynchronousQueue、LinkedBlockingQueue或者DelayedWorkQueue等不同阻塞队列来实现不同特征的线程池。\n关注下execute(Runnable command)方法中调用到的addIfUnderCorePoolSize，workQueue.offer(command) ， ensureQueuedTaskHandled(command)，addIfUnderMaximumPoolSize(command)这几个操作。尤其几个名字较长的private方法，把方法名的驼峰式的单词分开，加上对方法上下文的了解就能理解其功能。\n因为前面说到的几个方法在里面即是操作，又返回一个布尔值，影响后面的逻辑，所以不大方便在方法体中为每条语句加注释来说明，需要大致关联起来看。所以首先需要把execute方法的主要逻辑说明下，再看其中各自方法的作用。\n如果线程池的状态是RUNNING，线程池的大小小于配置的核心线程数，说明还可以创建新线程，则启动新的线程执行这个任务。 如果线程池的状态是RUNNING ，线程池的大小小于配置的最大线程数，并且任务队列已经满了，说明现有线程已经不能支持当前的任务了，并且线程池还有继续扩充的空间，就可以创建一个新的线程来处理提交的任务。 如果线程池的状态是RUNNING，当前线程池的大小大于等于配置的核心线程数，说明根据配置当前的线程数已经够用，不用创建新线程，只需把任务加入任务队列即可。如果任务队列不满，则提交的任务在任务队列中等待处理；如果任务队列满了则需要考虑是否要扩展线程池的容量。 当线程池已经关闭或者上面的条件都不能满足时，则进行拒绝策略，拒绝策略在RejectedExecutionHandler接口中定义，可以有多种不同的实现。 上面其实也是对最主要思路的解析，详细展开可能还会更复杂。简单梳理下思路：构建线程池时定义了一个额定大小，当线程池内工作线程数小于额定大小，有新任务进来就创建新工作线程，如果超过该阈值，则一般就不创建了，只是把接收任务加到任务队列里面。但是如果任务队列里的任务实在太多了，那还是要申请额外的工作线程来帮忙。如果还是不够用就拒绝服务。这个场景其实也是每天我们工作中会碰到的场景。我们管人的老大，手里都有一定HC（Head Count），当上面老大有活分下来，手里人不够，但是不超过HC，我们就自己招人；如果超过了还是忙不过来，那就向上门老大申请借调人手来帮忙；如果还是干不完，那就没办法了，新任务咱就不接了。\n1public void execute(Runnable command) { 2 if (command == null) 3 throw new NullPointerException(); 4 if (poolSize \u0026gt;= corePoolSize || !addIfUnderCorePoolSize(command)) { 5 if (runState == RUNNING \u0026amp;\u0026amp; workQueue.offer(command)) { 6 if (runState != RUNNING || poolSize == 0) 7 ensureQueuedTaskHandled(command); 8 } 9 else if (!addIfUnderMaximumPoolSize(command)) 10 reject(command); // is shutdown or saturated 11 } 12} addIfUnderCorePoolSize方法检查如果当前线程池的大小小于配置的核心线程数，说明还可以创建新线程，则启动新的线程执行这个任务。 1private boolean addIfUnderCorePoolSize(Runnable firstTask) { 2 Thread t = null; 3 //如果当前线程池的大小小于配置的核心线程数，说明还可以创建新线程 4 if (poolSize \u0026amp;lt; corePoolSize \u0026amp;\u0026amp; runState == RUNNING) 5 // 则启动新的线程执行这个任务 6 t = addThread(firstTask); 7 return t != null; 8 } 和上一个方法类似，addIfUnderMaximumPoolSize检查如果线程池的大小小于配置的最大线程数，并且任务队列已经满了（就是execute方法试图把当前线程加入任务队列时不成功），说明现有线程已经不能支持当前的任务了，但线程池还有继续扩充的空间，就可以创建一个新的线程来处理提交的任务。 1private boolean addIfUnderMaximumPoolSize(Runnable firstTask) { 2 // 如果线程池的大小小于配置的最大线程数，并且任务队列已经满了（就是execute方法中试图把当前线程加入任务队列workQueue.offer(command)时候不成功）,说明现有线程已经不能支持当前的任务了，但线程池还有继续扩充的空间 3 if (poolSize \u0026amp;lt; maximumPoolSize \u0026amp;\u0026amp; runState == RUNNING) 4 //就可以创建一个新的线程来处理提交的任务 5 t = addThread(firstTask); 6 return t != null; 7 } 在ensureQueuedTaskHandled方法中，判断如果当前状态不是RUNING，则当前任务不加入到任务队列中，判断如果状态是停止，线程数小于允许的最大数，且任务队列还不空，则加入一个新的工作线程到线程池来帮助处理还未处理完的任务。 1private void ensureQueuedTaskHandled(Runnable command) { 2 // 如果当前状态不是RUNING，则当前任务不加入到任务队列中，判断如果状态是停止，线程数小于允许的最大数，且任务队列还不空 3 if (state \u0026lt; STOP \u0026amp;\u0026amp; 4 poolSize \u0026lt; Math.max(corePoolSize, 1) \u0026amp;\u0026amp; 5 !workQueue.isEmpty()) 6 //则加入一个新的工作线程到线程池来帮助处理还未处理完的任务 7 t = addThread(null); 8 if (reject) 9 reject(command); 10 } 在前面方法中都会调用adThread方法创建一个工作线程，差别是创建的有些工作线程上面关联接收到的任务firstTask，有些没有。该方法为当前接收到的任务firstTask创建Worker，并将Worker添加到作业集合HashSet workers中，并启动作业。 1private Thread addThread(Runnable firstTask) { 2 //为当前接收到的任务firstTask创建Worker 3 Worker w = new Worker(firstTask); 4 Thread t = threadFactory.newThread(w); 5 w.thread = t; 6 //将Worker添加到作业集合HashSet\u0026lt;Worker\u0026gt; workers中，并启动作业 7 workers.add(w); 8 t.start(); 9 return t; 10 } 至此，任务提交过程简单描述完毕，并介绍了任务提交后ExecutorService框架下线程池的主要应对逻辑，其实就是接收任务，根据需要创建或者维护管理线程。\n维护这些工作线程干什么用？先不用看后面的代码，想想我们老大每月辛苦地把老板丰厚的薪水递到我们手里，定期还要领着大家出去happy下，又是定期的关心下个人生活，所有做的这些都是为什么呢？木讷的代码工不往这边使劲动脑子，但是猜还是能猜的到的，就让干活呗。本文想着重表达细节，诸如线程池里的Worker是怎么工作的，Task到底是不是在这些工作线程中执行的，如何保证执行完成后，外面等待任务的老大拿到想要结果，我们将在下篇文章中详细介绍。\n","link":"https://idouba.com/executor-framework-thread-pool-task-execution-part-01/","section":"posts","tags":["Infoq","Executor","Java"],"title":"戏（细）说Executor框架线程池任务执行全过程（上）"},{"body":"","link":"https://idouba.com/tags/%E7%A8%8B%E5%BA%8F%E5%91%98/","section":"tags","tags":null,"title":"《程序员》"},{"body":"发表于《程序员》2015年4月B的一篇文章，在博客归档下。根据杂志社要求，在自己博客发表该文章亦须注明：本文为CSDN编译整理，未经允许不得转载，如需转载请联系market#csdn.net(#换成@)\n想通过原理来说明一些技术白皮书上“什么时候应该使用什么”这个“应该”后面的原因。通过数据结构中经典的排序查找算法来推倒解释数据库中几种经典的表连接背后的算法原理，和原理决定的在各种数据库中不同的应用和限制。以简单的算法来讲出数据库系统中看着核心强大功能的本质的算法设计。较之白皮书中不同数据库的不同描述，尽量去除差异，通过原理来描述功能，做到深入浅出。\n一、前言 Join的语义是把两张表的通过属性的关联条件值组合在一起，一般意义上数据库范式越高，表被拆分的越多，应用中要被Join的表可能会越多。在我们日常开发中几乎找不到不涉及Join的SQL语句，哪怕未显示包含Join这个关键字。在数据库系统执行时，都会选用一种明确最优的Join方式来执行。通过对Join原理的理解能帮助理解数据库的Join选择，进而更深刻的理解查询计划，理解同样的表结构数据行数变化、索引的变化为什么会影响数据库Join方式的选择，从而更针对性的做好性能优化。\n如下图分别是oracle和mssql的执行计划，是涉及四个表的内连接。在查询计划中显示每一个步骤的表连接方式，从图中可以看到共有三个表连接操作(有意思的是，对于相似的数据集，两个数据库选择的连接方式不同)。\n​ 图表 1 Oracle执行计划\nJoin原理在数据库厂商的文档中和经典的《数据库系统概念》中分别从应用和理论的角度上描述的足够详细。本文尝试从另外一个视角，通过数据结构中最基础的查找排序算法(Search\u0026amp;Sort)来类推和归纳最常用的三种Join方式的原理：嵌套循环连接(Nested Loop Join)、排序归并连接 (Sorted Merge Join)、 哈希连接(Hash Join)。进而了解三种方式之间的联系、演变以及原理决定的其应用上的细微差别。说明这几种表连接方式是数据库的技术，但和数据库索引等其他数据库技术一样，也是是典型的数据结构的延伸。\n​ 图表 2 Mssql执行计划\n二、实例数据 为了示意清楚，本文中用到的示例中的两个数据集比较简单。为如下两个表。\nT_FC Fc_id Fc_name OfficialSite CityId 1 Inter www.inter.it 2 2 Barcelona http://www.fcbarcelona.com/ 8 3 Manchester united http://www.manutd.com 12 4 Liverpool http://www.liverpoolfc.com/ 11 5 Arsenal http://www.arsenal.com/ 36 6 ACM http://www.acmilan.com/ 2 7 Chelsea http://www.chelseafc.com/ 36 8 Manchester city http://www.mcfc.co.uk/ 12 ​ 图表 3 测试数据T_FC\nT_City Fc_id*CityId* Fc_name Country Desc 36 Landon Britain ? 11 Liverpool Britain ? 12 Manchester Britain ?2 Milan Italy ? ​ 图表 4 测试数据 T_City\n第一个表是一个实体表，记录了几个Football Club的一般性描述。其中第一条记录是作者支持了17年的球队：位于Milan城的大国米！第二个表是一个城市信息的实体表，两个表通过CityId列关联，表示那个俱乐部位于那座城市。从示例数据中可以看到，不同的俱乐部可以有相同的CityId，这就是传说中的德比，最著名、最伟大、最火爆的，也是作者一直关注的米兰德比，还有最近几年才开始关注的曼彻斯特德比。\n为了简单期间，以最简单的内连接中的等值连接条件举例。\n1SELECT T_FC.Fc_id, T_FC.Fc_name, T_FC.OfficialSite, T_FC.CityId, T_City.City_Name, T_City.Country 2FROM T_FC , T_City 3WHERE T_FC. CityId = T_City. CityId 数据准备好了，然后进入正文，看看如何从数据结构的Search Sort 归纳出Join。\n三 、从查找排序算法看表连接原理分析 表连接主要考察的是连接列的关系，于是作为切入点，暂时忽略掉连接键之外的列，发现待连接的两个表变成了两个数组，讨论问题陡然看着清爽简单了很多。所谓的连接无非就是把这两个关联列中符合连接条件对应元素找出来配对即可。我们例子是一个等值条件，这个匹配条件转为查找的讨论就更直接了，即考察一个集合的元素，找出另外一个集合值其相等的元素。\n图表 5 关联列\n看出来了吗？我们在试图偷换概念。用检索search来替换连接Join。是的，这是我们的切入点，后面会再绕回来的。检索，这在数据结构和算法中是多么经典的技术。后面讨论我们会发现数据库系统中“发明”的高大上的连接方式，其实也也是从对应的经典的检索算法推演而来。\n嵌套循环连接Nested Loop Join 看着前面的这两组数字，不用太浪费脑细胞，最笨的办法，穷举呗，拿着一边的数据，一个一个拿出来，看看和隔壁的哪个对上眼。表现在算法实现上其实就是两重循环，循环体内就是一个匹配看是否满足连接条件，对于这个例子就是取值是否相等。即拿着一组中的数字，逐个另一组中去匹配，看是否有等值的元素存在。\n假设我们先考虑T_City的数据，即外层循环是遍历T_City表的数据。则写成伪代码如下。\n1for each CityId in T_City do begin 2for each tuple CityId in T_FC do begin 3 if T_City.CityId=T_FC.CityId 4 output(T_FC.CityId,T_City.CityId) 下图描述了这种连接方式的过程，从_T_City_中逐行选取记录，在_T_FC_的_CityId_列上按个查找符合连接条件_T_City.CityId=T_FC.CityId_的记录。找到则添上一个箭头表示建立连接(Join)，多么形象！从图中看到总共添加了7个箭头，对应Join结果集中的7个Join结果。\n图表 6 嵌套循环连接示例 这就是检索中对于没有规则的数据集的最笨的检索办法。恭喜你，你上面写的这段最笨的检索伪代码正是数据库系统中嵌套循环连接(Nested Loop Join)的主要思路。术语上，外围循环的数据集_T_City_被称为外部表(Outer Table)，又称驱动表(Driving Table)；内部循环的表_T_FC_称为内部表(Inner Table)。\n实现中扫描外部表的每一行，对每一行数据，扫描内部表中满足Join条件的记录，将其作为输出结果集。最终的结果集是每次外部扫描的结果集并在一起。因为要通过两重循环对两组中的每一个数据进行比较，以判断是否满足Join条件，很容易发现这种方法是开销是相对比较昂贵的。\n对于Nested Loop Join，Oracle、Mysql、Mssql几乎所有的关系数据库都支持。虽然实现上细节上稍微有不同。思路都一样。\n根据这个原理我们不难分析到最好外部表即驱动表不能太大。如果是两张表比较大，尤其是Outer Table比较大的情况，Inner Table会被扫描很多次，这时候的算法复杂度增加得非常快，总的资源消耗量也会增加得很快。同时，如果Inner Table在Join条件上有索引，则每次内循环的效率会很高，Inner表数据集大也没有关系，不用挨个扫描Inner表就可以定位到匹配的行。\n排序归并连接(Sort Merge Join) 那有没有比整个列表扫描更高效的方法呢？当然还是按照检索的思路。对于一个没有规则的数据集只能全部扫描，检索的时间复杂度是O(n)，而如果记录集是排好序的，查找会变得简单。\n结合上面的例子，如果把两组数都排好序，那么Join会是什么样子？马上脑海里浮现出了如下场景：把一个有序集合插入到另外一个有序集合的对应位置，使得最终的集合也保持有序，即把两个有序集合合并成一个有序集合。如图，把左边T_City表中黄色的记录和右边T_FC中绿色的记录合并在一起。\n图表 7 归并排序示例 这个不正是经典的归并排序吗？没错，就是归并！假设两个集合都是从小到大排序的，归并排序的主要思路是：在输入的有序集合设定两个指针，起始位置分别为两个已经排序序列的起始位置，比较两个指针指向元素，选择相对小的元素放入输出的合并空间，并移动指针到下一个位置，直到一个集合为空，把另外一个集合的剩余元素一下全部放到输出的合并空间。哪怕不仔细了解内部逻辑，只是看循环，就能发现和嵌套循环连接最大的区别是循环只有一层了。因为不用对两个集合的每两个元素都要进行比较。那时间复杂度就一定降低了一个级别。\n上面归并排序示例图经过小的改动，就会成下图。输入是一样的，两个有序集合。中间的merge过程也类似，不同的是，没有把一个有序集合的元素对应插入到另外一个有序集合中，而只是按顺序在两个有序集合中相等的元素间建立联系。\n图表 8 排序归并连接示例\n参照连接条件T_City.CityId=T_FC.CityId，根据左边T_City表上CityId上列的取值，挨个在右边的T_FC上查找，和嵌套循环连接的示例图一样，结果也是添加了满足条件的7个箭头，并且也同样得到了7个Join结果，但是箭头的分布还是有挺大差别。在Sort Merge Join中的箭头是没有交叉的，而Nested Loop Join中箭头交叉很多。这个简单的特征其实反应了两种方法的原理上的差别。以T_City中CitypId=2的行发起的箭头为例，在第Sort Merge Join中，因为右边行集是排序的，因此是扫描了前两行的两个匹配记录后，碰到第一个不匹配的第三行，则结束扫描；而在Nested Loop Join中，右边的行集是没有顺序的，要找到符合连接条件的，必须对右边行集进行完整扫描。\n观察到Outer Table中的连接的键值每次和Inner Table 的连接键值比较时，都不用从第一个元素开始，也不必一直扫描到最后一个元素。因此时间复杂度也会比Nested Loop Join降低。\n根据这个原理很容易发现使用排序归并连接可以获得比较高的连接效率。但是所谓没有免费的午餐，使用排序归并连接需要待连接的两个集合在连接列上是排好序的。我们知道对于大的数据集排序代价一般是不小的，因此优化器在选择是否采用排序归并连接还要参考其他条件。如果输入的集合本身是在有序的(如在Join条件的列上有B树索引)，或者SQL语义中有order by要求，在连接列上本来就是要求排序的，则优化器可能会考虑选择排序归并连接。一般对于大的集合如果不能采用排序归并连接，则另外一种连接方式Hash连接会被考虑。\n哈希连接(Hash Join) 还是从查找算法思路来考虑。查找算法中有一个通过构建一点额外空间而换取O(1)的时间复杂度的检索方法，那就是hash查找。将该算法思路类推到数据库中，就是大名鼎鼎且功能强大的哈希连接。Hash查找中通过一个Hash函数，对待查找的元素集合进行分组，使得元素的存储位置和元素值之间有一定联系。则对于要查找的元素，只要计算其Hash值得到存储位置，直接访问即可(Hash函数的选择不好引起的过多的Hash冲突，造成计算得到的存储位置定位精准度不够在此不作讨论)。\n在Hash Join中，同样使用一个合适的Hash函数对两个行集上的Join条件列分别进行分组，在匹配中只要比较两边相同分组内的记录即可。不用像嵌套循环连接一样，挨个比较两个集合中每两个元素。其实也可以认为，嵌套循环连接就是Hash Join中只有一个分组的情况。如果Hash函数选择越好，分的组越均匀，则Hash冲突越少，分组就越有效，性能也越好。\n在示例中使用最简单的哈希函数_Hash(n)=n%10_将两个输入行集的Join条件列CityId分为10个组。由Hash函数的定义可以知道，Join条件满足如下推导： 因此在图中左边和右边Hash后partiton的对应记录的箭头是水平的。如图标黄的部分中12和2均落入partition2，则Join时左边标黄的partition2中的记录只用考察和右边partition2中的记录建立连接即可。\n图表 10 哈希连接示例\n在数据库系统实现中，Hash Join方式一般通过一个输入集来构造Hash Table，扫描另外一个输入集，对其中的每个元素通过Hash查找的方式快速定位到Hash Table的对应位置，即找了满足连接条件的记录。如示例中，根据_T_City_表中的_CityId_建立Hash Table。对_T_FC_中的_CityId_使用同一个Hash函数，计算得到Hash值，定位到Hash Table的元素位置，只需在该位置中查找匹配的元素建立连接即可。这种方式和上面的描述的对两个数据集都采用同样的Hash函数进行分组，然后组内检查匹配逻辑上是一个意思。\n通过分析Hash Join的原理，我们可以分析出如果用于两个数据集差别比较大的时候，如果小的数据集能加载到内存构造Hash Table，然后从磁盘上扫描另外一个较大的数据集，对大数据集中的Join列计算Hash值得到内存Hash Table的Index，然后建立连接。这样性能会非常高，这时真正的物理IO的花销就只有造Hash Table时对小数据集的扫描和匹配时对大数据集进行的扫描。当然，很容易看出Hash Join的一个天然限制，那就是连接条件必须是等值条件，因为Hash Join的机制需要计算Hash值得到Hash Table的位置，来建立连接。但是Hash Join不需要数据集事先按照什么顺序排序，也不要求上面有索引。\n四、总结 本文尝试以数据结构中最基础的查找排序算法为切入点来归纳三种经典排序算法在实现原理上的联系和差别。比较有意思的是，最简单的办法不是看作者啰啰嗦嗦的描述，而是分别观察几张Join示意图中连接箭头的特征，同样的两个数据集做连接，都是T_City在左T_FC在右，因此箭头的方向都是从左指向右。但是箭头的特征还是有差别：嵌套循环连接(Nested Loop Join)的箭头是偏上或者偏下各种方向都有，且是彼此交叉的；排序归并连接(Sort Merge Join)的箭头是虽然也是偏上偏下方向都有，但彼此没有交叉；而哈希连接(Hash Join)的箭头最具有特征，所有箭头都是水平的。\n以上仅仅是在逻辑数据集上比较纯粹的分析数据库系统中三种连接方式的原理。实际情况要复杂的多，并不存在这样一种方法，比其他的方法高明高效。数据库的查询优化器在选择一个连接方式时会结合操作对象的数据统计情况，查询语句的要求，选择一个合适的连接方式。另外支持一种以上Join方式的数据库系统(如oracle和mssql)中都支持Join hint使得用户可以要求优化器在使用指定的连接方式覆盖优化器自己选择的Join方式。一般还是认为优化器足够聪明，不使用Join Hint。\n最后，基于之上的原理分析在下表中对三种连接方式在使用场景等几个细节方面做个更全面的比较和归纳：\n嵌套循环连接(Nested Loop Join) 排序归并连接(Sort Merge Join) 哈希连接(Hash Join) 支持的常用数据库 oracle, mssql,mysql oracle, mssql oracle, mssql 适用场景 两个较小的数据集，Inner table Join列上有索引 较大数据集，Join字段上有索引，或者语句要求结果集被排序。 大数据集，Join字段上没有索引也不要求排序 连接字段表达式 任何表达式 \u0026lt;、?\u0026lt;=、?\u0026gt;、?\u0026gt;=;不包括?\u0026lt;\u0026gt; 仅用于等价连接 数据集排序特征 不要求 要求 不要求 内存资源要求 要求较少 当需要对数据集排序时，内存要求较高。 高 返回第一次查询结果 当索引选择性比较高或限制条件比较多时能较快返回第一次查询结果。 必须在得到全部结果后才能返回数据。 因为要在内存中简历哈希表，第一次返回结果较慢。 完。 ","link":"https://idouba.com/learn_table_join_from_search_and_sort/","section":"posts","tags":["表连接","《程序员》"],"title":"从Search Sort到Join"},{"body":"","link":"https://idouba.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/","section":"categories","tags":null,"title":"数据库"},{"body":"","link":"https://idouba.com/tags/%E8%A1%A8%E8%BF%9E%E6%8E%A5/","section":"tags","tags":null,"title":"表连接"},{"body":"年底了，该总结了。例行的招呼几个项目负责人对几个项目执行情况进行总结，项目上中实践比较成功的，记录细节，评估其他项目借鉴的可能性；项目上存在的问题，重点一起剖析下。由表面现象，到深层次的原因，力图通过讨论或争论都能有一致的认识，形成改进。但希望是理解了改进，而不是被要求改进。为了更生动的说明自己的观点，作为主持人的笔者准备了一个大家都比较熟悉的例子，公司附近的一个工地上项目。居然发现效果很好，新鲜，生动，容易被大家理解和引起共鸣。也是，在批判自己的过程中，肆无忌惮的给第三方挑毛病的确是很爽的一件事情！笔者在进入软件这个行当之前干的就是在[铁路工地上干活][1]的，经常不自觉的把这两种项目拿来比较。虽然看上去是有点跨界，其实也差别不大，项目那些事儿无非就如何把一组人组织起来有效的完成一件事情，达到规划的目标，做出一个有用的东西。因此，存在的问题和问题产生的原因也差不多。\n例子中的这个项目是公司附近大家吃饭下班都会路过的一个工地，位于某某高新开发区区政府广场附近。项目一部分是图中B处一个过街地下通道，另外一个是不远C处的一个地铁站。从位置上就能看出其地位来，算是这个核心片区的重点项目。这么一个形象工程、利民项目被拿来作为一个反面案例，自认有点惶恐，有点忐忑。\n先看B处这个过街地下通道，严格意义上讲不上一个新项目，因为原来就有，这次虽然项目规模很大，工程持续时间很长，给周边大家的生活带来了挺大影响，但其实工程的性质就是对原有项目的完善。即做了个2.0版本，只是对1.0版本的升级。\n2.0版本的最主要的需求当然就是1.0版本存在的问题。那1.0版本最主要的问题是什么呢？就是没人用。在附近生活了四年了，几乎没有见过几个个人使用过。那么2.0版本的主要功能是什么呢？\n地下通道的地面和墙壁都进行了粉刷和重新整修，尽管原来那个版本因为几乎未被使用，也没有什么损耗。即对1.0版本的UI进行了全面改善。 在西北、西南两个出口的台阶楼梯边上增加了两部手扶电梯，用户体验更好了。 在四个出口处都增加了玻璃的顶棚，下雨这种异常情况也处理的更得体了。 四个出口周边增加了绿化和石凳，整个产品风格一下就小清新了。 在西北出口处增加了保安，现场对产品进行技术支持和运维。 那2.0版本做的这么多看着很漂亮的功能是否解决了1.0的问题呢？本山大叔说过“看疗效！”。遗憾的是，在完工的近半年里，笔者没观察到什么变化，每天早晚上下班时间最大的人流还是选择从图示的A处，鲜有人会绕道这边来。非要说带来的变化，这个落寂的爱民工程倒是使得这个新区的政府广场看上去一下子高档了很多。\n费了很大劲做了一个没人用的产品，这个项目难被评价为一个成功的项目。为什么花了这么大的代价，大半年时间，做出的产品用户却不买账呢？这是我们总结中让大家来分析的，因为我们的项目中也曾经发上过至少两次同样的事情，虽然规模和时间没有这么夸张。原因无一例外就是需求没有搞清楚，详细说分两种，一种是没有搞清楚产品的目标用户是谁，另外一种是没有搞清楚目标用户需要什么。\n这个项目的目标用户是谁呢？在该项目往西一百多米的A处。马路南面和北面正对着是本区域最大的公交站，北面是该新区最繁华的商业步行街Star Road，是商场和写字楼聚集地。在1.0的时代，每天早晚上下班的时候总有那么一批人一大拨一大拨的从马路这边匆匆的冲到马路对面，先是穿过车流，然后来到中间绿化带，跨过环卫大妈们一遍一遍重铺过的草地，拉起的栏杆等各种障碍，再穿过另外一边的车流，艰难上岸，再冲进一个个大楼里。2.0后，可能随着新区的发展，人流量更多了，可怜的环卫大妈仍绞尽脑汁在和这些写字楼中的白领们斗争着，草坪踩了又种，种了又踩；隔离带由绳索升级为铁栏杆，再由铁栏杆升级为“过街走地道行路讲文明”的告示牌依然不能阻止上班族们在上下班时从写字楼和马路对面的公交站之间之间穿行。看上去项目的目标用户正是这些从城市的各个角落赶到这个公交站，为了掐着点赶着去写字楼打卡考勤，早点都要路上边走边吃，舍不得绕半个街区的时间来使用这个便民项目，冒着风险两次穿过车流，顶着巨大的道德压力跨过“过街走地道行路讲文明”的可怜的小白领们。他们是这个项目的目标用户，但却在项目立项的时候被无情的忽视了，不管是1.0版本和2.0版本。\n再来看下提供的是不是用户期望的功能。我们来看看该项目中完成的五大feature，前四个无一例外都是用户体验上的改善。UI更干净了，操作方式做的更智能便利了，异常处理做的更完善了，整个产品风格都变得更清新了。那么用户的实际需求是不是对体验不满呢？是不愿意走楼梯而愿意选电梯，是因为下雨天漏雨而不愿意使用吗？是嫌这边没有花花草草才去马路中间的绿化带里寻找去了？显然都不是，观察到用户的基本需求仅仅是能便利、节省时间的安全的通行，比让穿过马路方便安全就行。如果这个基本需求不能满足，体验上的改善很难打动他们改变主意。\n在一个个分析2.0的功能时候，有人提出了不同认识，那就是功能5中提到的不是技术支持，因为通过观察这些人从事的不是引导搀扶小老弱病残这样的支持类工作，也不是配合维护电梯通道等的维护类工作，而是把B处行人拉到楼梯口通行，这应该算是产品推广性质的工作吧。配合A处各种生物、物理、精神路障，B处活人路障是铁了心要用大禹前辈的治水方式来把人“堵”到通道中去。为了产品推广还真是蛮拼的！但直到今天只有B处零星的行人是被堵成功了，A处的真正目标用户一点变化也没有。\n上面两个问题在我们软件项目中也是很容易碰到的，分析我们曾经出问题的两个项目底层的原因正是这样。项目立项的时候要先明确目标用户，搞清楚谁对这个产品有需求，这个产品做好了给谁用，这看似是个基本原则。但实践中经常这个最基本的原则却经常被忽视。很多营销上噱头的东西经常压倒了对目标用户的分析成为我们考虑的重点。没有明确目标用户，就开始做需求开发，然后功能分解后就开工干了。最终做出的东西找不到人用也就不足为奇了。例子中这个项目很可能在1.0立项的时候根本没有考虑目标用户，只是要建成区政府广场一角的一个形象工程。天安门广场有个过街通道，咱也要搞一个，这是标配。至于给谁用谁来用先不考虑了。\n另外一个问题存在的就更普遍，有一个在开发的产品中正不同程度的还存在着。就是当产品推广不力的时候，我们最容易想到的就是用户体验上改善，什么图表不漂亮了，页面色彩不美观了，UI风格不舒服了这些，而忽视了用户对产品功能最基本的诉求。做出这样的决策也不难理解，第一用户体验的问题较之产品功能更容易被发现，更容易被提出；第二，从开发人员角度来说，用户体验的修改较之底层功能调整代价和影响要小；第三，虽然不好意思说，但实际中却经常发生，提出用户体验的要求对项目组外的领导们技术和业务能力要求最低，最容易发挥影响力来指点。没有冒犯权贵的意思，但真的不排除 例子中这个2.0改造项目是某某大领导亲自批示的，除了这个项目外，整个区政府周边整体UI在浩浩荡荡的也在被翻新。我们经常嘲笑那个地方刚栽好的花毁了种草，刚种好了的草铲了铺路这样瞎折腾。却忘了我们项目中花了多少力气反复干把一个网页横的变成竖的，蓝色的变成绿色的这样的事情。\n挑了毛病，那能建议个方案吗？其实也不难想，用户怎么用，就怎么提供产品呗。A处 是主要人流的地方，那在A处建一个过街通道即可。在讨论中有人在此基础上提出了另外一个方案获得了大家的积极响应。\n看图上最右边标识C处是一个地铁站，是新区最大的地铁枢纽，一号线已投入使用，规划有其他线路在这里换乘。从现阶段的使用情况来看，从地下上到地面上出站的主要人流方向也是涌向左边的区政府和这个商业街区。有没有可能把地铁的出站口延伸到A处，则那个期待的过街通道就被复用为地铁出口。本来距离也不是很远，而且在Start Road步行街地下一层原本就是另一个步行街。那事情就变得非常简单，把地铁站和Start Road 的地下一条街打通，孤立的两个项目就被整合为一个项目了。\n整合的优点是很明显的，这种设计其实是很成熟并且在大量被使用。延伸的通道可以增加很多商业，作为Start Road地下商业街的一个延伸，不评估购买力，单人流量延伸的这段就比原来的那段要多很多。项目B这样的原来就存在的小过街通道转型为地铁出口，可以直接出站到区政府。当然在A处公交站和Start Road马路两侧的增加的地铁出口，不仅可以解决此处大量人流的过街需求，还可以方便的完成此处公交和地铁的换乘，可以一路引导地铁出站的人很方便的到达商业街。最重要的是可以更方便的让大量来商业街和区政府这两处的外来人找到地铁口。\n说到找地铁口，笔者在地铁开通的这一年里，不下十次的在区政府和Start Road周边被问到“请问地铁站在哪儿”，开始也非常好奇，不是有地铁标识了吗？老头老太太就算了，怎么看着大学生模样的也不认识标识。于是乘地铁时留意了下周边的标识，被这些工程师同行的杰作给雷到了。150M这个较实际距离偏大暂不说，你把它放在出口处的边上就有点疑问了。就因为我手里有一个150M的牌子和一个100M的牌子，我就分别两个方向目测下距离装好就行。我们做这样设计的时候有没有想过，在商业街、区政府主要人流量的地方需要引导用户的地方没有标识，而用户一路询问都看见地铁口的广告牌了，发现此处你杵着一个标识告诉我需继续往前再走若干米米。感慨我们这些工程师们做项目时候，“用户观点”总是挂在嘴边，但实施的时候工程师的思维还是根深蒂固。\n回到两个项目整合的问题，这种整合方案其实不难想到。就这这个例子，我们这种干另外类型项目的人根据自己的背景都能指手画脚，施工的同行们一定也应该是考虑过的。这种方案在地铁站规划的时候一定作为一种方案被提出过。和我们软件项目一样，两个功能模块在设计初期，根据规划，先分开设计和开发，留好扩展的接口，在适当的时机进行整合。即使设计初期没有考虑到，两个功能完全不相干，分开设计和开发。在产品演变过程中，表现出了相关性，整合的必要出现了，也会在适当的时机进行整合。但实际中我们都有这样的经验，大多数时候这不是一个设计问题，而是一个执行的问题。\n一般经验来说，当基于整体需求的评估，要做整体考虑方案，则越早越好。但实际执行中，单个模块工期的压力，整合带来的实施上的不确定性和潜在的风险，甚至两个项目原本不同项目组开发引起人员上的困难，都使得下决心“整”变得困难。一般会被冠以“时机不成熟，待下个版本解决”这样一次一次的拖下去。这个时候，考验的不是大局观，设计能力，而是执行能力。考验的不是智慧，而是的是勇气、魄力。拖下去的好处是当时安生，但把麻烦留给了以后。经常在上一个版本多一点努力可以完成的，在下一个版本需要费很大气力。尤其是接口部分，要么是丑陋、别扭的适配，要么就是大量推翻重写。就像这个项目一样，如果在地铁站规划中能扩展考虑到延伸方案，则一下解决了多个问题，而像现在这样独立的来做，只解决了部分问题，待到下期整合时，结合处上上下下的台阶斜坡，通道处拐弯转交这样的接口硬伤是避免不了。\n夹生饭再下锅蒸熟了品质当然不如一贯的好，这是结果上看，从过程上看，这样工程执行需要付出更多的代价。付出的人力和时间成本虽然在当前版本中不会考虑，在下个版本中也会以正当的理由冠冕堂皇的被贴上 重构这样高大上的标签，项目方案洋洋洒洒，项目计划宏伟庞大，但实际上从整个项目来看是做了无用功，其实就是出现了返工。就像例子中这个项目一样，在一两年后如果要打通的时候，我们又会看到和今年上半年这里一样忙碌的工作场景，重新圈起来的工地上轰隆隆的的电锤杂碎的每个石子儿都是两年前同伴们的辛苦劳作。工程上这样修了又拆，拆了又修的案例最近几年经常见诸报端，我们这些在屋子里面做项目的人经常对这些工地项目的粗鲁做法嗤之以鼻，好像那些做项目的人是野蛮人，骂其瞎折腾。但却忘了其实很多时候我们更野蛮，这样的例子在我们自己项目中其实也不也一直在发生，从上层被折腾的UI，到底层被折腾的实现，从某个独立模块的实现方式，到两个模块间的接口方案，当然大部分我们可以理直气壮的讲这是经过充分的评估和讨论的，是改进，不是瞎折腾，但是其中有多少是真折腾，过程中参与其中的人有时候恐怕自己也判别不清楚，在我们这种总结会议上大家跳出来争论后才可能形成结论。只是我们这种折腾进行的比较隐蔽罢了，但其带来的浪费真的不见得比看到的我们的同行们在工地上砸的几块地砖，扒掉些混凝土影响小，代价少。\n洋洋洒洒会议持续了两个小时，扯了很多。平时不允许这样浪费时间来开会，既然讨论的这么热烈，感觉讨论通了，尽兴了当然就也就理解了，达到目的了。很快会议纪要邮件就发出来了。一级标题如下：\n搞清目标用户 理解用户根本需求 UI好可增加颜值，但是功能好才是真的好。 做项目的要会想，但更多时候要会just do it。 项目规划和下棋一样，走着瞧，但尽量要多看几步，否则悔棋不易。 满满当当的项目规划和执行计划有时候是蒙老板的，有料没料要看是否厚道。 用户观点，挂在嘴边不够，应刻在脑门上，最好深一点，刻到骨头上。 当然每个抽象的观点下面都有详细展开，包括实际例子来说明，这个跨界的工程只是其中很小的一部分花絮素材，大部分还是说我们自己项目中实际的问题。比较散，有点乱，但都是有联系的。虽然纪要中是按照观点来展开的，但是如果按照涉及的项目来索引发现很少有一个项目仅存在一个问题。通过讨论，大家问题想通了想透了，最终改进也就有了前提了。做好明年的项目，争取少犯这些错误。\n有心者在会议纪要上回复如下：“对例子中项目的诸多评价仅是一帮屌丝工程师学术讨论，个人观点，纯属YY，特此声明。”也作为本文的一个声明。\n","link":"https://idouba.com/about-projects/","section":"posts","tags":["随笔"],"title":"跨界看项目那些事儿"},{"body":"","link":"https://idouba.com/tags/aqs/","section":"tags","tags":null,"title":"AQS"},{"body":"","link":"https://idouba.com/tags/%E5%B9%B6%E5%8F%91/","section":"tags","tags":null,"title":"并发"},{"body":"1. 前言 AQS(AbstractQueuedSynchronizer)是 java.util.concurrent的基础。J.U.C中宣传的封装良好的好用的同步工具类Semaphore、CountDownLatch、ReentrantLock、ReentrantReadWriteLock、FutureTask等虽然各自都有不同特征，但是简单看一下源码，每个类内部都包含一个如下的内部类定义：\nabstract static class Sync extends AbstractQueuedSynchronizer 同时每个类内部都包含有这样一个属性，连属性名都一样！注释已经暗示了，该类的同步机制正是通过这个AQS的子类来完成的。不得不感叹：“每个强大的同步工具类，内心都有一把同样的锁！”\n1/** All mechanics via AbstractQueuedSynchronizer subclass */ 2 private final Sync sync; 几种同步类提供的功能其实都是委托sync来完成。有些是部分功能，有些则是全部功能。 本文中就是想尝试比较分析下在几个同步工具类下面定义的AQS的子类如何来实现工具类要求的功能。当然包括两部分，一部分是这些工具类如何使用其Sync这种类型的同步器，也就是工具类向外提供的方法中，如何使用sync这个句柄；第二部分，就是工具类中自己定义的内部类Sync继承自AQS，那到底override了哪些方法来做到以父类AQS为基础，提供受委托工具类的功能要求。\n关于第一部分，sync如何被其工具类使用，请允许我无耻的在一个文章中把一个类所有代码贴出来。\n所幸方法很多，总的代码行不多，因为每个方法都是一个风格，就是换个名直接调用sync的对应方法。这是Semaphore中对sync的使用。是不是觉得写这个代码的作者比写这个文章的作者还要无耻？在其他几个工具类中，没有这么夸张，b但基本上也是这个风格，即以一个helper的方式向外面的封装类提供功能支持。所以第一个问题，在文章中说到这里，后面涉及到也只会简单描述。 主要是求索第二个问题，即每个工具类中自己定义的Sync到底是什么样子，有哪些不同的特征，其实也就是代码上看这些Sync类对父类AQS做了哪些修改。\n2. AQS简介 要介绍子类的特征，父类总得大致介绍下。AQS的原理、设计等比较系统的东西，在这里就不想涉及了。可以参照《深入浅出 Java Concurrency》{#viewpost1_TitleUrl}系列的深入浅出 Java Concurrency (7): 锁机制 part 2 AQS{#viewpost1_TitleUrl}一节，谢谢这个系列，作者讲的确实非常的深入浅出！要想了解更多，可以参考Doug Lea大师的原著The java.util.concurrent Synchronizer Framework。最简单的办法其实就是的耐心把AbstractQueuedSynchronizer源码前面注释的javadoc完整的读一遍就可以了。笔者反正有这样的习惯。扎着脑袋看代码，看注释，然后自己看看是否能把一个package有个系统的视图，如果需要再看相关的参考文档来确认这个系统的视图。\n看一个对象有什么本事，看他的构成是什么样，远比看他由哪些行为来的要深远。其实在OOP这种以class方式承载功能的编程中，即看一个类包含的属性，比他的方法也更容易理解对象的作用。看AQS类，暂时抛开outline视图下需要两屏才能看完的重要方法（还未展开ConditionObject和Node两个重要的内部类），只看该类包含的三个重要属性的定义就能看出端倪。\n1private transient volatile Node head; 2 private transient volatile Node tail; 3 private volatile int state; 注释其实已经告诉我们了，Node类型的head和tail是一个FIFO的wait queue；一个int类型的状态位state。到这里也能猜到AQS对外呈现（或者说声明）的主要行为就是由一个状态位和一个有序队列来配合完成。 最简单的读一下主要的四个方法：\n1//获取排他锁 2 public final void acquire(int arg) { 3 if (!tryAcquire(arg) \u0026amp;\u0026amp; 4 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) 5 selfInterrupt(); 6 } 7 8//释放排他锁 9public final boolean release(int arg) { 10 if (tryRelease(arg)) { 11 Node h = head; 12 if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) 13 unparkSuccessor(h); 14 return true; 15 } 16return false; 17 } 18 19 //获取共享锁 20 public final void acquireShared(int arg) { 21 if (tryAcquireShared(arg) \u0026amp;lt; 0) 22 doAcquireShared(arg); 23 } 24//释放共享锁 25public final boolean releaseShared(int arg) { 26 if (tryReleaseShared(arg)) { 27 doReleaseShared(); 28 return true; 29 } 30 return false; 31 } 分别对应锁的获取和释放，只是**shared后缀的表示一组表示共享锁，而另外一组没有后缀的表示排他锁。只用关注每个方法的第一行，都是这种try字体的风格：\n1if (try*****(arg)) { 2} 即做一个判断，然后做获取或者释放锁。 其实AQS主要的工作思路正是如此：在获取锁时候，先判断当前状态是否允许获取锁，若是可以则获取锁，否则获取不成功。获取不成功则会阻塞，进入阻塞队列。而释放锁时，一般会修改状态位，唤醒队列中的阻塞线程。 跟踪这几个try字体的方法定义，发现一个惊人的巧合，这几个方法在AQS中居然都是一样的定义：\n1protected boolean tr***(int arg) { 2 throw new UnsupportedOperationException(); 3 } 即都是父类中只有定义，在子类中实现。子类根据功能需要的不同，有选择的对需要的方法进行实现。父类中提供一个执行模板，但是具体步骤留给子类来定义，不同的子类有不同的实现。\n3. AQS的重要方法定义 简单看下下面几个方法的源码发现定义中都涉及到了getState(),setState(int)，即对状态位state的维护。`\ntryAcquire(int)\ntryRelease(int)\n的调用，可以看的更清楚看到，说明几个同步工具类内定义的Sync类，即自定义子类中其实都涉及到对state的操作。`\n而同时不小心观察到AQS中有一大组final的方法，就是子类不能覆盖的，大致看下方法内的定义，大部分都是直接或间接涉及对head和tail的操作，即对等待队列的维护。\n那在AQS的子类中有没有对有序队列的操作呢？检索下对head和tail的引用即可找到结论。\n对head的操作仅限于在AQS类内部，观察方法的修饰，除了final就是private，即表示这些方法不可能被子类override，或者不可能在子类中直接被调用。看下图对于tail的调用也是同样的风格，即对等待队列的操作全部不超过AQS类内部。\n于是几乎可以有这样的结论：在AQS的设计中，在父类AQS中实现了对等待队列的默认实现，无论是对共享锁还是对排他锁。子类中几乎不用修改该部分功能，而state在子类中根据需要被赋予了不同的意义，子类通过对state的不同操作来提供不同的同步器功能，进而对封装的工具类提供不同的功能。 在下面尝试对以上观点在AQS各个子类在各个工具类中的使用进行验证。\n4. AQS在子类中的使用 对每个考察会从如下几个方面来进行\n工具类的主要作用 主要获取锁方法（其他的类似方法如对应的可以更好的处理中断和超时或者异步等特性） 主要释放锁方法（其他的类似方法如对应的可以更好的处理中断和超时或者异步等特性） 工具类的构造方法（构造方法能告诉我们一个类最在意，最根本的属性） Sync构造方法 Sync接口方法 Sync对AQS方法的override state的作用 state维护重要逻辑 我们的问题就是这些AQS的子类如何配合父类AQS的框架方法来完成各个工具类不同的锁需求。分析思路是这样：\n这个工具类是干什么用的？可以理解为是功能需求。 这个工具类是通过哪些方法来实现这些功能的？可以理解为分解的需求 AQS的子类Sync是如何支持这些方法的？可以理解为需求的实现。 按照如下的思路对每个工具类尝试进行解析，只是注意以上观点，可能并没有覆盖这个工具类的所有内容(其实就是方法)和对应Sync的所有内容。为了表达清楚些，把重点方法的代码引用在文章中，并对重点语句做了标记。因为五钟同步工具类在一起说明，看上去引用的代码有点多。\n1） Semaphore 先看doc中对Semaphore的功能要求：\nA counting semaphore. Conceptually, a semaphore maintains a set of permits. Each acquire blocks if necessary until a permit is available, and then takes it. Each release adds a permit, potentially releasing a blocking acquirer. However, no actual permit objects are used; the Semaphore just keeps a count of the number available and acts accordingly.\n信号量Semaphore的主要作用是来控制同时访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。 Semaphore只是计数，不包括许可对象，并且Semaphore也不会把许可与线程对象关联起来，因此一个线程中获得的许可可以在另外一个线程中释放。关于这点的理解可以参照What is mutex and semaphore in Java ? What is the main difference ?的说明。 Semphore对外的两个方法是 acquire()和release()方法。在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每调用 release() 添加一个许可，释放一个正在阻塞的获取者。\n1public void acquire() throws InterruptedException { 2 sync.acquireSharedInterruptibly(1); 3 } 4 public void release() { 5 sync.releaseShared(1); 6 } 达到这样的操作是通过同步器Sync来操作，可以是FairSync，也可以是NonfairSync。 从Sync的构造方法中，就可以看出Semphore中所谓的permit其实就是AQS中的state。\n1public Semaphore(int permits, boolean fair) { 2 sync = (fair)? new FairSync(permits) : new NonfairSync(permits); 3 } 4 5Sync(int permits) { 6 setState(permits); 7 } 工具类是通过Sync的acquireSharedInterruptibly和ReleaseShared的方法提供功能。AQS中定义的这两个final方法调用的是子类对应的try*方法。在这里覆盖了tryAcquireShared和tryReleaseShared方法。每一次请求acquire()一个许可都会导致计数器减少1，同样每次释放一个许可release()都会导致计数器增加1，一旦达到了0，新的许可请求线程将被挂起。\n1protected final boolean tryReleaseShared(int releases) { 2 for (;;) { 3 int p = getState(); 4 //释放锁时，许可递加 5 if (compareAndSetState(p, p + releases)) 6 return true; 7 } 8 } 每次释放锁时先调用该方法时，作用就修改state值为state+release，即表示增加新释放的许可数。 而tryAcquireShared对应于FairSync，NonfairSync有两种不同的实现。 FairSync中，总是判断当前线程是等待队列的第一个线程时，获得锁，且修改state值为state-acquires。\n1protected int tryAcquireShared(int acquires) { 2 Thread current = Thread.currentThread(); 3 for (;;) { 4 //\u0026amp;nbsp;FairSync中，总是判断当前线程是等待队列的第一个线程时，获得锁 5 Thread first = getFirstQueuedThread(); 6 if (first != null \u0026amp;\u0026amp; first != current) 7 return -1; 8 int available = getState(); 9 //获得锁，则计数递减 10 int remaining = available - acquires; 11 if (remaining \u0026amp;lt; 0 || 12 compareAndSetState(available, remaining)) 13 return remaining; 14 } 15 } 对NonfairSync，不用考虑等待队列，直接修改state许可数。\n1protected int tryAcquireShared(int acquires) { 2 return nonfairTryAcquireShared(acquires); 3 } 4 final int nonfairTryAcquireShared(int acquires) { 5 for (;;) { 6 //对NonfairSync，不用考虑等待队列，直接修改state许可数 7 int available = getState(); 8 int remaining = available - acquires; 9 if (remaining \u0026amp;lt; 0 || 10 compareAndSetState(available, remaining)) 11 return remaining; 12 } 13 } 即不管是公平还是非公平，acquire方法总是会判断是否还有许可可用，如果有，并且当前线程可以获得，则获得锁，许可数相应减少。state在此的作用就是许可数。\n**总结：**在Semaphore中使用AQS的子类Sync，初始化state表示许可数，在每一次请求acquire()一个许可都会导致计数器减少1，同样每次释放一个许可release()都会导致计数器增加1。一旦达到了0，新的许可请求线程将被挂起。\n2） CountDownLatch 要求完成的功能是： A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes. __A CountDownLatch is initialized with a given count. The await methods block until the current count reaches zero due to invocations of the countDown method, after which all waiting threads are released and any subsequent invocations of await return immediately.\n就像名字Latch所表达的一样，把一组线程全部关在外面，在某个状态时候放开。即一种同步机制来保证一个或多个线程等待其他线程完成。初始化了一个count计数，当count未递减到0时候，每次调用**await方法都会阻塞。每次调用countDown**来是的的count递减。 这是CountDownLatch 中“规定”的该工具类应该满足的功能，详细的使用的例子不再此介绍。只是分析如何借助Sync同步器来达到以上功能的。 __ 从构造函数中可以看到该类也维护了一个计数count。这个计数其实也是通过AQS的state来完成的，\n1public CountDownLatch(int count) { 2 if (count \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;count \u0026lt; 0\u0026#34;); 3 this.sync = new Sync(count);} CountDownLatch的两个重要方法是await和countDown方法。定义分别如下。定义await方法的作用是在计数器不为0时候阻塞调用线程，为0时候立即返回；countDown方法的作用是计数递减。\n1public void await() throws InterruptedException { 2 sync.acquireSharedInterruptibly(1); 3 } 4 public void countDown() { 5 sync.releaseShared(1); 6 } 看到这两个方法最终的执行还是同步器中的对应方法。在CountDownLatch中也定义了一个继承于AQS的Sync。在前面的分析中知道父类的acquireSharedInterruptibly方法和releaseShared其实是分别调用到了子类中定义的tryAcquireShared和tryReleaseShared方法。 在CountDownLatch的Sync类中也就仅仅实现了这两个方法。\n其中tryAcquireShared方法内容非常简单，只是一个三元表达式，但是这个state值为0赋值1，不为0却赋值-1。看着不太符合我们一般的用法，这主要是为了配合父类AQS中的逻辑。当state为0表示计数递减完成，则返回值为1，在父类调用中直接方法结束，不阻塞；当state不为0表示计算器递减未完成，则返回值为-1，在父类中满足小于0的条件，执行后续的阻塞操作。\n1public int tryAcquireShared(int acquires) { 2 //当state不为0表示计数递减未完成，则返回值为-1，在父类调用会阻塞 3 return getState() == 0? 1 : -1; 4 } tryReleaseShared方法主要是对state值的维护，当已经为0，则返回false，父类releaseShared方法直接返回；当state不为0（其实就是大于0，因为count初始化是一个正数），则递减，并通过cas的方式更新state的值。\n1public boolean tryReleaseShared(int releases) { 2 // Decrement count; signal when transition to zero 3 for (;;) { 4 int c = getState(); 5 if (c == 0) 6 //当已经为0，则返回false，父类releaseShared方法直接返回 7 return false; 8 //当state不为0（其实就是大于0，因为count初始化是一个正数），则递减，并通过cas的方式更新state的值。 9 int nextc = c-1; 10 if (compareAndSetState(c, nextc)) 11 return nextc == 0; 12 } 13 } 总结：CountDownLatch 委托自定义的Sync中的，await()和**countDown()方法来完成阻塞线程到计数器为0的功能和计数器递减功能。而该这两个方法委托给自定义的Sync的acquireSharedInterruptibly()和releaseShared**(int arg)方法。真正实现对state(count)维护的是父类AQS中调用子类定义的tryAcquireShared(int)和tryReleaseShared(int)来维护计数count。计数count使用的是AQS的状态位state。每次调用countDown方法计数递减，在计数递减到0之前，调用await的线程都会阻塞。`\n3）ReentrantLock 名字翻译很好，可重入锁。功能需求如下 A reentrant mutual exclusion Lock with the same basic behavior and semantics as the implicit monitor lock accessed using synchronized methods and statements, but with extended capabilities._ _A ReentrantLock is owned by the thread last successfully locking, but not yet unlocking it. A thread invoking lock will return, successfully acquiring the lock, when the lock is not owned by another thread. The method will return immediately if the current thread already owns the lock. This can be checked using methods isHeldByCurrentThread, and getHoldCount.\n可重入锁应该是几种同步工具里面被用的对多的一个。标准的互斥操作，也就是一次只能有一个线程持有锁，可能是AQS中最重要的一个类。基本功能就关键字Synchronize所支持的功能。关于ReentrantLock和Synchronize的差别比较等文章很多，可以参照Java 理论与实践: JDK 5.0 中更灵活、更具可伸缩性的锁定机制和《Java Concurrency in Practice》的对应章节。 ReentrantLock对外的主要方法是lock()，tryLock()和unlock()方法，当然还有其他变种的lockInterruptibly()、tryLock(long timeout, TimeUnit unit)等。\n**lock**的功能是获取锁。如果没有线程使用则立即返回，并设置state为1；如果当前线程已经占有锁，则state加1；如果其他线程占有锁，则当前线程不可用，等待。\n1public void lock() { 2 sync.lock(); 3 } tryLock的功能是 如果锁可用，则获取锁，并立即返回值 true。如果锁不可用，立即返回值 false。\n1public boolean tryLock() { 2 return sync.nonfairTryAcquire(1); 3 } unlock的功能是尝试释放锁，如果当前线程占有锁则count减一，如果count为0则释放锁。若占有线程不是当前线程，则抛异常。\n1public void unlock() { 2 sync.release(1); 3 } 可以看到也是借助Sync来完成，我们下面详细看下Sync是如何实现这些”规定”的需求的。ReentrantLock的构造函数告诉我们，其支持公平和非公平两种锁机制。\n1public ReentrantLock(boolean fair) { 2 sync = (fair)? new FairSync() : new NonfairSync(); 3 } 在该类中对应定了两种FairSync和NonfairSync两种同步器，都继承者AQS。可以看到对应执行的是lock、release、和Sync的nonfairTryAcquire。从前面AQS源码知道release是在父类AQS中定义的方法，lock和nonfairTryAcquire是这个Sync中特定的方法，不是对父类对应方法的覆盖。 lock方法有对于FairSync和NoFairSync有两种不同的实现，对于非公平锁只要当前没有线程持有锁，就将锁给当前线程；而公平锁不能这么做，总是调用acquire方法来和其他线程一样公平的尝试获取锁。\n1/**NoFairSync**/ 2 final void lock() { 3 if (compareAndSetState(0, 1)) 4 //对于非公平锁只要当前没有线程持有锁，就将锁给当前线程 5 setExclusiveOwnerThread(Thread.currentThread()); 6 else 7 acquire(1); 8 } 9 /**FairSync**/ 10 final void lock() { 11 acquire(1); 12 } acquire(int arg)方法是在父类AQS中定义，在其实现中先会调用子类的**tryAcquire**(int arg)方法。 对于非公平锁，通过state是否为0判断，当前是否有线程持有锁，如果没有则把锁分配给当前线程；否则如果state不为0，说明当前有线程持有锁，则判断持有锁的线程是否就是当前线程，如果是增加state计数，表示持有锁的线程的重入次数增加。当然增加重入数也会检查是否超过最大值。\n1protected final boolean tryAcquire(int acquires) { 2 return nonfairTryAcquire(acquires); 3 } 4final boolean nonfairTryAcquire(int acquires) { 5 final Thread current = Thread.currentThread(); 6 int c = getState(); 7 if (c == 0) { 8 //通过state是否为0判断，当前是否有线程持有锁，如果没有则把锁分配给当前线程 9 if (compareAndSetState(0, acquires)) { 10 setExclusiveOwnerThread(current); 11 return true; 12 } 13 } 14 else if (current == getExclusiveOwnerThread()) { 15 //否则如果state不为0，说明当前有线程持有锁，则判断持有锁的线程是否就是当前线程，如果是增加state计数，表示持有锁的线程的重入次数增加 16 int nextc = c + acquires; 17 if (nextc \u0026amp;lt; 0) // overflow 18 throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); 19 setState(nextc); 20 return true; 21 } 22 return false; 23 } 对于公平锁，其**tryAcquire**(int arg)方法中，如果state为0表示没有线程持有锁，会检查当前线程是否是等待队列的第一个线程，如果是则分配锁给当前线程；否则如果state不为0，说明当前有线程持有锁，则判断持有锁的线程释放就是当前线程，如果是增加state计数，表示持有锁的线程的重入次数增加。\n1protected final boolean tryAcquire(int acquires) { 2 final Thread current = Thread.currentThread(); 3 int c = getState(); 4 if (c == 0) { 5 if (isFirst(current) \u0026amp;\u0026amp; 6 compareAndSetState(0, acquires)) { 7 //如果state为0表示没有线程持有锁，会检查当前线程是否是等待队列的第一个线程，如果是则分配锁给当前线程 8 setExclusiveOwnerThread(current); 9 return true; 10 } 11 } 12 else if (current == getExclusiveOwnerThread()) { 13 //如果state不为0，说明当前有线程持有锁，则判断持有锁的线程释放就是当前线程，如果是增加state计数，表示持有锁的线程的重入次数增加 14 int nextc = c + acquires; 15 if (nextc \u0026amp;lt; 0) 16 throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); 17 setState(nextc); 18 return true; 19 } 20 return false; 21 } 比较公平锁机制和非公平锁机制的差别仅仅在于如果当前没有线程持有锁，是优先把锁分配给当前线程，还是优先分配给等待队列中队首的线程。 释放锁时候调用AQS的**release(int arg)方法，前面定义知道父类的该方法会先调用子类的tryRelease**(int arg)方法。在该方法中主要作用是state状态位减少release个，表示释放锁，如果更新后的state为0；表示当前线程释放锁，如果不为0，表示持有锁的当前线程重入数减少。\n1protected final boolean tryRelease(int releases) { 2 int c = getState() - releases; //state状态位减少release个 3 if (Thread.currentThread() != getExclusiveOwnerThread()) 4 throw new IllegalMonitorStateException(); 5 boolean free = false; 6 if (c == 0) { 7 //如果更新后的state为0，表示当前线程释放锁 8 free = true; 9 setExclusiveOwnerThread(null); 10 }//如果不为0，表示持有锁的当前线程重入数减少。 11 setState(c); 12 return free; 13 } 总结： ReentrantLock中定义的同步器分为公平的同步器和非公平的同步器。在该同步器中state状态位表示当前持有锁的线程的重入次数。在获取锁时，通过覆盖AQS的**tryAcquire(int arg)方法，如果没有线程持有则立即返回，并设置state为1；如果当前线程已经占有锁，则state加1；如果其他线程占有锁，则当前线程不可用。释放锁时，覆盖了AQS的tryRelease**(int arg)，在该方法中主要作用是state状态位减少release个，表示释放锁，如果更新后的state为0，表示当前线程释放锁，如果不为0，表示持有锁的当前线程重入数减少。\n4）ReentrantReadWriteLock可重入读写锁 读写锁的要求是： A ReadWriteLock maintains a pair of associated locks, one for read-only operations and one for writing. The read lock may be held simultaneously by multiple reader threads, so long as there are no writers. The write lock is exclusive. All ReadWriteLock implementations must guarantee that the memory synchronization effects of writeLock operations (as specified in the Lock interface) also hold with respect to the associated readLock. That is, a thread successfully acquiring the read lock will see all updates made upon previous release of the write lock.\n即读和读之间是兼容的，写和任何操作都是排他的。这种锁机制在数据库系统理论中应用的其实更为普遍。 允许多个读线程同时持有锁，但是只有一个写线程可以持有锁。读写锁允许读线程和写线程按照请求锁的顺序重新获取读取锁或者写入锁。当然了只有写线程释放了锁，读线程才能获取重入锁。写线程获取写入锁后可以再次获取读取锁，但是读线程获取读取锁后却不能获取写入锁。 ReentrantReadWriteLock锁从其要求的功能上来看，是对前面的ReentrantLock的扩展，因此功能复杂度上来说也提高了，看看该类下面定义的内部类，除了支持公平非公平的Sync外，还有两种不同的锁，ReadLock和WriteLock。\n在向下进行之前，有必要回答这样一个问题，WriteLock和ReadLock好像完成的功能不一样，看上去似乎是两把锁。ReentrantReadWriteLock中分别通过两个public的方法**readLock()和writeLock**()获得读锁和写锁。\n1private final ReentrantReadWriteLock.ReadLock readerLock; 2 private final ReentrantReadWriteLock.WriteLock writerLock; 3 private final Sync sync; 4 public ReentrantReadWriteLock.WriteLock writeLock() { return writerLock; } 5 public ReentrantReadWriteLock.ReadLock readLock() { return readerLock; } 但是如果是两把锁，可以实现前面功能要求的读锁和读锁直接的兼容，写锁和写锁直接的互斥，这本身共享锁和排他锁就能满足要求，但是如何实现对同一个对象上读和写的控制？明显，只有一把锁才能做到。 看上面代码片段时候，不小心看到了一个熟悉的字段Sync，前面的几个同步工具我们知道了，这些工具类的所有操作最终都是委托给AQS的对应子类Sync来完成，这里只有一个同步器Sync，那是不是就是只有一把锁呢。看看后面的构造函数会验证我们的猜想。\n1public ReentrantReadWriteLock(boolean fair) { 2 sync = (fair)? new FairSync() : new NonfairSync(); 3 //使用了同一个this，即统一this里面的同一个sync来构造读锁和写锁 4 readerLock = new ReadLock(this); 5 writerLock = new WriteLock(this); 6 } 没错，ReadLock和WriteLock使用的其实是一个private的同步器Sync。 下面看下可重入读写锁提供哪些锁的方法来满足上面的需求的。\n看到ReadLock提供了**lock()、lockInterruptibly()、tryLock()、tryLock(long timeout, TimeUnit unit)和unlock()方法。我们看下主要的几个方法的实现如下：lock()方法的作用是获取读锁；tryLock()的作用是尝试当前没有其他线程当前持有写锁时获取读锁；unlock**方法的作用是释放读锁。\n1public void lock() { 2 sync.acquireShared(1); 3 } 4 public boolean tryLock() { 5 return sync.tryReadLock(); 6 } 7 public void unlock() { 8 sync.releaseShared(1); 9 } 分别调用到Sync的三个方法**acquireShared(int arg) 、releaseShared(int arg)和 tryReadLock()方法，其中前两个是AQS父类中定义的，后一个是该Sync中根据自己需要实现的方法。 前面AQS父类的介绍中知道，acquireShared(int arg) 和releaseShared**(int arg)方法是在父类中定义的，调用子类的对应try字体的方法，我们看下在子类Sync中定义对应的try*字体的方法怎么满足功能的。先看acquireShared中定义的tryAcquireShared\n1protected final int tryAcquireShared(int unused) { 2 Thread current = Thread.currentThread(); 3 int c = getState(); 4 if (exclusiveCount(c) != 0 \u0026amp;\u0026amp; 5 getExclusiveOwnerThread() != current) 6 //如果有排他锁，且持有排他锁的线程不是当前线程，则获取失败。 7 return -1; 8 if (sharedCount(c) == MAX_COUNT) 9 //否则如果已经加读锁的个数超过允许的最大值，抛出异常 10 throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); 11 if (!readerShouldBlock(current) \u0026amp;\u0026amp; 12 compareAndSetState(c, c + SHARED_UNIT)) { 13 //否则检查是否需要阻塞当前线程，如果不阻塞，则使用CAS的方式给更新状态位state。其中readerShouldBlock在Sync的两个子类中实现，根据公平非公平的策略有不同的判断条件 14 HoldCounter rh = cachedHoldCounter; 15 if (rh == null || rh.tid != current.getId()) 16 cachedHoldCounter = rh = readHolds.get(); 17 rh.count++; 18 return 1; 19 } 20 return fullTryAcquireShared(current); 21 } 尝试获取读锁的方法是这样的：如果有排他锁，但是持有排他锁的线程不是当前线程，则获取失败；否则如果已经加读锁的个数超过允许的最大值，抛出异常；否则检查是否需要阻塞当前线程，如果不阻塞，则使用CAS的方式给更新状态位state。其中readerShouldBlock在Sync的两个子类中实现，根据公平非公平的策略有不同的判断条件。\n对应的releaseShared中调用的tryReleaseShared定义如下\n1protected final boolean tryReleaseShared(int unused) { 2 HoldCounter rh = cachedHoldCounter; 3 Thread current = Thread.currentThread(); 4 if (rh == null || rh.tid != current.getId()) 5 rh = readHolds.get(); 6 if (rh.tryDecrement() \u0026amp;lt;= 0) 7 throw new IllegalMonitorStateException(); 8 for (;;) { 9 int c = getState(); 10 // 释放读锁时更新状态位的值 11 int nextc = c - SHARED_UNIT; 12 if (compareAndSetState(c, nextc)) 13 return nextc == 0; 14 } 15 } 可以看到主要的作用在准备释放读锁时更新状态位的值。 Sync中提供给ReadLock用的tryReadLock方法和tryAcquireShared内容和逻辑差不多，而且本文想着重分析的Sync对父类AQS的方法如何改变来达到需要的功能，所以这个方法这里不分析了。 可以看到加锁时候state增加了一个SHARED_UNIT，在释放锁时state减少了一个SHARED_UNIT。为什么是SHARED_UNIT，而不是1呢？这个看了下面两个方法的定义就比较清楚了。\n1/** Returns the number of shared holds represented in count */ 2 static int sharedCount(int c) { return c \u0026amp;gt;\u0026amp;gt;\u0026amp;gt; SHARED_SHIFT; } 3 /** Returns the number of exclusive holds represented in count */ 4 static int exclusiveCount(int c) { return c \u0026amp; EXCLUSIVE_MASK; } 原来为了只用一个state状态位来表示两种锁的信息，高位16位表示共享锁的状态位，低位16位表示独占锁的状态位。至于读锁和写锁的状态位的意思，随着后面分析会逐步更清楚。 在看到这里的时候，读锁的状态位的意思应该是比较清楚，表示当前持有共享锁的线程数。有一个新的线程过了想使用共享锁，如果其他线程也只是加了共享锁，则当前线程就可以加共享锁，每加一次，状态位递加一些，因为存储在高16位，所以递加时是加一个SHARED_UNIT。\n接着关注下WriteLock的方法。和 ReadLock 类似，提供出来的还是lock()、tryLock()、unlock()三个和其相似方法。\n1public void lock() { 2 sync.acquire(1); 3 } 4public boolean tryLock( ) { 5 return sync.tryWriteLock(); 6 } 7public void unlock() { 8 sync.release(1); 9 } 看到分别调用了Sync的acquire() release() 和tryWriteLock方法，其中前两个都是定义在父类AQS的方法。调用了子类定义的对应try字体的方法。tryAcquire和tryRelease方法。这里我们就看下子类的这两个try*字体的方法做了哪些事情。\n1protected final boolean tryAcquire(int acquires) { 2 Thread current = Thread.currentThread(); 3 int c = getState(); 4 int w = exclusiveCount(c); 5 if (c != 0) { 6 //通过state的判断，当有读锁时获取不成功 7 //当有写锁，如果持有写锁的线程不是当前线程，则获取不成功 8 // (Note: if c != 0 and w == 0 then shared count != 0) 9 if (w == 0 || current != getExclusiveOwnerThread()) 10 return false; 11 if (w + exclusiveCount(acquires) \u0026amp;gt; MAX_COUNT) 12 throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); 13 }//如果可以获取，则CAS的方式更新state，并设置当前线程排他的获取锁 14 if ((w == 0 \u0026amp;\u0026amp; writerShouldBlock(current)) || 15 !compareAndSetState(c, c + acquires)) 16 return false; 17 setExclusiveOwnerThread(current); 18 return true; 19 } tryAcquire中尝试获取排他锁。结合排他锁的语义和代码逻辑不难看到：通过state的判断，当有读锁时获取不成功，当有写锁，如果持有写锁的线程不是当前线程，则获取不成功。如果可以获取，则CAS的方式更新state，并设置当前线程排他的获取锁。writerShouldBlock定义在Sync的子类中，对于FaireSync和UnFairSync有不同的判断。 接下来看tryRelease方法，主要作用是在释放排他锁时候更新state，减去releases的数目。看到这里发现写锁中用到的Sync和可重入锁ReentrantLock整个逻辑都对应的差不多。\n1protected final boolean tryRelease(int releases) { 2 //释放排他锁时候更新state，减去releases的数目 3 int nextc = getState() - releases; 4 if (Thread.currentThread() != getExclusiveOwnerThread()) 5 throw new IllegalMonitorStateException(); 6 if (exclusiveCount(nextc) == 0) { 7 setExclusiveOwnerThread(null); 8 setState(nextc); 9 return true; 10 } else { 11 setState(nextc); 12 return false; 13 } 14 } 只是观察到写锁state更新加和减和前面的几种比较类似，直接操作的就是传入的整形参数，这在读锁的时候讲过了，因为排他锁的状态位是存储在state的低16位。\n总结： ReentrantReadWriteLock中提供了两个Lock：ReentrantReadWriteLock.ReadLock和ReentrantReadWriteLock.WriteLock。对外提供功能的是两个lock，但是内部封装的是一个同步器Sync，有公平和不公平两个版本。借用了AQS的state状态位来保存锁的计数信息。高16位表示共享锁的数量，低16位表示独占锁的重入次数。在AQS子类的对应try字体方法中实现对state的维护。\n5）FutureTask 先看需求 A cancellable asynchronous computation. This class provides a base implementation of Future, with methods to start and cancel a computation, query to see if the computation is complete, and retrieve the result of the computation. The result can only be retrieved when the computation has completed; the get method will block if the computation has not yet completed. Once the computation has completed, the computation cannot be restarted or cancelled.\n理解其核心需求是，一个执行任务，开始执行后可以被取消，可以查看执行结果，如果执行结果未完成则阻塞。 一般表示一个输入待执行任务。在线程池中FutureTask中一般的用法就是构造一个FutureTask，然后提交execute，返回的类型还是FutureTask，调用其get方法即可得到执行结果。 run方法定义的就是任务执行的内容，在工作线程中被调用。通过构造函数可以看到FutureTask封装的了一个Runnable的对象，另外一个泛型参数result。猜也可以猜到前者就是执行的任务内容，后者是来接收执行结果的。可以看到功能还是委托给Sync对象，构造的参数是一个有执行结果的调用Callable，也可以直接使用一个Callable参数。\n1public FutureTask(Runnable runnable, V result) { 2 sync = new Sync(Executors.callable(runnable, result)); 3 }public FutureTask(Runnable runnable, V result) { 4 sync = new Sync(Executors.callable(runnable, result)); 5 } 6 public FutureTask(Callable\u0026lt;V\u0026gt; callable) { 7 if (callable == null) 8 throw new NullPointerException(); 9 sync = new Sync(callable); 10} FutureTask实现了RunnableFuture接口，也即实现了Runnable和Future接口。作业线程执行的内容是FutureTask的的run方法内定义的任务内容。如线程池ThreadPoolExecutor.Worker.runTask(Runnabletask)方法可以看到在线程池的Worker线程中调用到执行任务的run方法。这里使用Sync的作用，就是在任务执行线程和提交任务（同时也是获取任务执行结果）的线程之间维持一个锁的关系，保证只有执行结束后才能获取到结果。\nFutureTask的任务执行方法是\n1public void run() { 2 sync.innerRun(); 3} 获取执行结果的方法是\n1public V get() throws InterruptedException, ExecutionException { 2 return sync.innerGet(); 3 } 设置执行结果的方法是\n1protected void set(V v) { 2 sync.innerSet(v); 3 } 能看到，都是调到对应的Sync的对应方法。最主要的是innerRun方法，通过CAS的方式设置任务执行状态位RUNNING，执行传入的回调，并把执行结果调用innerSet进行赋值。\n1void innerRun() { 2 //设置任务执行状态位RUNNING 3 if (!compareAndSetState(0, RUNNING)) 4 return; 5 try { 6 runner = Thread.currentThread(); 7 if (getState() == RUNNING) // recheck after setting thread 8 //获取和设置回调的结果 9 innerSet(callable.call()); 10 else 11 releaseShared(0); // cancel 12 } catch (Throwable ex) { 13 innerSetException(ex); 14 } 15 } 在innerSet方法中设置执行状态位为执行结束，并把执行结果赋值给result。\n1void innerSet(V v) { 2\tfor (;;) { 3\tint s = getState(); 4\tif (s == RAN) 5\treturn; 6 if (s == CANCELLED) { 7\treleaseShared(0); 8 return; 9 } 10 //设置执行状态位为执行结束，并把执行结果赋值给result 11\tif (compareAndSetState(s, RAN)) { 12 result = v; 13 releaseShared(0); 14 done(); 15\treturn; 16 } 17 } 18 } 前面方法把执行结果放在result中，我们知道future接口定义的get方法来获取执行结果，那如何来判断另外一个线程已经执行完毕呢？看到FutureTask的get方法还是调用到Sync的innerGet方法。 innerGet方法根据判断执行状态来获取执行结果。acquireSharedInterruptibly方法其实调用的是子类中定义的tryAcquireShared来判断任务释放执行完毕或者取消。如果未完毕或取消，则挂起当前线程。\n1V innerGet() throws InterruptedException, ExecutionException { 2 //acquireSharedInterruptibly方法其实调用的是子类中定义的tryAcquireShared来判断任务释放执行完毕或者取消。如果未完毕或取消，则挂起当前线程 3 acquireSharedInterruptibly(0); 4 if (getState() == CANCELLED) 5 throw new CancellationException(); 6 if (exception != null) 7 throw new ExecutionException(exception); 8 return result; 9 } tryAcquireShared方法的定义如下，调用innerIsDone方法，根据state的状态值做出判断，如果结束则返回1，未结束返回-1。当tryAcquireShared返回-1，则在父类AQS中获取共享锁的线程会阻塞。即实现“任务未完成调用get方法的线程会阻塞”这样的功能。\n1protected int tryAcquireShared(int ignore) { 2 //调用innerIsDone方法，根据state的状态值做出判断，如果结束则返回1，未结束返回-1。当tryAcquireShared返回-1，则在父类AQS中获取共享锁的线程会阻塞。 3 return innerIsDone()? 1 : -1; 4 } 5 boolean innerIsDone() { 6 return ranOrCancelled(getState()) \u0026amp;\u0026amp; runner == null; 7 } 8 private boolean ranOrCancelled(int state) { 9 return (state \u0026amp; (RAN | CANCELLED)) != 0; 10 } tryReleaseShared没有做什么事情，因为不像前面四种其实都有锁的意味，需要释放锁。在FutureTask中state表示任务的执行状态，在几乎每个方法的开始都会判读和设置状态。\n1protected boolean tryReleaseShared(int ignore) { 2 runner = null; 3 return true; 4 } **总结：**在FutureTask实现了异步的执行和提交，作为可以被Executor提交的对象。通过Sync来维护任务的执行状态，从而保证只有工作线程任务执行完后，其他线程才能获取到执行结果。AQS的子类Sync在这里主要是借用state状态位来存储执行状态，来完成对对各种状态以及加锁、阻塞的实现。\n最后终于理解了这早前就算了解的类，名字为什么叫FutureTask，实现了Future接口（满足在future的某个时间获取执行结果，这是Future接口的取名的意义吧），另外在执行中作为对执行任务的封装，封装了执行的任务内容，同时也封装了执行结果，可以安全的把这个任务交给另外的线程去执行，只要执行get方法能得到结果，则一定是你想要的结果，真的是很精妙。\n5. 总结对照 本文主要侧重AQS的子类在各个同步工具类中的使用情况，其实也基本涵盖了这几个同步工具类的主要逻辑，但目标并不是对这几个同步工具类的代码进行详细解析。另外AQS本身的几个final方法，才是同步器的公共基础，也不是本文的主题，也未详细展开。其实写这篇文章的一个初始目的真的只是想列出如下表格，对比下AQS中的各个子类是怎么使用state的，居然啰嗦了这么多。\n工具类 工具类作用 工具类加锁方法 工具类释放锁方法 Sync覆盖的方法 Sync非覆盖的重要方法 state的作用 锁类型 锁维护 Semaphore 控制同时访问某个特定资源的操作数量 acquire：每次请求一个许可都会导致计数器减少1，，一旦达到了0，新的许可请求线程将被挂起 release：每调用 添加一个许可，释放一个正在阻塞的获取者 tryAcquireShared tryReleaseShared 表示初始化的许可数 共享锁 每一次请求acquire()一个许可都会导致计数器减少1，同样每次释放一个许可release()都会导致计数器增加1，一旦达到了0，新的许可请求线程将被挂起。 CountDownLatch 把一组线程全部关在外面，在某个状态时候放开。一种同步机制来保证一个或多个线程等待其他线程完成。 await：在计数器不为0时候阻塞调用线程，为0时候立即返回 countDown ：计数递减 tryAcquireShared tryReleaseShared 维护一个计数器 共享锁 初始化一个计数，每次调用countDown方法计数递减，在计数递减到0之前，调用await的线程都会阻塞 ReentrantLock 标准的互斥操作，也就是一次只能有一个线程持有锁 lock：如果没有线程使用则立即返回，并设置state为1；如果当前线程已经占有锁，则state加1；如果其他线程占有锁，则当前线程不可用，等待 tryLock：如果锁可用，则获取锁，并立即返回值 true。如果锁不可用，则此方法将立即返回值 false unlock：尝试释放锁，如果当前线程占有锁则count减一，如果count为0则释放锁。如果占有线程不是当前线程，则抛异常 tryAcquire tryRelease nonfairTryAcquir state表示获得锁的线程对锁的重入次数。 排他锁。 获取锁时，如果没有线程使用则立即返回，并设置state为1；如果当前线程已经占有锁，则state加1；如果其他线程占有锁，则当前线程不可用。释放锁时，在该方法中主要作用是state状态位减少release个，表示释放锁，如果更新后的state为0，表示当前线程释放锁，如果不为0，表示持有锁的当前线程重入数减少 ReentrantReadWriteLock 读写锁。允许多个读线程同时持有锁，但是只有一个写线程可以持有锁。写线程获取写入锁后可以再次获取读取锁，但是读线程获取读取锁后却不能获取写入锁 ReadLock#lock ：获取读锁 ReadLock#tryLock:尝试当前没有其他线程当前持有写锁时获取读锁 WriteLock#lock：获取写锁 WriteLock#tryLock：尝试当前没有其他线程持有写锁时，呼气写锁。 ReadLock#unlock：释放读锁 WriteLock#unlock：释放写锁 acquireShared releaseShared tryAcquire tryRelease tryReadLock tryWriteLock 高16位表示共享锁的数量，低16位表示独占锁的重入次数 读锁：共享 写锁：排他 对于共享锁，state是计数器的概念。一个共享锁就相对于一次计数器操作，一次获取共享锁相当于计数器加1，释放一个共享锁就相当于计数器减1；排他锁维护类似于可重入锁。 FutureTask 封装一个执行任务交给其他线程去执行，开始执行后可以被取消，可以查看执行结果，如果执行结果未完成则阻塞。 V get() run() set(V) cancel(boolean) tryAcquireShared tryReleaseShared innerGet innerRun() innerSet innerIsCancelled state状态位来存储执行状态RUNNING、RUN、CANCELLED 共享锁 获取执行结果的线程(可以有多个)一直阻塞，直到执行任务的线程执行完毕，或者执行任务被取消。 ","link":"https://idouba.com/sync-implementation-by-aqs/","section":"posts","tags":["AQS","并发","java"],"title":"源码剖析AQS在几个同步工具类中的使用"},{"body":"","link":"https://idouba.com/tags/12306/","section":"tags","tags":null,"title":"12306"},{"body":"前言 快过年了，又到了一年抢票时。今年douba和douma计划要带着doudou回姥姥家。昨天在家用抢票软件居然发现了一个bug，那就是在猎豹抢票中跨站推荐的车票几天里一直是没有，但是在12306手动尝试不同的跨站可以买到票，怀疑是猎豹在处理车次信息的时候对于变化的车次没有考虑到所致。在文中以实际操作尝试对这个bug做个比较详细的描述，并加上一点定位和分析，希望可以帮助这款神器的使用者和开发者提供些有用信息。按照douma的指示，今天上班来中午吃完饭不休息了，匆匆写下发表出来，供其他焦急的抢票战友们借鉴。不要只是懒懒的用工具刷，过于依赖神器，该动手时候要动动手才有可能抢到票。祝大家都能抢到心仪的车票。\n正文 在本次战役中，除了360三代外，douma推荐了另外一款抢票神器猎豹。并用自己的成功经验诠释了下该神器的与其他工具的比较优势，那就是除了普通工具都有的自动刷票功能外，还提供了一款推荐功能。可以提示用户购买超出区间的余票，并很nice很体贴的提示出票价超了多少（类似信息检索技术中的查询扩展，都是为了提高查全率）。比如douma本来要买两张hangzhou’到yangzhou的软卧，没有买到，就通过该功能成功的抢了两张wenzhou发车的票，尽管多付了66%的票款（这个败家媳妇！），但是能抢到就是很了不起了！尤其据douma的消息，今年除了起点站很多过路的票都很难抢，因此这个跨站抢票的功能就显得尤为重要了。可是就是这个重要功能，在后续使用中被发现存在问题。\n\u0026nbsp;\n本来douma的两张超出常规票价66%的两张软卧（败家媳妇，说到这儿就得再骂一遍）可以把doudou和douba带到姥姥家。可是偏偏doudou的姥姥家住的就是这么偏僻，下了火车还要再倒一班3个半小时的短途火车。就是这两张车票让douma和douba抢了三四天都没有抢到。怎么想这个短途车也不至于这么热吧？打了电话12306，声音甜美的美眉客服也是说票都被秒杀啦。但是昨天在家里打开笔记本打算继续扫之前，douba无意做了个操作，却发现了个神奇的现象。\n360和猎豹显示“本次车票已经卖完了”。12306其他地方也是显示yanzhou这个过站没有票。说明确实过站的票是没有了。\n然后当然是尝试跨站的起点站购票了。douma天天都盯着猎豹的跨站推荐的地方。但是三天里推荐功能一直显示：“订票助手正在为您监控以下跨站票”。让douma望眼欲穿！看来douma比较拿手的始发站下手的策略也没有办法实施。难道这个票从“zaozhuangxi”就卖完啦？douma一个本地人非常不解，这个车以前好像都是没人坐吧，又慢又不好的，要不是带着doudou等大巴麻烦，我才不选这个破车呢。douma领着douba和doudou回家从yanzhou到juxian就这一班火车的哦。急死了！\n就这样焦急了三天，天天上班douba和douma都是到座位开机后不忘把两个神器开起来，然后才忙别的，期间还会QQ、电话互通下进展。晚上下班回家，第一件事情也是把doudou先放一边，打开神器再忙别的。一天一天，douma都要放弃了，也就商量着直接打个车得了。\n直到周末，准确说是周日昨天（周六都懒得搞了），吃完饭开机搞别的，居然被douba轻松手动的从12306的官方售票处买到了这趟车的车票。你能想到在douba支付宝支付的时候，douma和doudou那敬仰的神态吗？家长就应该是这个样子滴！截至到douba这会儿在写这个小文章，去12306上截个图，发现这趟车依然有票在买，从放票开始已经过去整整5天了，票还没卖完。也验证了douma的了解，这趟车确实没有那么热，问题就是为什么猎豹推荐的zaozhuangxi始发的车就是没有呢？结论当然是抢票神器出了问题。\n为什么好几天了直到现在12306一直说有票，抢票神器却说没票呢？“应该是这个软件有bug”，CS工学硕士douma第一时间给出了结论，非常镇定，当时的神态都非常认真和专业，尽管不是在公司，而是穿着拖鞋在家里客厅。\ndouma说的对，但是分析是什么bug呢？就需要一定的业务知识的支持，这个就要请教在读研搞CS前曾经修过铁路的douba了。应该是一个非常tricky的问题，抢票神器可能没有考虑到。那就是车次的问题，细心的朋友应该看到，尽管douma没有选车次，但是神器推荐的是5037?，而douba在12306上买到的是5036。不是Yanzhou到juxian只有一趟车吗？怎么有两个车次?\n随即douba打开12306的列车信息演示给douma看，原来这是一趟车。\n我们都是有文化的人，但是看到车次这一列，想必大部分人都会“蛋疼了”。有这么折腾人的吗？一趟车走早上七点到八点半叫5036，走到九点就叫5037，再走到中午12点又叫5036，走了三个多小时后，到了下午三点多又叫5037。能不能不要这么任性啊！你都不怕列车广播车次的时候把刚上车的大叔大妈吓得认为坐错车了。反正douma看见这张表是晕菜了。\n再听douba啰嗦下《铁路概论》课本上学到的规定的我国的火车车次的编排规则：\n我国火车车次的编制和上行下行有关。铁路规定进京方向或是从支线到干线被称为上行，反之离京方向或是从干线到支线被称为下行。上行的列车车次为偶数（双数），下行的列车车次为奇数（单数）。在铁路调度中，调度员能一看车次就知道行驶方向,十分方便。因此就会出现有的车在运行途中会因为线路上下行的改变而改变车次。例如：1392/1393 到天津前是往北京方向，天津站后是远离北京方向。所以这趟火车有两个车次编号。\n同样的5036/5037也是。看看这个地图中这趟车的行进线路上和我们伟大祖国的首都的相对位置变化就不难理解其为什么走俩小时变个车次，走俩小时又变一下了。\n说到这里，douma大概理解了抢票神器中的问题。“猎豹应该就是只是按照5037进行查询了”。然后douma完整了描述了下bug：“当用户在神器中推荐功能其实是神器自己列举了若干个站到站查询，但是进行这步站到站查询的时候，带上了车次的的信息，不巧这个车次在后面站是存在的，在这个延伸的站是不存在的。即使用的车次不是这趟车的真正的唯一标识，而只是一个看上去的唯一标识。”\n说白了用户查yanzhou到juxian的列车，神器先得到车次信息是5037，然后跨站推荐中把查询的起始和终点站向前后延伸，尝试做二次查询。如尝试起点站zaozhuangxi到juxian，但是这个时候却错误的带上了前面的车次信息。两个条件与在一起就查不出来了。在douma的例子中，按照5037去跨站搜，发现zaozhuangxi根本就没有该趟车，不是卖完了，是从来就没有过！以为这趟车在zaozhuangxi人家叫5036。所以就会出现例子中douba在12306上输入zaozhuangxi到juxiain一直有票，而猎豹刷票推荐了好几条，跨站的地方总是zaozhuangxi到juxian没票。\n试想如果该趟车在铁路管理系统中的一条记录如下:\nId 车次 起点站 终点站 运行时间 里程 硬卧下 … 889 5036/5037 枣庄西 烟台 11.小时 710 170 890 K112 贵阳 上海南 25小时 1982 416 … 891 … 如果我们用889来标识这趟车是没有问题，如果我们用5036/5037这九个字符来标识这趟车也没有问题，但是如果我们只是用5036或5037这其中四个字符来标识这趟车就有问题了。这样的全称来表达一辆连续的列车是合适的，看好多网站上上车票信息的查询中对车次的标示用的就是用5036/5037这样的标示，而不是一个车次来标示来查。\n铁路上是通过运行区间来管理的（以前一个抬杠的熟语是说“铁路警察是管长不管宽”说的就是这个道理），狭长的铁路必须分而治之，因而就有了北京段、济南段这样的分段，但是把运行（现在应该讲是飞驰）在其上的火车也分段命名仅仅是为了调度上好区分而搞单双号，douba一个从铁路系统上跑出来的程序员认为是有点过时了。\n但是对于开发神器这个系统的开发者来说这个明显是用户现状了，我们开发者不大能指望客户做多大调整，更何况当你的客户是中国铁路这么一个有深远历史和影响的庞然大物了，所以只能是系统来适应。\n前面报bug，包括bug描述以及截图都应该是完整、精确、并且尽可能做到可以reproduct的。但是后面的分析，是douba为了博得doudou和douma再次的敬仰当场发挥的，也就老婆孩子面前装13的，完全是基于自己的理解和分析。可能是跑偏了，只能算是对发现的bug做了个简单的外围的分析和定位吧。但求开发神器的伟大攻城狮们了解到，快点改好。douma还惦记着用神器买返程票呢。作为同行，代表doudou和douma对你们的工作表示由衷的感谢，这才是有用的东西，为我们这些屌丝大众造福的好产品。\n另外对于大多数和douma一样的急着抢票的神器用户，douba想说，人家 12306官方都说了大家不要使用刷票软件，也就悠着点用吧，千万不要太迷信刷票软件，该动手时实施自己动手尝试下，说不定会有惊喜哦。\n完。\n","link":"https://idouba.com/one-new-bug-report-of-liebao-huochepiao/","section":"posts","tags":[12306,"随笔"],"title":"实际抢票体验描述和分析“猎豹抢票跨站推荐功能有票刷不到”的疑似bug"},{"body":"","link":"https://idouba.com/tags/oracle/","section":"tags","tags":null,"title":"Oracle"},{"body":"一、前言 在一个有30亿条数据的大表上分页，为了对方案进行性能测试，先忽略其他条件查询的影响，单看下分页部分的性能，顺便考察说明下oracle中rownum使用中一些比较tricky的地方。 实验条件： 表结构如下，内有2千万条实验数据。\n二、实验 提供7种不同方式（其实是5种，二和四是一种、三和五是一种）方式的 。第一种只是为了demo一下假设的一种错误逻辑方式，第二种和第四种是一种逻辑正确，但是性能极差的方式。筛选下来看上去性能可行的方式是第五、第六、第七方式。 这里仅仅记录没中方式的执行结果和计划。\n方式1 笨笨的想想。Oracle里面不是有个变量叫rownum，顾名思义，就是行号的意思，我要获取第十行到第二十行的数据，sql写起来很精练！比myslq的limit和mssql的top折腾看着还要优雅！\n1select * from idouba.APP_CLUSTEREDAUDITLOG where rownum between 10 and 20 喔！十条记录执行了十分钟还么有结果，一定是哪儿有问题了，shut了重试。那就来个简单的：\n1select * from idouba.APP_CLUSTEREDAUDITLOG where rownum =10 也没有记录，再尝试rownum=2都不会有记录。 分析rownum的原理就不难理解。rownum是查询到的结果集中的一个伪列，可以理解成在我们查询到的结果上加序号。按照这个逻辑，写rownum=1是能得到结果集的第一行。执行rownum=2时，先比较第一行，其rownum是1，则扔掉，考察下一行，rownum又是1，直到扫描完整个表，没有满足条件的结果集。 查询计划如下。\n已用时间: 00: 04: 01.81\n1执行计划 2---------------------------------------------------------- 3Plan hash value: 2858290634 4-------------------------------------------------------------------------------- 5| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 6-------------------------------------------------------------------------------- 7| 0 | SELECT STATEMENT | | 20M| 146G| 102K (1)| 00:20:29 | 8| 1 | COUNT | | | | | | 9|* 2 | FILTER | | | | | | 10| 3 | INDEX FAST FULL SCAN| PK_ID | 20M| 146G| 102K (1)| 00:20:29 | 11-------------------------------------------------------------------------------- 12Predicate Information (identified by operation id): 13--------------------------------------------------- 14 2 - filter(ROWNUM=2) 15Note 16----- 17 - dynamic sampling used for this statement 18 19统计信息 20---------------------------------------------------------- 21 0 recursive calls 22 0 db block gets 23 461958 consistent gets 24 221499 physical reads 25 0 redo size 26 1956 bytes sent via SQL*Net to client 27 374 bytes received via SQL*Net from client 28 1 SQL*Net roundtrips to/from client 29 0 sorts (memory) 30 0 sorts (disk) 31 0 rows processed\u0026lt;/pre\u0026gt; 执行了4分钟，没有得到一条记录。尝试下面的方法。\n方法2 明白了rownum的意思，意识到解决问题的办法，是再加一层查询，即里面括号的是我们要的数据，然后从上面选择rownum，其实就是行号为10到20的数据行。\n1select * 2 from (select rownum no, Id,uniqueId,IP,IPNum,Mac,app_url,title,updatetime 3 from idouba.APP_CLUSTEREDAUDITLOG) 4 where no \u0026gt;= 10 5 and no \u0026lt; 20; 看到返回的是希望的10到19的记录，但是耗费的时间有点长，达到了34S。 查询计划如下\n1已选择10行。 2已用时间: 00: 00: 35.43 3 4执行计划 5---------------------------------------------------------- 6Plan hash value: 3666119494 7 8-------------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10-------------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 20M| 8965M| 102K (1)| 00:20:29 | 12|* 1 | VIEW | | 20M| 8965M| 102K (1)| 00:20:29 | 13| 2 | COUNT | | | | | | 14| 3 | INDEX FAST FULL SCAN| PK_ID | 20M| 8717M| 102K (1)| 00:20:29 | 15-------------------------------------------------------------------------------- 16 17Predicate Information (identified by operation id): 18--------------------------------------------------- 19 1 - filter(\u0026#34;NO\u0026#34;\u0026amp;lt;20 AND \u0026#34;NO\u0026#34;\u0026amp;gt;=10) 20Note 21----- 22 - dynamic sampling used for this statement 23 24统计信息 25---------------------------------------------------------- 26 7 recursive calls 27 0 db block gets 28 462193 consistent gets 29 431581 physical reads 30 0 redo size 31 1174 bytes sent via SQL*Net to client 32 385 bytes received via SQL*Net from client 33 2 SQL*Net roundtrips to/from client 34 0 sorts (memory) 35 0 sorts (disk) 36 10 rows processed\u0026lt;/pre\u0026gt; 方式3 尝试另外一种写法，看起来语义好像也差不多。先取出满足条件的前20条记录，然后在中间选择行号大于10的，即10到20行的记录。\n1select * 2 from (select rownum no,Id, uniqueId,IP,IPNum,Mac,app_url,title,updatetime 3 from idouba.APP_CLUSTEREDAUDITLOG 4 where rownum \u0026lt; 20) 5 where no \u0026gt;= 10; 看到结果集，和2相同，但是耗费时间只有，时间是0.078S。问题出在哪儿呢，观察下查询计划。\n1---------------------------------------------------------- 2Plan hash value: 2707800419 3-------------------------------------------------------------------------------- 4| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 5-------------------------------------------------------------------------------- 6| 0 | SELECT STATEMENT | | 19 | 8911 | 3 (0)| 00:00:01 | 7|* 1 | VIEW | | 19 | 8911 | 3 (0)| 00:00:01 | 8|* 2 | COUNT STOPKEY | | | | | | 9| 3 | INDEX FAST FULL SCAN| PK_ID | 20M| 8717M| 3 (0)| 00:00:01 | 10-------------------------------------------------------------------------------- 11 12 13Predicate Information (identified by operation id): 14--------------------------------------------------- 15 16 1 - filter(\u0026#34;NO\u0026#34;\u0026amp;gt;=10) 17 2 - filter(ROWNUM\u0026amp;lt;20) 18 19Note 20----- 21 - dynamic sampling used for this statement 22 23统计信息 24---------------------------------------------------------- 25 7 recursive calls 26 0 db block gets 27 264 consistent gets 28 0 physical reads 29 0 redo size 30 1174 bytes sent via SQL*Net to client 31 385 bytes received via SQL*Net from client 32 2 SQL*Net roundtrips to/from client 33 0 sorts (memory) 34 0 sorts (disk) 35 10 rows processed 对比2和3的查询计划。不用仔细分析，看计划步骤中间的每一步操作的涉及的行数，以及consistent?gets和physical?reads的不同量级即可理解的差不多。2是把获取所有数据，然后在上面选择10到20的行，3是只获取前20行，从中选择10行之后的数据行。\n方式4 方式2中有order by，这是最常见的一种场景了，按照某个列排序，然后去中间某几条记录，其实就是某一页。\n1select * 2 from (select rownum no, Id,uniqueId,IP,IPNum,Mac,app_url,title,updatetime 3 from (select * from idouba.APP_CLUSTEREDAUDITLOG order by Id)) 4 where 5 no \u0026lt; 20 and 6 no \u0026gt;= 10 执行了13分钟，获取10行数据。看看查询计划。\n1执行计划 2---------------------------------------------------------- 3Plan hash value: 2624114486 4---------------------------------------------------------------------------- 5| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 6---------------------------------------------------------------------------- 7| 0 | SELECT STATEMENT | | 20M| 8965M| 463K (1)| 01:32:40 | 8|* 1 | VIEW | | 20M| 8965M| 463K (1)| 01:32:40 | 9| 2 | COUNT | | | | | | 10| 3 | VIEW | | 20M| 8717M| 463K (1)| 01:32:40 | 11| 4 | INDEX FULL SCAN| PK_ID | 20M| 146G| 462K (1)| 01:32:30 | 12---------------------------------------------------------------------------- 13 14Predicate Information (identified by operation id): 15--------------------------------------------------- 16 17 1 - filter(\u0026#34;NO\u0026#34;\u0026amp;gt;=10 AND \u0026#34;NO\u0026#34;\u0026amp;lt;20) 18 19Note 20----- 21 - dynamic sampling used for this statement 22 23统计信息 24---------------------------------------------------------- 25 7 recursive calls 26 0 db block gets 27 460837 consistent gets 28 210018 physical reads 29 0 redo size 30 1196 bytes sent via SQL*Net to client 31 385 bytes received via SQL*Net from client 32 2 SQL*Net roundtrips to/from client 33 0 sorts (memory) 34 0 sorts (disk) 35 10 rows processed 方式5 方式3中加上order by条件。\n1select * 2 from (select rownum no, 3 Id, 4 uniqueId, 5 IP, 6 IPNum, 7 Mac, 8 app_url, 9 title, 10 updatetime 11 from (select * from idouba.APP_CLUSTEREDAUDITLOG order by Id) 12 where rownum \u0026lt; 20) 13 where no \u0026gt;= 10; 和方式2类似，花费时间也是毫秒级。 查询计划\n1执行计划 2---------------------------------------------------------- 3Plan hash value: 3021574494 4---------------------------------------------------------------------------- 5| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 6---------------------------------------------------------------------------- 7| 0 | SELECT STATEMENT | | 19 | 8911 | 3 (0)| 00:00:01 | 8|* 1 | VIEW | | 19 | 8911 | 3 (0)| 00:00:01 | 9|* 2 | COUNT STOPKEY | | | | | | 10| 3 | VIEW | | 20M| 8717M| 3 (0)| 00:00:01 | 11| 4 | INDEX FULL SCAN| PK_ID | 20M| 146G| 2 (0)| 00:00:01 | 12---------------------------------------------------------------------------- 13Predicate Information (identified by operation id): 14--------------------------------------------------- 15 16 1 - filter(\u0026#34;NO\u0026#34;\u0026amp;gt;=10) 17 2 - filter(ROWNUM\u0026amp;lt;20) 18 19Note 20----- 21 22 - dynamic sampling used for this statement 23 24统计信息 25---------------------------------------------------------- 26 7 recursive calls 27 0 db block gets 28 240 consistent gets 29 2 physical reads 30 0 redo size 31 1196 bytes sent via SQL*Net to client 32 385 bytes received via SQL*Net from client 33 2 SQL*Net roundtrips to/from client 34 0 sorts (memory) 35 0 sorts (disk) 36 10 rows processed 方式6 使用minus\n1select rownum no, Id, uniqueId, IP, IPNum, Mac, app_url, title, updatetime from idouba.APP_CLUSTEREDAUDITLOG where rownum \u0026lt; 20 2minus 3select rownum no, Id, uniqueId, IP, IPNum, Mac, app_url, title, updatetime from idouba.APP_CLUSTEREDAUDITLOG where rownum \u0026lt; 10 对应查询计划如下\n1执行计划 2---------------------------------------------------------- 3Plan hash value: 944274637 4----------------------------------------------------------------------------------------- 5| Id | Operation | Name | Rows | Bytes |TempSpc| Cost (%CPU)| Time | 6----------------------------------------------------------------------------------------- 7| 0 | SELECT STATEMENT | | 19 | 12768 | | 5463K (51)| 18:12:46 | 8| 1 | MINUS | | | | | | | 9| 2 | SORT UNIQUE | | 19 | 8664 | 17G| 2731K (1)| 09:06:23 | 10|* 3 | COUNT STOPKEY | | | | | | | 11| 4 | INDEX FAST FULL SCAN| PK_ID | 20M| 8717M| | 102K (1)| 00:20:29 | 12| 5 | SORT UNIQUE | | 9 | 4104 | 17G| 2731K (1)| 09:06:23 | 13|* 6 | COUNT STOPKEY | | | | | | | 14| 7 | INDEX FAST FULL SCAN| PK_ID | 20M| 8717M| | 102K (1)| 00:20:29 | 15----------------------------------------------------------------------------------------- 16Predicate Information (identified by operation id): 17--------------------------------------------------- 18 19 3 - filter(ROWNUM\u0026amp;lt;20) 20 6 - filter(ROWNUM\u0026amp;lt;10) 21 22Note 23----- 24 25 - dynamic sampling used for this statement 26 27统计信息 28---------------------------------------------------------- 29 0 recursive calls 30 0 db block gets 31 58 consistent gets 32 0 physical reads 33 0 redo size 34 1174 bytes sent via SQL*Net to client 35 385 bytes received via SQL*Net from client 36 2 SQL*Net roundtrips to/from client 37 2 sorts (memory) 38 0 sorts (disk) 39 10 rows processed 方式7： 采用row_number()解析函数\n1SELECT num, Id, uniqueId, IP, IPNum, Mac, app_url, title, updatetime FROM( 2SELECT Id, uniqueId, IP, IPNum, Mac, app_url, title, updatetime,row_number() over(ORDER BY Id)AS num 3FROM idouba.APP_CLUSTEREDAUDITLOG t 4) WHERE num BETWEEN 10 AND 19; 执行计划如下：\n1执行计划 2---------------------------------------------------------- 3Plan hash value: 1698779179 4-------------------------------------------------------------------------------- 5| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 6-------------------------------------------------------------------------------- 7| 0 | SELECT STATEMENT | | 20M| 8965M| 463K (1)| 01:32:40 | 8|* 1 | VIEW | | 20M| 8965M| 463K (1)| 01:32:40 | 9|* 2 | WINDOW NOSORT STOPKEY| | 20M| 8717M| 463K (1)| 01:32:40 | 10| 3 | INDEX FULL SCAN | PK_ID | 20M| 8717M| 462K (1)| 01:32:30 | 11-------------------------------------------------------------------------------- 12Predicate Information (identified by operation id): 13--------------------------------------------------- 14 15 1 - filter(\u0026#34;NUM\u0026#34;\u0026amp;gt;=10 AND \u0026#34;NUM\u0026#34;\u0026amp;lt;=19) 16 2 - filter(ROW_NUMBER() OVER ( ORDER BY \u0026#34;ID\u0026#34;)\u0026amp;lt;=19) 17 18Note 19----- 20 21 - dynamic sampling used for this statement 22 23统计信息 24---------------------------------------------------------- 254 recursive calls 26 0 db block gets 27 123 consistent gets 28 0 physical reads 29 0 redo size 30 1197 bytes sent via SQL*Net to client 31 385 bytes received via SQL*Net from client 32 2 SQL*Net roundtrips to/from client 33 0 sorts (memory) 34 0 sorts (disk) 35 10 rows processed 1 - filter(\u0026quot;NUM\u0026quot;\u0026gt;=10 AND \u0026quot;NUM\u0026quot;\u0026lt;=19) 2 - filter(ROW_NUMBER() OVER ( ORDER BY \u0026quot;ID\u0026quot;)\u0026lt;=19)\nNote dynamic sampling used for this statement 统计信息 三、总结 为了简单期间，只是从2千万条记录中查询10到20行的数据。考察发现如下三种方式性能上是可以接受的3、5、6、7写法是可以接受的（3和5其实差不多，如果如实验所示，在order?by列上是索引聚集的话），都是毫秒级可以出结果。 但是当查询后若干条记录的时候，如一千万行的前十行记录。每种也都需要几分钟的执行时间。\n使用row_number函数 1SELECT num, Id, uniqueId, IP, IPNum, Mac, app_url, title, updatetime FROM( 2SELECT t.*,row_number() over(ORDER BY Id )AS num 3FROM idouba.APP_CLUSTEREDAUDITLOG t 4) 5WHERE num BETWEEN 9999990 AND 10000000; 三层的select\n1select * 2 from (select rownum no, 3 Id, 4 uniqueId, 5 IP, 6 IPNum, 7 Mac, 8 app_url, 9 title, 10 updatetime 11 from (select * from idouba.APP_CLUSTEREDAUDITLOG order by Id) 12 where rownum \u0026amp;lt; 10000000) 13 where no \u0026amp;gt;= 9999990; 使用minus关键字\n1select rownum no, Id, uniqueId, IP, IPNum, Mac, app_url, title, updatetime from (select * from idouba.APP_CLUSTEREDAUDITLOG order by Id) where rownum \u0026amp;lt; 10000000 2minus 3select rownum no, Id, uniqueId, IP, IPNum, Mac, app_url, title, updatetime from (select * from idouba.APP_CLUSTEREDAUDITLOG order by Id) where rownum \u0026amp;lt; 9999990 性能比较如下：\n使用row_number函数 三层的select 使用minus关键字 第1000万行前面10行 766.234 404.125 795.562 第100万行前面10行 12.468 12.438 26.125 第10万行前面10行 1.265 1.125 1.5 第1万行前面10行 0.329 0.312 0.25 第1000行前面10行 0.438 0.468 0.203 第100行前面10行 0.25s 0.219s 0.265 五、最后 本来到这里几种对照就应该结束了。尤其是场景2和场景4，场景3和场景5只是多了个order by子句，因为本来就是以ID列为主键的索引组织表。理解就是按照ID顺序来存记录的，则是否显示的写order by子句应该没有影响，却观察到2、3的结果和4、5的结果不一样呢/sp为了主题集中期间，这个问题放在下一篇介绍。\n附： 1CREATE TABLE \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; 2 (\t\u0026#34;ID\u0026#34; NUMBER NOT NULL ENABLE, 3\t\u0026#34;UNIQUEID\u0026#34; NUMBER, 4\t\u0026#34;POLICYID\u0026#34; NUMBER, 5\t\u0026#34;IP\u0026#34; VARCHAR2(200) NOT NULL ENABLE, 6\t\u0026#34;IPNUM\u0026#34; NUMBER NOT NULL ENABLE, 7\t\u0026#34;MAC\u0026#34; VARCHAR2(200), 8\t\u0026#34;USERVISIT\u0026#34; VARCHAR2(100), 9\t\u0026#34;PHONE\u0026#34; VARCHAR2(200), 10\t\u0026#34;GROUPNAME\u0026#34; VARCHAR2(100), 11\t\u0026#34;PORT\u0026#34; NUMBER, 12\t\u0026#34;PKI\u0026#34; VARCHAR2(200), 13\t\u0026#34;PKIUSERID\u0026#34; VARCHAR2(200), 14\t\u0026#34;APP_URL\u0026#34; VARCHAR2(200), 15\t\u0026#34;TITLE\u0026#34; VARCHAR2(200), 16\t\u0026#34;REQUESTS\u0026#34; VARCHAR2(1000), 17\t\u0026#34;REQIDENTITYCARD\u0026#34; VARCHAR2(1000), 18\t\u0026#34;REQCARCARD\u0026#34; VARCHAR2(1000), 19\t\u0026#34;REQPHONEKEY\u0026#34; VARCHAR2(1000), 20\t\u0026#34;ANSIDENTITYCARD\u0026#34; VARCHAR2(3000), 21\t\u0026#34;ANSCARCARD\u0026#34; VARCHAR2(3000), 22\t\u0026#34;ANSPHONEKEY\u0026#34; VARCHAR2(3000), 23\t\u0026#34;UPDATETIME\u0026#34; DATE, 24\t\u0026#34;PIGEONHOLE\u0026#34; VARCHAR2(200), 25\t\u0026#34;AUDITTYPE\u0026#34; NUMBER, 26\t\u0026#34;TITLEID\u0026#34; NUMBER NOT NULL ENABLE, 27\t\u0026#34;SUBTITLEID\u0026#34; NUMBER, 28\t\u0026#34;IFWARN\u0026#34; NUMBER, 29\t\u0026#34;SERVERIP\u0026#34; VARCHAR2(200), 30\t\u0026#34;DOMAINNAME\u0026#34; VARCHAR2(200), 31\t\u0026#34;PKIUSERNAME\u0026#34; VARCHAR2(200), 32\tCONSTRAINT \u0026#34;PK_ID\u0026#34; PRIMARY KEY (\u0026#34;ID\u0026#34;) ENABLE 33 ) ORGANIZATION INDEX NOCOMPRESS PCTFREE 10 INITRANS 2 MAXTRANS 255 LOGGING 34 STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645 35 PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT) 36 TABLESPACE \u0026#34;USERS\u0026#34; 37 PCTTHRESHOLD 50 OVERFLOW 38 PCTFREE 10 PCTUSED 40 INITRANS 1 MAXTRANS 255 LOGGING 39 STORAGE(INITIAL 16384 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645 40 PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT) 41 TABLESPACE \u0026#34;USERS\u0026#34; ; 42 CREATE UNIQUE INDEX \u0026#34;idouba\u0026#34;.\u0026#34;PK_ID\u0026#34; ON \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; (\u0026#34;ID\u0026#34;) 43 PCTFREE 10 INITRANS 2 MAXTRANS 255 44 STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645 45 PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT) 46 TABLESPACE \u0026#34;USERS\u0026#34; ; 47 48 ALTER TABLE \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; ADD CONSTRAINT \u0026#34;PK_ID\u0026#34; PRIMARY KEY (\u0026#34;ID\u0026#34;) 49 USING INDEX PCTFREE 10 INITRANS 2 MAXTRANS 255 50 STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645 51 PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT) 52 TABLESPACE \u0026#34;USERS\u0026#34; ENABLE; 53 54 ALTER TABLE \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; MODIFY (\u0026#34;ID\u0026#34; NOT NULL ENABLE); 55 ALTER TABLE \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; MODIFY (\u0026#34;IP\u0026#34; NOT NULL ENABLE); 56 ALTER TABLE \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; MODIFY (\u0026#34;IPNUM\u0026#34; NOT NULL ENABLE); 57 ALTER TABLE \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; MODIFY (\u0026#34;TITLEID\u0026#34; NOT NULL ENABLE); 58 ALTER TABLE \u0026#34;idouba\u0026#34;.\u0026#34;APP_CLUSTEREDAUDITLOG\u0026#34; ADD CONSTRAINT \u0026#34;PK_ID\u0026#34; PRIMARY KEY (\u0026#34;ID\u0026#34;) ","link":"https://idouba.com/performance-comparison-of-oracle-paging/","section":"posts","tags":["Oracle"],"title":"oracle分页技术性能比较"},{"body":"","link":"https://idouba.com/categories/jvm/","section":"categories","tags":null,"title":"jvm"},{"body":"","link":"https://idouba.com/tags/jvm/","section":"tags","tags":null,"title":"JVM"},{"body":"","link":"https://idouba.com/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","section":"tags","tags":null,"title":"垃圾回收"},{"body":"一、简介 JVM采用分代垃圾回收。在JVM的内存空间中把堆空间分为年老代和年轻代。将大量（据说是90%以上）创建了没多久就会消亡的对象存储在年轻代，而年老代中存放生命周期长久的实例对象。年轻代中又被分为Eden区(圣经中的伊甸园)、和两个Survivor区。新的对象分配是首先放在Eden区，Survivor区作为Eden区和Old区的缓冲，在Survivor区的对象经历若干次收集仍然存活的，就会被转移到年老区。\n简单讲，就是生命期短的对象放在一起，将少数生命期长的对象放在一起，分别采用不同的回收策略。生命期短的对象回收频率比较高，生命期长的对象采用比较低回收频率，生命期短的对象被尝试回收几次发现还存活，则被移到另外一个地方去存起来。就像现在夏天了，勤劳的douma把doudou和douba常穿的衣服放在顺手的地方，把冬天的衣服打包放在柜子\n另一个地方。虽然把doudou的小衣服类比成虚拟机里的对象有点不合适，大致意思应该就是这样。\n本文中通过最简单的一个例子来demo下这个过程，代码很短，很简单，希望剖析的细一点，包括每一步操作后对象的分配和回收对内存堆产生的影响。设定上包括对堆中年轻代（年轻代中eden区和survivor区）、年老代大小的设定，以及设置阈值控制年轻代到年老代的晋升。\n二、示例代码 下面是最简单的代码，通过代码的每一步的执行来剖析其中的规则。\n1package com.idouba.jvm.demo; 2/** 3 * @author idouba 4 * Use shortest code demo jvm allocation, gc, and someting in gc. 5 * 6 * In details 7 * 1) sizing of young generation (eden space，survivor space),old generation. 8 * 2) allocation in eden space, gc in young generation, 9 * 3) working with survivor space and with old generation. 10 * 11 */ 12 public class SimpleJVMArg { 13 /** 14 * @param args 15 */ 16 public static void main(String[] args) 17 { 18 demo(); 19 } 20 21 /** 22 * VM arg：-verbose:gc -Xms200M -Xmx200M -Xmn100M -XX:+PrintGCDetails -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:+PrintTenuringDistribution 23 * */ 24 @SuppressWarnings(\u0026#34;unused\u0026#34;) 25 public static void demo() { 26 27 final int tenMB = 10* 1024 * 1024; 28 29 byte[] alloc1, alloc2, alloc3; 30 31 alloc1 = new byte[tenMB / 5]; 32 alloc2 = new byte[5 * tenMB]; 33 alloc3 = new byte[4 * tenMB]; 34 alloc3 = null; 35 alloc3 = new byte[6 * tenMB]; 36 } 37 } package com.idouba.jvm.demo; /** * @author idouba * Use shortest code demo jvm allocation, gc, and someting in gc. * * In details * 1) sizing of young generation (eden space，survivor space),old generation. * 2) allocation in eden space, gc in young generation, * 3) working with survivor space and with old generation. * */ public class SimpleJVMArg { /** * @param args */ public static void main(String[] args) { demo(); } /** * VM arg：-verbose:gc -Xms200M -Xmx200M -Xmn100M -XX:+PrintGCDetails -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:+PrintTenuringDistribution * */ @SuppressWarnings(\"unused\") public static void demo() { final int tenMB = 10* 1024 * 1024; byte[] alloc1, alloc2, alloc3; alloc1 = new byte[tenMB / 5]; alloc2 = new byte[5 * tenMB]; alloc3 = new byte[4 * tenMB]; alloc3 = null; alloc3 = new byte[6 * tenMB]; } } 三、执行输出 通过jvm 参数设定几个区域的大小，结合代码执行可以观察到对象在堆上分配和回收的过程。执行参数如下：\n1-verbose:gc -Xms200M -Xmx200M -Xmn100M -XX:+PrintGCDetails -XX:SurvivorRatio=8 -XX:+PrintTenuringDistribution 通过设这_-Xms200M -Xmx200M设置Java堆大小为200M，不可扩展，_-Xmn100M_设置其中100M分配给新生代，则200-100=100M，即剩下的100M分配给老年代。-XX:SurvivorRatio=8设置_了新生代中eden与survivor的空间比例是8:1。\n执行上述代码结果如下：\n1[GC [DefNew 2Desired survivor size 5242880 bytes, new threshold 15 (max 15) 3- age 1: 2237152 bytes, 2237152 total 4: 54886K-\u0026amp;gt;2184K(92160K), 0.0508477 secs] 54886K-\u0026amp;gt;53384K(194560K), 0.0508847 secs] [Times: user=0.03 sys=0.03, real=0.06 secs] 5[GC [DefNew 6Desired survivor size 5242880 bytes, new threshold 15 (max 15) 7- age 2: 2237008 bytes, 2237008 total 8: 43144K-\u0026amp;gt;2184K(92160K), 0.0028660 secs] 94344K-\u0026amp;gt;53384K(194560K), 0.0028957 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 9Heap 10 def new generation total 92160K, used 65263K [0x1a1d0000, 0x205d0000, 0x205d0000) 11 eden space 81920K, 77% used [0x1a1d0000, 0x1df69a10, 0x1f1d0000) 12 from space 10240K, 21% used [0x1f1d0000, 0x1f3f2250, 0x1fbd0000) 13 to space 10240K, 0% used [0x1fbd0000, 0x1fbd0000, 0x205d0000) 14 tenured generation total 102400K, used 51200K [0x205d0000, 0x269d0000, 0x269d0000) 15 the space 102400K, 50% used [0x205d0000, 0x237d0010, 0x237d0200, 0x269d0000) 16 compacting perm gen total 12288K, used 360K [0x269d0000, 0x275d0000, 0x2a9d0000) 17 the space 12288K, 2% used [0x269d0000, 0x26a2a3c0, 0x26a2a400, 0x275d0000) 18 ro space 8192K, 66% used [0x2a9d0000, 0x2af20f10, 0x2af21000, 0x2b1d0000) 19 rw space 12288K, 52% used [0x2b1d0000, 0x2b8206d0, 0x2b820800, 0x2bdd0000) 从中可以看到eden 大小为81920K, Survivor中from区域和to区域大小都是10240k。新生代总的92160K指的是eden和一个Survivor区域的和。\n即原始的内存如图：\n为了演示年轻代对象晋级到年老代的过程。需要设置一个VM参数， 这里设置MaxTenuringThreshold=1。前面不设置的时候，默认MaxTenuringThreshold取值15。当设置不同的阈值，jvm在内存处理会有不同。我们重点观察观察alloc1 这么小块区域在不同的MaxTenuringThreshold参数设置下的遭遇。\n这时候JVM的参数中加上MaxTenuringThreshold=1如下：\n1-verbose:gc -XX:+PrintGCDetails -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:+PrintTenuringDistribution 可以看到输出结果是：\n1[GC [DefNew 2Desired survivor size 5242880 bytes, new threshold 1 (max 1) 3- age 1: 2237152 bytes, 2237152 total 4: 54886K-\u0026amp;gt;2184K(92160K), 0.0641037 secs] 54886K-\u0026amp;gt;53384K(194560K), 0.0641390 secs] [Times: user=0.03 sys=0.03, real=0.06 secs] 5[GC [DefNew 6Desired survivor size 5242880 bytes, new threshold 1 (max 1) 7: 43144K-\u0026amp;gt;0K(92160K), 0.0036114 secs] 94344K-\u0026amp;gt;53384K(194560K), 0.0036418 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 8Heap 9 def new generation total 92160K, used 63078K [0x1a1d0000, 0x205d0000, 0x205d0000) 10 eden space 81920K, 77% used [0x1a1d0000, 0x1df69a10, 0x1f1d0000) 11 from space 10240K, 0% used [0x1f1d0000, 0x1f1d0000, 0x1fbd0000) 12 to space 10240K, 0% used [0x1fbd0000, 0x1fbd0000, 0x205d0000) 13 tenured generation total 102400K, used 53384K [0x205d0000, 0x269d0000, 0x269d0000) 14 the space 102400K, 52% used [0x205d0000, 0x239f2260, 0x239f2400, 0x269d0000) 15 compacting perm gen total 12288K, used 360K [0x269d0000, 0x275d0000, 0x2a9d0000) 16 the space 12288K, 2% used [0x269d0000, 0x26a2a3c0, 0x26a2a400, 0x275d0000) 17 ro space 8192K, 66% used [0x2a9d0000, 0x2af20f10, 0x2af21000, 0x2b1d0000) 18 rw space 12288K, 52% used [0x2b1d0000, 0x2b8206d0, 0x2b820800, 0x2bdd0000) 四、过程解析 下面观察每一步语句执行后，jvm内存的变化情况，并给出解析。\n1)在执行第一个语句，alloc1分配2M空间 1alloc1 = new byte[tenMB / 5]; 后，根据分代策略，在新生代的eden区分配2M的空间存储对象。\n2)在执行第二语句，alloc2分配50M 1alloc2 = new byte[5 * tenMB]; 前面alloc1分配2M后，因为eden的80M空间还有80-2=78M还可以容纳下allocation2要求的50M空间，因此接着在eden区域分配。\n3）当执行第三句，alloc3分配40M 1alloc3 = new byte[4 * tenMB]; 还是尝试在eden上分配，但是eden空间只剩下28M，不能容纳alloc3要求的40M空间。于是触发在新生代上的一次gc，将Eden区的存活对象转移到Survivor区。在这个里先将2M的alloc1对象存放（其实是copy，参见java 垃圾回收策略的描述）到from区，然后copy 50M的alloc2对象，显然survivor区不能容纳下alloc2对象，该对象被直接copy到年老代。需要说明的是复制到Survivor区的对象在经历一次gc后期对象年龄会被加一。\n在eden区gc后腾出空间可以存放allocation3的40M对象，则alloc3分配40M对象如图：\n4）执行第四句，将alloc3置空 1alloc3 = null; 这是eden上alloc3分配的的40M对象则变成可被回收状态。\n5) 执行第5句，对alloc重新分配60M空间 1allocation3 = new byte[6 * tenMB]; 还是尝试先在eden区上分配，发现超出了eden区域的容量，则再次触发新生代上的一次gc。首先eden上分配的40M对象因为没有被再使用，则直接被回收。而根据的设置不同，这次gc的行为会稍有不同。\n先看MaxTenuringThreshold不设置，即取默认值15的时候。eden区上无用的40M回收后，再考察Survivor区域的对象是否满足对象晋升老年代的年龄阈值，发现from中的2M对象，年龄是1，不满足晋升条件，则不被处理，只是把Survivor区域的经历这次回收未被处理的对象age加一，即新的age为2.如图：\n通过输出日志也显示：经过这次回收年轻代大小，由43114K变为2184k，总的大小由94344k变为53384k，即反映出回收了40M无用对象。\n1Desired survivor size 5242880 bytes, new threshold 15 (max 15) 2- age 2: 2237008 bytes, 2237008 total 3: 43144K-\u0026gt;2184K(92160K), 0.0028660 secs] 94344K-\u0026gt;53384K(194560K), 0.0028957 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 在年轻代上gc后腾出空间后，新的alloc3的60M空间被分配到eden 区域上。分配后堆如下：\n以上是不设置晋升阈值MaxTenuringThreshold情况下进行的gc，以及gc后alloc3的分配。\n再看看当MaxTenuringThreshold设置为1的情况。同样eden区上无用的40M回收后，再考察Survivor区域的对象是否满足对象晋升老年代的年龄阈值，发现from中的2M对象，年龄是1，满足晋升条件，则Survivor区域满足年龄的对象被拷贝到年老区。\n通过日志显示年轻代的大小被清0了，表示survivor的存活对象因为满足晋升条件被移到被移到年老代了。\n1[GC [DefNew 2Desired survivor size 5242880 bytes, new threshold 1 (max 1) 3: 43144K-\u0026amp;gt;0K(92160K), 0.0036114 secs] 94344K-\u0026amp;gt;53384K(194560K), 0.0036418 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 同样的，gc完后会在eden上分配空间来存储alloc3对象，这种情况下堆结构如图：\n比较上面两个图，发现差别就仅仅在于survivor中的2M对象是否被认为生存时间足够长科院被移到年老代中去。从上面日志高亮部分from区域的最终存储也可反映出了这个差别。\n比较前面两个日志可以看到：总的大小和上面设置和不设置MaxTenuringThreshold（其实是MaxTenuringThreshold设置1还是15）没有关系，都是由94344k变为53384k，即都是回收了40M eden区域无用对象。第N次gc时存活的满足晋升条件则由survivor移到年老代，不满足的还留在survivor区域，堆的总的大小没有变。\n五、最后 上面通过最简单的例子示意了下在jvm堆上对象是如果分配的，当空间不足时，是如何调整回收的。希望可以对jvm的堆上结构和gc思路有个基本的了解。当然相关参数（其实反映的是机制）远比这个复杂，有挺多细节，更多的是在实践中来体会。\n附：主要参数 JVM启动设置的参数很多，可以参照中Java HotSpot VM Options说明。例子中涉及的是最最基础的参数，这里只是对涉及到的几个参数进行说明。\n参数名称 含义 默认值 说明 -Xms 初始堆大小 物理内存的1/64(\u0026lt;1GB) 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制. -Xmx 最大堆大小 物理内存的1/4(\u0026lt;1GB) 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 -Xmn 年轻代大小 注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。 整个堆大小=年轻代大小 + 年老代大小 + 持久代大小. 增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8 -XX:SurvivorRatio Eden区与Survivor区的大小比值 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10 -XX:+PrintGC 开启GC日志打印 默认不启用 开启GC日志打印。打印格式例如： [Full GC 131115K-\u0026gt;7482K(1015808K), 0.1633180 secs] 该选项可通过?com.sun.management.HotSpotDiagnosticMXBean API?和?Jconsole?动态启用。 详见?http://java.sun.com/developer/technicalArticles/J2SE/monitoring/#Heap_Dump -XX:+PrintGCDetails 打印GC回收的细节。 1.4.0引入，默认不启用 打印格式例如： [Full GC (System) [Tenured: 0K-\u0026gt;2394K(466048K), 0.0624140 secs] 30822K-\u0026gt;2394K(518464K), [Perm : 10443K-\u0026gt;10443K(16384K)], 0.0625410 secs] [Times: user=0.05 sys=0.01, real=0.06 secs] 该选项可通过?com.sun.management.HotSpotDiagnosticMXBean API?和?Jconsole?动态启用 详见?http://java.sun.com/developer/technicalArticles/J2SE/monitoring/#Heap_Dump XX:+PrintTenuringDistribution 查看每次minor GC后新的存活周期的阈值 打印对象的存活期限信息。打印格式例如： [GC Desired survivor size 4653056 bytes, new threshold 32 (max 32) - age 1: 2330640 bytes, 2330640 total - age 2: 9520 bytes, 2340160 total 204009K-\u0026gt;21850K(515200K), 0.1563482 secs] Age1 2表示在第1和2次GC后存活的对象大小。 -XX:MaxTenuringThreshold 垃圾最大年龄。Survivor区经过该阈值次回收依然存活的对象会被移到年老代。 如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率 附配图可编辑件：jvm-allocation-and-gc\n","link":"https://idouba.com/a-simple-example-demo-jvm-allocation-and-gc/","section":"posts","tags":["垃圾回收","JVM"],"title":"最简单例子图解JVM内存分配和回收"},{"body":"最近一篇发表于《程序员》2014年6月刊上的文章。有点遗憾发现，有些部分被编辑修改过了，读起来有点怪怪的。最典型的是习惯于对某些比较经典的定义引用wikipedia或者原始白皮书中原始的E文，在文中发现都被硬译过了，表达的意思自己都有点看不懂了！\n最终修改后提交的版本归档下：\n引言 关于数据库索引，随便Google一个Oracle index，Mysql index总能得到“某某索引之n条经典建议”之类大量结果。笔者认为，较之直接借鉴，在搞清实际需求的基础上，对备选方案的原理尽可能深入全面的了解会更有利于我们的决策。因为某种方案或者技术呈现出某种优势（包括可能没有被介绍到但一定存在的限制），不是厂商的白皮书这样规定，是由实现机制决定的或者说本身的结构决定的。\n本文重点介绍数据结构中经典的树（B树）结构在数据库索引中的经典应用，也会涉及到几种数据库中对此支持的细微不同，以期比较完整的描述实现原理。最终会发现这几种被不同数据库厂商冠以不同名字的东西原理上其实差不多，理论上其实是一个东西。文中只是略微空洞的介绍其实现原理，不涉及应用上具体的使用建议。\n关键字：B树 数据库索引 索引组织表（Index-Organized Tables） 聚集索引 非聚集索引 Oracle Mysql Mssql\n一、关于数据库索引 数据库索引在维基中的定义：A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and the use of more storage space to maintain the extra copy of data. Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed.\n如果Index被翻译成目录可能更能体现出其本质的作用。和其他很多计算机科学中的概念一样，Index也是现实事物中的一种常见结构。由目录最容易联想到的是图书馆的书籍管理，如果没有个目录，很难想象要从图书馆的那么多书架上找到一本书是多么困难的事情。\n当然映射的最好的是小时候厚厚的新华字典前面的目录，对于字典中数据根据两种拼音和笔画(四角号码)两种属性进行索引。字典前面和后面多出来的那么几十页纸（额外的存储）的用处就是快速定位到字典中某个词条的完整记录。如果没有这个Index，要查找字典的某个字就只有full table scan一样的挨着翻页了。\n二、关于B树索引 数据库中比较常用的索引结构有B树、位图等几种。其中B树是几乎所有数据库的默认索引结构，也是用的最多的索引结构。\n索引的基本作用是用于查找。数据结构的查找算法中最基本的是顺序查找，即从列表上逐个匹配关键字，其时间复杂度是O(n)，当n比较大的时候这个效率是不能承受的。于是计算机科学尝试能不能在存储上做些文章发明效率更高的算法，然后就有了数据结构中我们熟悉的基于排序树的查找。B树（其实是B+树）是一种树的结构，通常用于数据库和操作系统的文件系统中。B+树的创造者Rudolf Bayer没有解释B代表什么。最常见的观点是B代表平衡(balanced)，因为所有的叶子节点在树中都在相同的级别上，B也可能代表Bayer，或者是波音（Boeing），因为他曾经工作于波音科学研究实验室。下图是一件简单的B树的例子。\n1Function: search (k) 2 return tree_search (k, root); 3Function: tree_search (k, node) 4 if node is a leaf then 5 return node; 6 switch k do 7 case k \u0026amp;lt; k_0 8 return tree_search(k, p_0); 9 case k_i ≤ k \u0026amp;lt; k_{i+1} 10 return tree_search(k, p_{i+1}); 11 case k_d ≤ k 12 return tree_search(k, p_{d+1}); 关于B树的一个性质，在数据库中采用的B树结构的索引，除了上面平衡树的基础特征外，结合数据库索引使用的需要，都有如下的结构要求。\n内节点不存储data，只存储key和指向下级节点的指针；叶子节点不存储指针，存储真正的数据。即内节点的作用是导航，叶节点才真正存数据。不同的索引类型，叶节点data域存储的东西会有不同，导致查询也会不同。在后面会对此详细介绍。 在叶子节点上都会有个双向的指针指向相邻的叶子节点。提高在索引键上的区间访问的性能。 通常在B树上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点。因此可以对B树进行两种查找运算：一种是从最小关键字起顺序查找，另一种是从根节点开始，进行随机查找。\n三、B树在数据库索引中的几种应用 结合数据库实现对B树结构的不同应用，主要是叶子节点存储的内容不同，把B树分为两种：一种是叶节点存完整的行数据，一种是叶节点只是存一个指向实际数据行的指针。根据表中数据存储格式不同，指针又分为物理指针和逻辑指针。这样B树的结构被分成三类：\nB树叶节点存完整数据的索引结构 B树叶节点存物理指针的索引结构 B树叶节点存逻辑指针的索引结构 听着都不太高大上。为了讨论方便，且这样分了。\n(一)B树叶节点存物理指针的索引结构 这是最普通的一种索引结构。数据插入时存储位置是随机的，由数据库内部存储的空闲情况决定。这种表数据的存储结构称为堆表（heap table）。在堆表中记录是无序的，插入速度会比较快。但是查找一个数据会比较麻烦，需要扫描整个堆表才可以。如下图表T是示意的一个简单表，表上有三列。每行前十六进制数字仅示意该行的存储位置。\n假设查找出C2=43的行，我们需要从第一行开始，逐行的检查每行上C2的取值。即使第三行找到了。但还是需要扫描接下来的行，因为不能保证在前方还有没有满足条件的行。对一个数据量比较大的表，这样的方式是不可以接受的。\n于是乎就有了索引的概念，即另外开辟一个存储结构，按照某个列进行排序，并记录每行的在该列上取值的以及该行在表中的对应位置。这恐怕是索引本质的意思了吧。就像字典上某个拼音和页码的关系。\n几乎所有的这类索引都采用B树结构。叶节点的key是索引列在每行上的值，而对应的data域保存了该行的一个引用，可理解为指向实际存储数据的指针。如图中在C2上建立索引，按照C2的属性构建B树，在每个叶节点上和索引键对应的都有一个指针记录该行数据的存储位置。尽管右下角的表上的数据是无序的，同样要找到C2=43的记录行，从索引树上只要经过三个节点即可以找到叶节点存储的指针，并通过指针找到对应的行。\n因为几种数据库中最典型的索引，结构也就基本相同。Oracle 中直接根据存储结构把这种索引称为B树索引，索引叶节点存储(key: rowid)，其中rowid标识了该行的物理存储位置。\n对于Mssql来说，这种索引称为非聚集索引。当没有创建聚集索引的时候，即表示表是以堆的形式(heap structure)存储。同样叶节点也是存储（key: RID），其中RID指定数据存储物理位置的行和页。\n在Mysql 中索引结构因不同的存储引擎实现而不同。两种比较常用的存储引擎中,Myisam表上的数据总是按照堆的结果存储的，索引采用和上图类似的索引结构。详细点说Myisam上的主键索引、唯一索引、辅助索引都是这种结构。不同的是，主键索引要求选择的索引列是表的主键，唯一索引要求索引列取值的唯一性约束，而辅助索引没有这些要求。\n(二)B树叶节点存数据的索引结构 B树构造的另外一种索引，与其说是一种索引方式，倒不如说是以一种表数据的存储方式（Oracle 中就称之为索引组织表）。这种结构的一个特点是B树的叶节点中和索引键对应存储的是实际的数据行。即（Key: Row）的结构。即在叶节点上完整的保存了数据行。如图，在C3列上构建索引，则整个表中的数据按照C3的顺序来存储。第一个叶节点上存储了C3=5和C3=25的完整的行，同时整个表按照C3取值的顺序存储。即整个表的数据按照C3列在聚集（哦，难怪在Mssql中这种结构被称为聚集索引）。\nOracle 中，不认为该种方式的存储是索引，而是更形象的称为索引组织表（Index-Organized Tables）；在Mssql中，这种结构正是其所谓的聚集索引（Clustered Index）；在Mysql 中， Innodb是支持这种结构的，称之为聚集（Clustered index）。即便三种数据库都支持该索引结构，其相互之间还是有些比较tricky的差别，这正是想对照着强调的。\n在Oracle 的索引组织表根据主键排序后的顺序进行排列的，即索引的列必须是表的主键列，在建表的同时要指定主键约束，可以是单字段，也可以是复合主键约束。创建索引组织表时，必须要设定主键，否则报错。\n在Mysql的Innodb的存储引擎中，Innodb的数据文件本身要按主键聚集，按主键顺序存储。所以Innodb要求表必须有主键，如果没有显式指定，Mysql系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则Mysql自动为Innodb表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。\n而在Mssql中，关于该索引列的要求就没有那么高，并未要求索引列必须是主键，也不要求该列上有唯一性约束。当在没有聚集索引的表上创建主键时，Mssql会自动在该主键列上创建一个聚集索引。当在没有唯一约束的列上创建聚集索引时，Mssql会自动的在重复的键值上添加一个4 byte的uniqueifier使得该值唯一，这个对用户是透明的。\n(三)B树叶节点存逻辑指针的索引结构 根据前面的描述，当表中数据按照传统堆结构组织的时候，构造索引（非聚集）的B树的叶节点上上存储（key: rowid）这样的结构，即关联到数据行的物理指针。但当数据本身是按照B树存储的时候，数据库认为有了逻辑标识一个行的标签，叶节点存储的对指针会稍有不同。不再存储一个物理指针，而是存储对应的聚集索引键这样一个逻辑指针，即叶节点上存储（key: clusterKey）这样的结构。如图，在C3上创建了聚集索引，C1上创建一个非聚集的索引。则在C1的索引树上叶节点处存储了C3取值作为聚集索引的键。如第三个叶子节点，C1对应的值为Inter，对应的聚集索引在该行的值为C3=151.即通过151这个cluster key来关联到实际数据行。数据行在另外一个按C3列构造的B树上存储。\n因为几种数据库对于聚集索引的要求有细微差别，当存在聚集索引情况下的非聚集索引也相应的有所不同。\n在Oracle 中，该索引称为辅助索引（Secondary Indexes on Index-Organized Tables）。因为Oracle 的索引组织表（Index-Organized Tables）的索引键必须是主键，则该辅助索引相应管理的是一个代表了主键的逻辑rowid。 在Mysql的Innodb中，和Oracle 几乎完全相同，这种索引也称为辅助索引（secondary indexes）。因为其聚集索引列也是要求必须是主键，相应辅助索引关联的也是对应的主键。\n在Mssql中，这种索引称为非聚集索引(Nonclustered Index)。在B树的叶节点上存储索引列和聚集索引对应聚集索引键（clustered index key）。上面讨论聚集索引的时候说到过，Mssql的聚集索引的列不要求唯一性，也不要求是主键。但是为了非聚集索引能通过聚集索引键唯一定位到一行数据，在重复的聚集索引键上会添加一个唯一标示来使得其唯一，这个操作对用户是透明的。\n如上图在C3上有重复的值，按照Mysql和Oracle 的要求，在该列上是不能创建聚集索引的，但是在Mssql中，在该列上可以建聚集索引。图示C3列有重复值的聚集索引的情况下在C1列上的构建非聚集索引，C1上创建的非聚集索引的每一行数据都能通过聚集索引key唯一关联到实际的数据行上。\nmsdn中关于no-clustered index的介绍：If the clustered index is not a unique index, SQL Server makes any duplicate keys unique by adding an internally generated value called a uniqueifier. This four-byte value is not visible to users. It is only added when required to make the clustered key unique for use in nonclustered indexes.\n四、总结 为了更清晰的对照，整理出一个对照列表。发现大部分都是相同的，除了术语上，SQL语法上，或者某些约定限制的程度上稍有不同。因为原理是一样的。同样因为结构相同，造成使用也是完全相同。如：\n根据聚集索引的检索方式； 有聚集索引时根据非聚集索引检索方式； 没有聚集索引时根据非聚集索引检索方式 数据库(存储引擎)/项目 Oracle Mssql Mysql(Innodb) Mysql(Myisam) 表数据B树结构存储（即创建了聚集索引） 支持表数据B树存储 支持 支持 支持 不支持 术语 索引组织表（Index-Organized Tables） 聚集索引Clustered Indexes 聚集索引(主键索引) Clustered Index 不支持 聚集索引键要求 必须是主键 没有主键要求，也没有唯一性要求 必须是主键 不支持 B树叶节点结构 (Key: ROW)索引key和整行数据 同Oracle 同Oracle 不支持 根据聚集索引访问数据行 聚集索引上检索聚集索引键，找到索引叶节点即访问到整行数据 同Oracle 同Oracle 不支持 索引（非聚集）名称 辅助索引 非聚集索引 辅助索引 不支持 索引（非聚集）B树叶节点结构 (Key：ClusterKey)索引（非聚集）键和聚集索引键的对应关系。 同Oracle 同Oracle 不支持 根据索引（非聚集）访问数据行 二次检索：1.检索索引（非聚集），定位到索引行所在叶节点，得到索引键对应的聚集索引键；2.在聚集索引上检索聚集索引键，即访问到数据行。 同Oracle 同Oracle 不支持 表数据堆存储方式heap structure （聚集索引不存在) 索引（非聚集）名称 B树索引 非聚集索引 不支持 主键索引、唯一索引、辅助索引 索引（非聚集）B树叶节点结构 (Key：ROWID) 索引（非聚集）键和行存储物理位置 同Oracle 不支持 同Oracle 根据索引（非聚集）访问数据行 1.从索引（非聚集）定位到索引行所在叶节点，即得到数据行的物理存储位置；2.直接根据物理存储位置从堆上访问数据行。 同Oracle 不支持 同Oracle 再根据原理多分析一点，不是使用建议，只是这种结构提示给我们的信息。只说it is ,不说you should。\n了解了聚集索引实现原理后，就能理解为什么不大建议在长字段上面建聚集索引，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大；也能理为什么说在聚集索引列上的查找，包括范围查找会比较高效，数据就是这样组织的；也能理解建了聚集索引后写入性能会怎样降低，因为数据组织有了约束，写入性能下降，插入/删除/更新聚集键值等，会导致记录的物理移动、页拆分等额外的磁盘操作；也不难理解非聚集的索引读数据时候，如果不能从索引上包含全部的查询列，需要关联表来查询，则会有两次查询，一次是从非聚集索引上定位到聚集索引键，然后再从聚集索引键查到数据。较之非聚集的索引，聚集索引也就只能有一个，也就相对珍贵些。一般选择会要比较慎重些。知道了这些原理后，对于到底要不要建聚集索引，根据业务特征在哪个列上创建，要不要创建非聚集索引，在哪个上创建，这些也就不难回答。很多时候这种选择其实是几种方案间的trade-off。即使理解了原理，在使用中几种方案实验结果的对照更能帮助我们做出正确的选择。有点像我们测试中的黑盒测试之于白盒测试的关系。\n同样对于其他数据库方面的技术，通过Database System Concepts 中数据库一般理论的稍微抽象的观点了解、看待其在我们开发中的应用，可以使我们对这些技术的理解更系统、更深刻。当项目需要游走于多个数据库之间的时候，不至于都是拿着manual，拿着tuning的手册来完全的从零开始在在操作层面上被指导。\n附：《程序员》原版 about-btrees-application-in-database-index-in-programmer\n","link":"https://idouba.com/about-btrees-application-in-database-index-in-programmer/","section":"posts","tags":["索引","《程序员》"],"title":"B树在数据库索引中的应用剖析(发表版本)"},{"body":"","link":"https://idouba.com/tags/%E7%B4%A2%E5%BC%95/","section":"tags","tags":null,"title":"索引"},{"body":"豆豆小朋友真的长大了！会调皮了。\n今天晚上，doudou指挥着和豆爸配合了一把棒球的游戏。doudou用的是乒乓球，douba用的是笤帚做球杆（棒球是不是叫球棒？）。工具都是 doudou提供和设计。总共进行了六轮，以姥姥指挥douba耍赖藏球结束比赛。哦，忘了补充一句，整个过程，doudou是在尿湿了裤子，外面裤子被 扒下来在膝盖位置，湿漉漉的秋裤贴着屁股的艰难环境下完成的。\n这个家伙怎么这么没有出息呀，就喜欢笤帚，在家里面扫地可起劲了。\n1for i in 0..6 2\tdoudou: 3\tstep1: 高高的撅着屁股往床下扔乒乓球 4\tstep2：结实的趴在地上瞄着床下确认球的位置 5\tstep3：扶着床沿爬起来，把笤帚递给douba 6\tstep4：用手和眼睛指挥douba把球捞出来 7\tdouba： 8\t按照doudou的指示用笤帚击球 9\tpingpang： 10\t滚出来（滚蛋） 11\tdoudou： 12\tstep1：大笑，傻笑 13\tstep2：晃晃悠悠跑到乒乓跟前 14\tstep2：猫腰撅起屁股做棒球投球状。。。出手！ 完。\n","link":"https://idouba.com/doudou-baseball/","section":"posts","tags":["doudou"],"title":"豆豆棒球记"},{"body":"一、 前言 在调查一个性能问题的时候，一个同事问道，为什么数据库有些时候这么不聪明，明明表上有索引，但是在执行一个简单的count的时候居然全表扫描了！难道不知道走索引更快么？\n试图从最简单的count来重新了解oracle查询计划的选择，以及最终产生的结果。虽然有些结果会让人觉得有些意外，并且可能会鄙视，这个查询计划选择真的不够聪明。但稍微用心点的去了解，做的已经足够细致了。大多数情况下，根据我们输入的信息，来自输入的SQL、表结构、索引状况、统计信息，会得出一个比较优的计划。所以和前面一直试图讲到索引和join方式一样，所有这样的选择不是因为数据库厂商这样规定的，而是基于存储的数据的实际情况，就**应该(甚至说不得不)**这么去选择。\n1-- Create table 2create table IDOUBA.APP_APPLICATIONAUDIT 3( 4 ID NUMBER, 5 UNIQUEID NUMBER, 6 POLICYID NUMBER, 7 IP VARCHAR2(200) not null, 8 IPNUM NUMBER not null, 9 MAC VARCHAR2(200), 10 USERVISIT VARCHAR2(100), 11 PHONE VARCHAR2(200), 12 GROUPNAME VARCHAR2(100), 13 PORT NUMBER, 14 APP_URL VARCHAR2(200), 15 TITLE VARCHAR2(200), 16 REQUESTS VARCHAR2(1000), 17 REQIDENTITYCARD VARCHAR2(1000), 18 REQKEY VARCHAR2(1000), 19 RESPONSEEKEY VARCHAR2(3000), 20 UPDATETIME DATE, 21 AUDITTYPE NUMBER, 22 TITLEID NUMBER, 23 SUBTITLEID NUMBER, 24 SERVERIP VARCHAR2(200), 25 DOMAINNAME VARCHAR2(200) 26) 27tablespace USERS 28 pctfree 10 29 initrans 1 30 maxtrans 255 31 storage 32 ( 33 initial 64K 34 minextents 1 35 maxextents unlimited 36 ); 37-- Create/Recreate indexes 38create index INDEX_UPDATETIME on IDOUBA.APP_APPLICATIONAUDIT (UPDATETIME) 39 tablespace SYSTEM 40 pctfree 10 41 initrans 2 42 maxtrans 255 43 storage 44 ( 45 initial 64K 46 minextents 1 47 maxextents unlimited 48 ); 表中有20001000条记录，有二十多个字段，包括几个长度比较大的字段。为了实验测试，只是讲IP字段设置为not null。在UpdateTime上建了默认的B树索引。\n三、输入 分为三种场景来讨论，然后比较其结果，然后分析该结果。说明数据库的执行计划的选择其实是足够聪明的。\n场景一： 表结构：只是在有一个可以为null的列UpdateTime上建了一个索引 SQL：最简单的Count。 1select count(*) from IDOUBA.App_ApplicationAudit; --count(*) 2select count(PolicyId) from IDOUBA.App_ApplicationAudit; --count(可以null的列) 3select count(IP) from IDOUBA.App_ApplicationAudit; --count(不可null的列) 4select count(UpdateTime) from IDOUBA.App_ApplicationAudit; --count(可以为null并且建了索引的列) 场景二： 表结构：和场景一完全相同，只是在有一个可以为null的列UpdateTime上建了一个索引 SQL：多了个Where子句，在Where子句中涉及到建了索引的可以为null的列。 1select count(*) from IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; --count(*) 2select count(PolicyId) from IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; --count(可以null的列) 3select count(IP) from IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; --count(不可null的列) 4select count(UpdateTime) from y IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; --count(可以为null并且建了索引的列) 场景三： 表结构：除了有一个可以为null的列UpdateTime上建了一个索引外，设置另外一列IpNum列为not null，并且在其上创建索引。 SQL：最简单的Count。 1select count(*) from IDOUBA.App_ApplicationAudit; --count(*) 2select count(PolicyId) from IDOUBA.App_ApplicationAudit; --count(可以null的列) 3select count(IP) from IDOUBA.App_ApplicationAudit; --count(不可null的列) 4select count(UpdateTime) from IDOUBA.App_ApplicationAudit; --count(可以为null并且建了索引的列) 四、实验结果 场景/Count类型 Count(*) Count(PolicyId) 非索引可null的列 Count(IP)非索引not null 的列 Count(Updatetime)可以null的索引列 Count(IPNum) not null 的索引列 执行时间(秒) 物理读 执行操作 执行时间(秒) 物理读 执行操作 执行时间(秒) 物理读 执行操作 执行时间(秒) 物理读 执行操作 执行时间(秒) 物理读 physical reads 执行操作 场景一 50.98 51671 表扫描 47.43 451672 表扫描 47.73 451672 表扫描 8.7 82393 UpdateTime上快速索引扫描 无 无 无 场景二 9.28 82395 UpdateTime上快速索引扫描 48.12 451673 表扫描 9.65 82428 UpdateTime上快速索引扫描 9.03 82394 UpdateTime上快速索引扫描 无 无 无 场景三 5.14 50224 IpNum上快速索引扫描 47.89 451672 表扫描 2.25 15153 IpNum上快速索引扫描 9.06 82393 UpdateTime上快速索引扫描 3.2 82393 IpNum上快速索引扫描 五、结果分析 其实观察查询计划，也就两种，一种快速索引扫描(INDEX FAST FULL SCAN)，另外一种是全表扫描（TABLE ACCESS FULL|）。因为实验表中数据量还是不算小，有两千多万，两者的对比还是比较明显。主要体现在总的执行时间，和物理读取上面，差着一个量级。下面对表中的执行结果逐个进行总结和解释。\n场景一： 当表中只在一个可以null的列上建索引的情况下。观察到count(*) 、count not null的列count(IP)，count 可以null的列PolicyId的count(PolicyId)的效果一样都是走全表扫描（TABLE ACCESS FULL|）；而对于count建了索引并且列可以null的列count(UpdateTime)，走快速索引扫描(INDEX FAST FULL SCAN)。即只在索引列UpdateTime上， count会走快速索引扫描(INDEX FAST FULL SCAN)，其他都是全表扫描。\n解释：因为oracle不会索引null，因此扫描该列上的索引只能知道该列上非null的值，而count(*) 和count(非空列)都是统计总行数，从索引上不能获得该信息，只能全表扫描。而count(可空列)就更不能参照该索引了，建了索引的可空列上非null的行数，和在count的可空列上非null的行数没有任何关系，因此也只能全表扫描了。\n场景二： 比场景一where子句中包含了可以null的列update的条件where updatetime \u0026gt; to_date(’2013-05-01 10:23:44′, ‘yyyy-mm-dd hh24:mi:ss’) 。观察到count(*)和count not null的列count(IP)和count UpdateTime一样都走updatetime上的快速索引扫描(INDEX FAST FULL SCAN)；而count 可null列PolicyId的count(PolicyId)还是全表扫描。\n解释：满足类似于pdatetime \u0026gt; to_date(’2013-05-01 10:23:44′, ‘yyyy-mm-dd hh24:mi:ss’)这样可空列上的条件，其实隐含的意思是满足该条件并且该列上取值不为null的行的行数。则在这列上的count，count(*)，在其他非null列上的count，表达的都是这个意思，因此可以利用该索引来做索引扫描。而在另外一个可null的列上的count是表示满足该where条件同时在count列上值非null的记录行数，索引列上不为空的行可能在该列上为空，因此不能参照那个索引，只能全表扫描。\n场景三： 当表中存在一个not null的列IpNum上建了索引。观察到count(*)和count（IP）在非空索引上快速索引扫描(INDEX FAST FULL SCAN)，但是count(ProjectId)还是走原来的全表扫描。即：当表上在一个not null列上建了索引，则只有可Null的列的count走全表扫描，其他的都会走这个建了索引的no null 列的快速索引扫描(INDEX FAST FULL SCAN)。\n解释：有一个not null的列上建了索引，则这个索引上的记录数就是表的行数，count(*),count(非空列)都是数行数。但是count(可null 列)是数这个列上不为空的记录数，因此不能参照索引，只能全表扫描了。\n六、总结 在Oracle中，Count(Column)是计算Column上不为该列取值不为null的行数，Count一个not null的列其实就是总行数。而Count(*)是不区分null或者not null就是所有记录行数。因此二者语义是一样的，实验也证明了在各种情况下其执行计划总是一样的。对于这两种Count，表上无论哪个索引能提供这样的语义（在例子中场景二和场景三的两个不同索引分别提供了这样的语义），就会走这个索引。如果没有索引能提供这个语义，就不得不走全表扫描了。而Count一个可以为null的列，因为要数本列上到底有多少行的值不为null，因此不能参照别的列，必须在该列上数数，如果该列上有索引，则会在该列的索引上扫描，如果该列上没有索引，则不得不全表扫描。\n附：各种场景的执行计划 只是在有一个可以为null的列UpdateTime上建了一个索引的Count(*) 1SQL\u0026gt; select count(*) from IDOUBA.App_ApplicationAudit; 1COUNT(*) 2---------- 3 20001000 4已用时间: 00: 00: 50.98 5执行计划 6---------------------------------------------------------- 7Plan hash value: 2649150711 8----------------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Cost (%CPU)| Time | 10----------------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 1 | 99985 (2)| 00:20:00 | 12| 1 | SORT AGGREGATE | | 1 | | | 13| 2 | TABLE ACCESS FULL| APP_APPLICATIONAUDIT | 19M| 99985 (2)| 00:20:00 | 14----------------------------------------------------------------------------------- 15统计信息 16---------------------------------------------------------- 17 166 recursive calls 18 0 db block gets 19 451721 consistent gets 20 451671 physical reads 21 0 redo size 22 410 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 5 sorts (memory) 26 0 sorts (disk) 27 1 rows processed 只是在有一个可以为null的列UpdateTime上建了一个索引的Count一个可以null的列policyId 1SQL\u0026gt; select count(policyId) from IDOUBA.App_ApplicationAudit; 1COUNT(POLICYID) 2--------------- 3 20001000 4已用时间: 00: 00: 47.43 5执行计划 6---------------------------------------------------------- 7Plan hash value: 2649150711 8------------------------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10------------------------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 1 | 2 | 100K (2)| 00:20:02 | 12| 1 | SORT AGGREGATE | | 1 | 2 | | | 13| 2 | TABLE ACCESS FULL| APP_APPLICATIONAUDIT | 19M| 38M| 100K (2)| 00:20:02 | 14------------------------------------------------------------------------------------------- 15统计信息 16---------------------------------------------------------- 17 1 recursive calls 18 0 db block gets 19 451701 consistent gets 20 451672 physical reads 21 0 redo size 22 417 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 0 sorts (memory) 26 0 sorts (disk) 27 1\trows processed 只是在有一个可以为null的列UpdateTime上建了一个索引的Count一个可以null的索引列updatetime：Count(UpdateTime) 1SQL\u0026gt; select count(updatetime) from IDOUBA.App_ApplicationAudit; 1COUNT(UPDATETIME) 2----------------- 3 20001000 4已用时间: 00: 00: 08.70 5执行计划 6---------------------------------------------------------- 7Plan hash value: 3909306724 8------------------------------------------------------------------------------------------ 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10------------------------------------------------------------------------------------------ 11| 0 | SELECT STATEMENT | | 1 | 8 | 18204 (3)| 00:03:39 | 12| 1 | SORT AGGREGATE | | 1 | 8 | | | 13| 2 | INDEX FAST FULL SCAN| INDEX_UPDATETIME | 19M| 152M| 18204 (3)| 00:03:39 | 14------------------------------------------------------------------------------------------ 15统计信息 16---------------------------------------------------------- 17 164 recursive calls 18 0 db block gets 19 82448 consistent gets 20 82393 physical reads 21 0 redo size 22 419 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 5 sorts (memory) 26 0 sorts (disk) 27 1 rows processed 只是在有一个可以为null的列UpdateTime上建了一个索引的Count一个not null的非索引列IP：Count(IP) 1SQL\u0026gt; select count(Ip) from IDOUBA.App_ApplicationAudit; 1COUNT(IP) 2---------- 3 20001000 4已用时间: 00: 00: 47.73 5执行计划 6---------------------------------------------------------- 7Plan hash value: 2649150711 8----------------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Cost (%CPU)| Time | 10----------------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 1 | 99985 (2)| 00:20:00 | 12| 1 | SORT AGGREGATE | | 1 | | | 13| 2 | TABLE ACCESS FULL| APP_APPLICATIONAUDIT | 19M| 99985 (2)| 00:20:00 | 14----------------------------------------------------------------------------------- 15统计信息 16---------------------------------------------------------- 17 218 recursive calls 18 0 db block gets 19 451730 consistent gets 20 451672 physical reads 21 0 redo size 22 411 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 5 sorts (memory) 26 0 sorts (disk) 27 1\trows processed 只是在有一个可以为null的列UpdateTime上建了一个索引，Query的Where子句中包含该UpdateTime列的条件where updatetime \u0026gt; to_date(’2013-05-01 10:23:44′, ‘yyyy-mm-dd hh24:mi:ss’) ，Count(*) 1SQL\u0026gt; select count(*) from IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; 1COUNT(*) 2---------- 3 20001000 4已用时间: 00: 00: 09.28 5执行计划 6---------------------------------------------------------- 7Plan hash value: 3909306724 8------------------------------------------------------------------------------------------ 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10------------------------------------------------------------------------------------------ 11| 0 | SELECT STATEMENT | | 1 | 8 | 18373 (4)| 00:03:41 | 12| 1 | SORT AGGREGATE | | 1 | 8 | | | 13|* 2 | INDEX FAST FULL SCAN| INDEX_UPDATETIME | 19M| 152M| 18373 (4)| 00:03:41 | 14------------------------------------------------------------------------------------------ 15Predicate Information (identified by operation id): 16--------------------------------------------------- 17 2 - filter(\u0026#34;UPDATETIME\u0026#34;\u0026amp;gt;TO_DATE(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd 18 hh24:mi:ss\u0026#39;)) 19统计信息 20---------------------------------------------------------- 21 227 recursive calls 22 0 db block gets 23 82464 consistent gets 24 82395 physical reads 25 0 redo size 26 410 bytes sent via SQL*Net to client 27 385 bytes received via SQL*Net from client 28 2 SQL*Net roundtrips to/from client 29 5 sorts (memory) 30 0 sorts (disk) 31 1 rows processed\u0026lt;/pre\u0026gt; 只是在有一个可以为null的列UpdateTime上建了一个索引，Query的Where子句中包含该UpdateTime列的条件where updatetime \u0026gt; to_date(’2013-05-01 10:23:44′, ‘yyyy-mm-dd hh24:mi:ss’) 。Count一个可以null的列PolicyId：Count(PolicyId) 1SQL\u0026gt; select count(policyId) from IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; 1COUNT(POLICYID) 2--------------- 3 20001000 4已用时间: 00: 00: 48.12 5执行计划 6---------------------------------------------------------- 7Plan hash value: 2649150711 8------------------------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10------------------------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 1 | 10 | 101K (3)| 00:20:20 | 12| 1 | SORT AGGREGATE | | 1 | 10 | | | 13|* 2 | TABLE ACCESS FULL| APP_APPLICATIONAUDIT | 19M| 190M| 101K (3)| 00:20:20 | 14------------------------------------------------------------------------------------------- 15Predicate Information (identified by operation id): 16--------------------------------------------------- 17 2 - filter(\u0026#34;UPDATETIME\u0026#34;\u0026amp;gt;TO_DATE(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;)) 18统计信息 19---------------------------------------------------------- 20 227 recursive calls 21 0 db block gets 22 451737 consistent gets 23 451673 physical reads 24 0 redo size 25 417 bytes sent via SQL*Net to client 26 385 bytes received via SQL*Net from client 27 2 SQL*Net roundtrips to/from client 28 5 sorts (memory) 29 0 sorts (disk) 30 1 rows processed 只是在有一个可以为null的列UpdateTime上建了一个索引，Query的Where子句中包含该UpdateTime列的条件where updatetime \u0026gt; to_date(’2013-05-01 10:23:44′, ‘yyyy-mm-dd hh24:mi:ss’) 。Count一个not null的列IP：Count(IP) 1 SQL\u0026gt; select count(ip) from IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; 1COUNT(IP) 2---------- 3 20001000 4已用时间: 00: 00: 09.65 5 6执行计划 7---------------------------------------------------------- 8Plan hash value: 3909306724 9------------------------------------------------------------------------------------------ 10| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 11------------------------------------------------------------------------------------------ 12| 0 | SELECT STATEMENT | | 1 | 8 | 18373 (4)| 00:03:41 | 13| 1 | SORT AGGREGATE | | 1 | 8 | | | 14|* 2 | INDEX FAST FULL SCAN| INDEX_UPDATETIME | 19M| 152M| 18373 (4)| 00:03:41 | 15------------------------------------------------------------------------------------------ 16Predicate Information (identified by operation id): 17--------------------------------------------------- 18 2 - filter(\u0026#34;UPDATETIME\u0026#34;\u0026amp;gt;TO_DATE(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;)) 19统计信息 20---------------------------------------------------------- 21 1 recursive calls 22 0 db block gets 23 82428 consistent gets 24 82393 physical reads 25 0 redo size 26 411 bytes sent via SQL*Net to client 27 385 bytes received via SQL*Net from client 28 2 SQL*Net roundtrips to/from client 29 0 sorts (memory) 30 0 sorts (disk) 31 1 rows processed 只是在有一个可以为null的列UpdateTime上建了一个索引，Query的Where子句中包含该UpdateTime列的条件where updatetime \u0026gt; to_date(’2013-05-01 10:23:44′, ‘yyyy-mm-dd hh24:mi:ss’) 。Count该可以 null的索引列UpdateTime：Count(UpdateTime) 1SQL\u0026gt; select count(updatetime) from IDOUBA.App_ApplicationAudit where updatetime \u0026gt; to_date(\u0026#39;2013-05-01 10:23:44\u0026#39;, \u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;) ; 1COUNT(UPDATETIME) 2----------------- 3 20001000 4已用时间: 00: 00: 09.03 5执行计划 6---------------------------------------------------------- 7Plan hash value: 3909306724 8------------------------------------------------------------------------------------------ 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10------------------------------------------------------------------------------------------ 11| 0 | SELECT STATEMENT | | 1 | 8 | 18204 (3)| 00:03:39 | 12| 1 | SORT AGGREGATE | | 1 | 8 | | | 13| 2 | INDEX FAST FULL SCAN| INDEX_UPDATETIME | 19M| 152M| 18204 (3)| 00:03:39 | 14------------------------------------------------------------------------------------------ 15统计信息 16---------------------------------------------------------- 17 185 recursive calls 18 0 db block gets 19 82460 consistent gets 20 82394 physical reads 21 0 redo size 22 419 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 5 sorts (memory) 26 0 sorts (disk) 27 1 rows processed 在一个not null的列IPNum创建索引count（*） 1SQL\u0026gt; select count(*) from IDOUBA.App_ApplicationAudit; 1COUNT(*) 2---------- 3 20001000 4已用时间: 00: 00: 05.14 5执行计划 6---------------------------------------------------------- 7Plan hash value: 2581539037 8----------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Cost (%CPU)| Time | 10----------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 1 | 11435 (5)| 00:02:18 | 12| 1 | SORT AGGREGATE | | 1 | | | 13| 2 | INDEX FAST FULL SCAN| INDEX_IPNUM | 19M| 11435 (5)| 00:02:18 | 14----------------------------------------------------------------------------- 15统计信息 16---------------------------------------------------------- 17 1 recursive calls 18 0 db block gets 19 50253 consistent gets 20 50224 physical reads 21 0 redo size 22 410 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 0 sorts (memory) 26 0 sorts (disk) 27 1\trows processed 10 在一个not null的列IPNum创建索引。Count可null的列PolicyId：Count(PolicyId)\n1SQL\u0026gt; select count(policyId) from IDOUBA.App_ApplicationAudit; 1COUNT(POLICYID) 2--------------- 3 20001000 4已用时间: 00: 00: 47.89 5执行计划 6---------------------------------------------------------- 7Plan hash value: 2649150711 8------------------------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10------------------------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 1 | 2 | 100K (2)| 00:20:02 | 12| 1 | SORT AGGREGATE | | 1 | 2 | | | 13| 2 | TABLE ACCESS FULL| APP_APPLICATIONAUDIT | 19M| 38M| 100K (2)| 00:20:02 | 14------------------------------------------------------------------------------------------- 15统计信息 16---------------------------------------------------------- 17 1 recursive calls 18 0 db block gets 19 451701 consistent gets 20 451672 physical reads 21 0 redo size 22 417 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 0 sorts (memory) 26 0 sorts (disk) 27 1\trows processed 11 在一个not null的列IPNum创建索引。Count not null的列IP：Count(IP)\n1SQL\u0026gt; select count(Ip) from IDOUBA.App_ApplicationAudit; 1COUNT(IP) 2---------- 3 20001000 4已用时间: 00: 00: 02.25 5执行计划 6---------------------------------------------------------- 7Plan hash value: 2581539037 8----------------------------------------------------------------------------- 9| Id | Operation | Name | Rows | Cost (%CPU)| Time | 10----------------------------------------------------------------------------- 11| 0 | SELECT STATEMENT | | 1 | 11435 (5)| 00:02:18 | 12| 1 | SORT AGGREGATE | | 1 | | | 13| 2 | INDEX FAST FULL SCAN| INDEX_IPNUM | 19M| 11435 (5)| 00:02:18 | 14----------------------------------------------------------------------------- 15统计信息 16---------------------------------------------------------- 17 196 recursive calls 18 0 db block gets 19 50290 consistent gets 20 15153 physical reads 21 0 redo size 22 411 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 5 sorts (memory) 26 0 sorts (disk) 27 1\trows processed 12 在一个not null的列IPNum创建索引。Count另外一个建了索引的可null的列UpdateTime：Count(UpdateTime)\n1SQL\u0026gt; select count(updatetime) from IDOUBA.App_ApplicationAudit; 1COUNT(UPDATETIME) 2----------------- 3 20001000 4已用时间: 00: 00: 09.06 5执行计划 6---------------------------------------------------------- 7Plan hash value: 3909306724 8------------------------------------------------------------------------------------------ 9| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | 10------------------------------------------------------------------------------------------ 11| 0 | SELECT STATEMENT | | 1 | 8 | 18204 (3)| 00:03:39 | 12| 1 | SORT AGGREGATE | | 1 | 8 | | | 13| 2 | INDEX FAST FULL SCAN| INDEX_UPDATETIME | 19M| 152M| 18204 (3)| 00:03:39 | 14------------------------------------------------------------------------------------------ 15统计信息 16---------------------------------------------------------- 17 1 recursive calls 18 0 db block gets 19 82428 consistent gets 20 82393 physical reads 21 0 redo size 22 419 bytes sent via SQL*Net to client 23 385 bytes received via SQL*Net from client 24 2 SQL*Net roundtrips to/from client 25 0 sorts (memory) 26 0 sorts (disk) 27 1\trows processed 完。 ","link":"https://idouba.com/analysis-oracle-execute-plan-from-count/","section":"posts","tags":["执行计划","Oracle"],"title":"从Count看Oracle执行计划的选择"},{"body":"","link":"https://idouba.com/tags/%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92/","section":"tags","tags":null,"title":"执行计划"},{"body":"引言 关于数据库索引，随便Google一个Oracle index，Mysql index总有大量的结果出来，其中不乏某某索引之n条经典建议。笔者认为，较之借鉴，在搞清楚了自己的需求的基础上，对备选方案的原理有个尽可能深入全面的了解会更有利于我们的选择和决策。因为某种方案或者技术呈现出某种优势（包括可能没有被介绍到但一定存在的限制），不是定义出来的，而是因为其实现机制决定的。就像LinkedList和ArrayList分别适用于什么应用不是Document里面定义的，是由其本身的结构决定的。数据库的索引也是一样，不是厂商的白皮书这样规定，而是其原理决定的。 本文只是重点介绍数据结构中经典的树（B树）结构在数据库索引中的经典应用，也会涉及到几种数据库中对此支持的细微不同，以期比较完整的描述实现原理。最终会发现这几种被不同数据库厂商冠以不同名字东西原理上其实差不多，理论上其实是一个东西。文中只是略微空洞的介绍其实现原理，不涉及应用上具体的使用建议。\n#####关键字：B树 数据库索引? 索引组织表（Index-Organized Tables） 聚集索引 非聚集索引 Oracle ?Mysql Mssql\n一、关于数据库索引 数据库索引在维基中的定义：A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and the use of more storage space to maintain the extra copy of data. Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed. Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records. 这个定义看上去挺长，简单讲就是为了对一个相对较大的数据结构访问快速方便，另外的存储了一个小的数据结构，依照待检索属性进行排序并且记录了该属性的记录在大的数据结构中的位置，以便快速的在大的数据结构中检索定位。如果Index被翻译成目录可能更能体现出其本质的作用。和其他很多计算机科学中的概念一样，Index也是现实事物中的一种常见结构。由目录最容易联想到的是图书馆的书籍管理，如果没有个目录，很难想象要从图书馆的那么多书架上找到一本书是多么困难的事情。\n当然映射的最好的是小时候厚厚的新华字典前面的目录，一般好像有两种，一种是拼音的，一种是笔画还是所谓的四角号码的。就是对于字典中数据根据两种不同属性不同进行索引。字典前面和后面多出来的那么几十页纸（额外的存储）的用处就是帮助检索者快速定位到字典中某个词条的完整记录。如果没有这个Index，要查找字典的某个字就只有来挨着翻页了(对应数据库索引的全表扫描full table scan)。\n（写完上文想着配个图，在google图片搜图片的时候搜到了这篇文章 ，直接摘录了，并为作者点个赞。我只是想做个数据库索引和字典索引的类别，作者居然做到了和B树索引的类别！）\n二、关于B树索引 数据库中比较常用的索引结构有B树、位图等几种。其中B树是几乎所有数据库的默认索引结构，也是用的最多的索引结构。 索引的基本作用是用于查找。数据结构的查找算法中最基本的是顺序查找，即从列表上逐个匹配关键字，其时间复杂度是O(n)，当n比较大的时候这个效率是不能承受的。于是计算机科学尝试能不能在存储上做些文章发明效率更高的算法，然后就有了数据结构中我们熟悉的基于排序树的查找。B树（其实是B+树）是一种树的结构，通常用于数据库和操作系统的文件系统中。特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。B+树的创造者Rudolf Bayer没有解释B代表什么。最常见的观点是B代表平衡(balanced)，因为所有的叶子节点在树中都在相同的级别上，B也可能代表Bayer，或者是波音（Boeing），因为他曾经工作于波音科学研究实验室。下图是一件简单的B树的例子。\nA simple B+ tree example linking the keys 1–7 to data values d1-d7. The linked list (red) allows rapid in-order traversal\nB树是一棵平衡树，采用树的结构是因为其O(logN)的查找复杂度，而平衡树是计算机科学中改进的二叉查找树。对一棵查找树（search tree）进行查询/新增/删除等动作,所花的时间与树的高度h成比例,并不与树的容量n成比例。在B树上不管查找成功与否，每次查找都是走了一条从根到叶子结点的路径。一个度为d的B树，设节点为数N，则其树高h的上限为logd((N+1)/2)，检索一个值，其查找节点个数的时间复杂度为O(logdN)。这样使得在B树中检索一个节点最多需要h个节点，而数据库系统中一般将一个节点的大小设定为一个页，每个节点一次IO。使B树的根节点常驻内存，则一次检索最多需要h-1次的I/O即可。关于数据结构中的树，二叉树、平衡树的结构，遍历方式、节点查找方式、节点的删除、添加等都是很典型的内容，不在此做介绍。B树检索的伪代码如下：\n1Function: search (k) 2 return tree_search (k, root); 3 4Function: tree_search (k, node) 5 if node is a leaf then 6 return node; 7 switch k do 8 case k \u0026amp;lt; k_0 9 return tree_search(k, p_0); 10 case k_i ≤ k \u0026amp;lt; k_{i+1} 11 return tree_search(k, p_{i+1}); 12 case k_d ≤ k 13 return tree_search(k, p_{d+1}); 关于B树的一个性质，在集中数据库中采用的B树结构的索引，除了上面平衡树的公共特征外，结合数据库索引使用的需要，都有如下的结构要求。\n内节点不存储data，只存储key和指向下级节点的指针；叶子节点不存储指针，存储真正的数据 (Oracle 中分别称为branch blocks 和 leaf blocks ；Mssql中称为intermediate level nodes和leaf nodes)。即内节点的作用是导航，叶子节点才真正存数据data。不同的索引类型，叶节点这个data域存储的东西会有不同，导致查询也会不同。在后面会对此详细介绍。\n在叶子节点上都会有个双向的指针指向相邻的叶子节点。提高在索引键上的区间访问的性能。\n通常在B树上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点。因此可以对B树进行两种查找运算：一种是从最小关键字起顺序查找，另一种是从根节点开始，进行随机查找。\n三、B树在数据库索引中的几种应用 结合数据库实现对B树结构的不同应用，主要是叶子节点存储的内容不同，我把B树其为两种：一种是叶节点存完整的行数据，一种是叶节点只是存一个指向实际数据行的指针。根据表中数据存储格式不同，指针又分为物理指针和逻辑指针。这样B树的结构被分成了三类：\nB树叶节点存完整数据的索引结构 B树叶节点存物理指针的索引结构 B树叶节点存逻辑指针的索引结构 因为几种数据库各自对这几种特征的索引的术语不同，暂且这样统一命名，虽然听着都不高大上。为了讨论方便，且这样分了。\n(一) B树叶节点存物理指针的索引结构 这是最普通的一种索引结构。数据插入时存储位置是随机的，主要是数据库内部存储的空闲情况决定。这种表数据的存储结构称为堆表（heap table）（本来heap这个概念就是生成时候分配空间的）。在堆表（heap table）中记录是无序的，插入速度会比较快。但是查找一个数据会比较麻烦，需要扫描整个堆表才可以。如下图表T是示意的一个简单表，表上有三列。前面的十六进制数字仅仅是示意这一行的存储位置。 想想我们需要在表中找出C2=43的行，我们需要从第一行开始，逐行的检查每一行上C2的取值。直到找到第三行找到了。但还是需要扫描接下来的行，因为你不能保证在你扫描的前方还有没有另外一个或者多个C2=43的行存在。即要进行全表的扫描，查找一条记录的时间复杂度是O(N)，N为记录行数。对一个数据量比较大的表，这样的方式几乎是不可以接受的。 于是乎就有了索引的概念，即另外开辟一个存储结构，按照某个列进行排序，并记录每行的在该列上取值的以及该行在表中的对应位置。这恐怕是索引本质的意思了吧。回到我们字典的类比上，想我们的字典能根据目录某个笔画找到那个词条，靠的是在目录中存的页码这样一个指针。 因为前面提到的B树的优点，几乎所有的这类索引都采用B树结构。在叶节点上，叶节点的key是索引列在每行上的值，而对应的data域保存了该行的一个引用，也可以理解为指向实际存储数据的指针。如图中在C2上建立索引，记录按照C2的属性构建B树，在每个叶节点上和索引键对应的都有一个指针记录该行数据的存储位置。尽管右下角的表上的数据是无序的。同样要找到C2=43的记录行，从索引树上只要经过三个节点即可以找到叶节点存储的指针，并通过指针找到对应的行。\n因为几种数据库中最典型的索引，结构也就基本相同。Oracle 中直接根据存储结构把这种索引称为B树索引，索引叶节点存储(key: rowid)，其中rowid标识了该行的物理存储位置。\n引用来自Oracle ?Database Concepts 对B-Tree Indexes的描述：\nThe leaf blocks contain every indexed data value and a corresponding rowid used to locate the actual row. Each entry is sorted by (key, rowid). Within a leaf block, a key and rowid is linked to its left and right sibling entries. The leaf blocks themselves are also doubly linked.\n对于Mssql来说，这种索引称为非聚集索引。当没有创建聚集索引的时候，即表示表是以堆的形式(heap structure)存储。同样叶节点也是存储（key: RID），其中RID指定数据存储物理位置的行和页。 引用[msdn.microsoft中Nonclustered Index Structures]的描述:\nIf the table is a heap, which means it does not have a clustered index, the row locator is a pointer to the row. The pointer is built from the file identifier (ID), page number, and number of the row on the page. The whole pointer is known as a Row ID (RID).\n在Mysql 中索引结构和表的存储方式都是和存储引擎相关，不同的存储引擎实现不同。两种比较常用的存储引擎中,Myisam表上的数据总是按照堆的结果存储的，在Myisam上的索引也都是采用和上图类似的索引结构。详细点说Myisam上的主键索引、唯一索引、辅助索引都是这种结构。不同的是，主键索引要求选择的索引列是表的主键，唯一索引要求索引列取值的唯一性约束，而辅助索引没有这些要求。\n(二)B树叶节点存数据的索引结构 B树构造的另外一种索引，与其说是一种索引方式，倒不如说是以一种表数据的存储方式（Oracle 中就称之为索引组织表（Index-Organized Tables））。这中结构的一个特点是B树的叶节点中和索引键对应存储的是实际的数据行。即（Key: Row）的结构。即在叶节点上完整的保存了数据行。如图，在C3上构建索引，则整个表中的数据按照C3的顺序来存储。第一个叶节点上存储了C3=5和C3=25的完整的行，同时整个表按照C3取值的顺序存储。即整个表的数据按照C3列在聚集（难怪在Mssql中这种结构被称为聚集索引呢）。\n在Oracle 中，并不认为该种方式的存储是索引，而是更形象的称为索引组织表（Index-Organized Tables）；在Mssql中，这种结构正是其所谓的聚集索引（Clustered Index）；在Mysql 中，因为索引属于存储引擎级别的概念，在常用的Innodb和Myisam存储引擎中，只有Innodb是支持这种结构的，称之为（clustered index）。即便三种数据库分别支持这种索引结构，其相互之间还是有些比较tricky的差别，这正是想对照着强调的。 在Oracle 的索引组织表（Index-Organized Tables）根据主键排序后的顺序进行排列的，即索引的列必须是表的主键列，在建表的同时要指定主键约束，可以是单字段主键，也可以是复合主键约束。创建索引组织表时，必须要设定主键，否则报错。\n引用来自Oracle ?Database Concepts 对Index-Organized Tables的描述：\nAn index-organized table is a table stored in a variation of a B-tree index structure. In a heap-organized table, rows are inserted where they fit. In an index-organized table, rows are stored in an index defined on the primary key for the table. Each index entry in the B-tree also stores the non-key column values. 在Mysql的Innodb的存储引擎中，Innodb的数据文件本身要按主键聚集，按主键顺序存储。所以Innodb要求表必须有主键，如果没有显式指定，Mysql系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则Mysql自动为Innodb表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 引用来自Mysql Manual关于clustered index的描述:\nIf you define a PRIMARY KEY on your table, Innodb uses it as the clustered index. If you do not define a PRIMARY KEY for your table, Mysql picks the first UNIQUE index that has only NOT NULLcolumns as the primary key and Innodb uses it as the clustered index. If the table has no PRIMARY KEY or suitable UNIQUE index, Innodb internally generates a hidden clustered index on a synthetic column containing row ID values. The rows are ordered by the ID that Innodb assigns to the rows in such a table. The row ID is a 6-byte field that increases monotonically as new rows are inserted. Thus, the rows ordered by the row ID are physically in insertion order. 而在Mssql中，关于该索引列的要求就没有那么高，并未要求改索引列必须是主键，也不要求该列上必须有唯一性约束。如果表上没有建聚集索引，当在表上创建主键的时候，Mssql会自动在该主键列上创建一个聚集索引。当在没有唯一约束的列上创建聚集索引是，Mssql会自动的在重复的键值上添加一个4 byte的uniqueifier使得该值唯一，这个对用户是透明的。\n来自technet.microsoft 的Create Clustered Indexes :\nWhen you create a PRIMARY KEY constraint, a unique clustered index on the column or columns is automatically created if a clustered index on the table does not already exist and you do not specify a unique nonclustered index. The primary key column cannot allow NULL values.\n(三) B树叶节点存逻辑指针的索引结构 根据前面的描述，当表中数据按照传统堆结构组织的时候，构造索引（非聚集）的B树的叶节点上上存储（key: rowid）这样的结构，即关联到数据行的物理指针。但当数据本身是按照B树存储的时候，数据库认为有了逻辑标识一个行的标签，叶节点存储的对指针会稍有不同。不在存储一个物理指针，而是存储该逻辑指针。即叶节点上存储（key: clusterKey）这样的结构，即关联到对应的聚集索引键，聚集索引键扮演了一个逻辑指针。如图，前面在C3上创建了聚集索引，C1上创建一个非聚集的索引。则在C1构造的索引树上叶节点处存储了每行C3取值作为聚集索引的键。如第三个叶子节点，C1对应的值为Inter，而对应的聚集索引在该行的值为C3=151.即通过151这个cluster key来关联到实际数据行。因为在C3上创建了聚集索引，数据行在另外一个按C3列构造的B树上存储。\n因为几种数据库对于聚集索引的要求有细微差别，在存在聚集索引情况下的非聚集索引也相应的有所不同。在Oracle 中，该索引称为辅助索引（Secondary Indexes on Index-Organized Tables）。因为Oracle 的索引组织表（Index-Organized Tables）的索引键必须是主键，则该辅助索引相应管理的是一个代表了主键的逻辑rowid。 引用Oracle Database Concepts的描述:\nAs explained in “Rowid Data Types”, Oracle ?Database uses row identifiers called logical rowids for index-organized tables. A logical rowid is a base64-encoded representation of the table primary key. The logical rowid length depends on the primary key length. 在Mysql的Innodb中，和Oracle 几乎完全相同，这种索引也称为辅助索引（secondary indexes）。因为其聚集索引列也是要求必须是主键，相应辅助索引关联的也是对应的主键。\n引用Mysql Manual关于clustered index的描述:\nAll indexes other than the clustered index are known as secondary indexes. In Innodb, each record in a secondary index contains the primary key columns for the row, as well as the columns specified for the secondary index. Innodb uses this primary key value to search for the row in the clustered index. 在Mssql中，这种索引称为非聚集索引(Nonclustered Index)。在B树的页节点上存储索引列和聚集索引对应聚集索引键（clustered index key）。上面讨论聚集索引的时候说到过，Mssql的聚集索引的列不要求唯一性，也不要求是主键。但是为了非聚集索引能通过聚集索引键唯一定位到一行数据，在重复的聚集索引键上会添加一个唯一标示来使得其唯一，这个操作对用户是透明的。\n如上图在C3上有重复的值，按照Mysql和Oracle 的要求，在该列上是不能创建聚集索引的，但是在Mssql中，在该列上可以建聚集索引。图示在C1列上的非聚集索引和C3列有重复值的聚集索引的情况下，C1上创建的非聚集索引的每一行数据都能通过聚集索引key唯一关联到实际的数据行上。\n来自msdn的no-clustered index\nIf the table has a clustered index, or the index is on an indexed view, the row locator is the clustered index key for the row. If the clustered index is not a unique index, SQL Server makes any duplicate keys unique by adding an internally generated value called a uniqueifier. This four-byte value is not visible to users. It is only added when required to make the clustered key unique for use in nonclustered indexes. SQL Server retrieves the data row by searching the clustered index using the clustered index key stored in the leaf row of the nonclustered index.\n四、总结 为了更清晰的对照，整理出一个对照列表。发现大部分都是相同的，除了术语上，SQL语法上，或者某些约定限制的程度上。因为原理是一样的。同样因为结构相同，造成使用也是完全相同。如：\n根据聚集索引的检索方式； 有聚集索引时根据非聚集索引检索方式； 没有聚集索引时根据非聚集索引检索方式 数据库(存储引擎)/项目 Oracle Mssql Mysql(Innodb) Mysql(Myisam) 表数据B树结构存储（即创建了聚集索引） 支持表数据B树存储 支持 支持 支持 不支持 术语 索引组织表（Index-Organized Tables） 聚集索引**Clustered Indexes** 聚集索引(主键索引) Clustered Index 不支持 聚集索引键要求 必须是主键 没有主键要求，也没有唯一性要求 必须是主键 不支持 B树叶节点结构 **(Key: ROW)**索引key和整行数据 **(Key: ROW)**索引key和整行数据 **(Key: ROW)**索引key和整行数据 不支持 根据聚集索引访问数据行 聚集索引上检索聚集索引键，找到索引叶节点即访问到整行数据 聚集索引上检索聚集索引键，找到索引叶节点即访问到整行数据 聚集索引上检索聚集索引键，找到索引叶节点即访问到整行数据 不支持 索引（非聚集）名称 辅助索引 非聚集索引 辅助索引 不支持 索引（非聚集）B树叶节点结构 (Key**：ClusterKey)**索引（非聚集）键和聚集索引键的对应关系。 (Key**：ClusterKey)**索引（非聚集）键和，聚集索引键的对应关系。 (Key**：ClusterKey)**索引（非聚集）键和，聚集索引键的对应关系。 不支持 根据索引（非聚集）访问数据行 二次检索：1.检索索引（非聚集），定位到索引行所在叶节点，得到索引键对应的聚集索引键；2.在聚集索引上检索聚集索引键，即访问到数据行。 二次检索：1.检索索引（非聚集），定位到索引行所在叶节点，得到索引键对应的聚集索引键；2.在聚集索引上检索聚集索引键，即访问到数据行。 二次检索：1.检索索引（非聚集），定位到索引行所在叶节点，得到索引键对应的聚集索引键；2.在聚集索引上检索聚集索引键，即访问到数据行。 不支持 表数据堆存储方式heap structure （聚集索引不存在) 索引（非聚集）名称 B树索引 非聚集索引 不支持 主键索引、唯一索引、辅助索引 索引（非聚集）B树叶节点结构 (Key**：ROWID)** 索引（非聚集）键和行存储物理位置 (Key**：ROWID)** 索引（非聚集）键和行存储物理位置 不支持 (Key**：ROWID)** 索引（非聚集）键和行存储物理位置 根据索引（非聚集）访问数据行 1.从索引（非聚集）定位到索引行所在叶节点，即得到数据行的物理存储位置；2.直接根据物理存储位置从堆上访问数据行。 1.从索引（非聚集）定位到索引行所在叶节点，即得到数据行的物理存储位置；2.直接根据物理存储位置从堆上访问数据行。 不支持 1.从索引（非聚集）定位到索引行所在叶节点，即得到数据行的物理存储位置；2.直接根据物理存储位置从堆上访问数据行。 再根据原理多分析一点，不是使用建议，只是这种结构提示给我们的信息。只说it is ,不说you should。\n如知道了聚集索引实现原理后，应该能理解为什么不大建议在长字段上面建聚集索引，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大；也能理为什么说在聚集索引列上的查找，包括范围查找会比较高效，因为聚集索引按照某个列在组织；也能理解建了聚集索引后写入性能会怎样降低，因为数据组织有了约束，写入性能下降，插入/删除/更新聚集键值等，会导致记录的物理移动、页拆分等额外的磁盘操作；也不难理解非聚集的索引读数据时候，如果不能从索引上包含全部的查询列，需要关联表来查询，则会有两次查询，一次是从非聚集索引上定位到聚集索引键，然后再从聚集索引键查到数据。\n较之非聚集的索引，数据存储方式只有一种，聚集索引也就只能有一个，也就显得相对珍贵些。一般选择会要比较慎重些。知道了这些原理后，对于到底要不要建聚集索引，根据业务特征在哪个列上创建，要不要创建非聚集索引，在哪个上创建。这些也就不难回答。\n当然即使理解了原理，在使用中参考使用场景的实验结果更能帮助我们做出满足要求的选择。有点像我们测试中的黑盒测试之于白盒测试的关系。\n同样对于其他数据库方面的技术，通过Database System Concepts 中数据库一般理论的稍微抽象的观点了解、看待其在我们开发中的应用，可以使我们对这些技术的理解更系统，更深刻。当项目需要游走于多个数据库之间的时候，不至于都是拿着manual，拿着tuning的手册来完全的从零开始被指导。\n五、附录 Database System Concepts 关于索引的一个介绍 View Ch11.ppt and other presentations by idouba. B树在数据库索引中的应用剖析.pdf ","link":"https://idouba.com/about-btrees-application-in-database-index/","section":"posts","tags":["索引","《程序员》"],"title":"B树在数据库索引中的应用剖析（原稿）"},{"body":"douma发给我的一篇文章，让douba好好阅读。\n上帝给我一个任务\n叫我牵一只蜗牛去散步。\n我不能走太快，\n蜗牛已经尽力爬，为何每次总是那么一点点？\n我催它，我唬它，我责备它，\n蜗牛用抱歉的眼光看着我，\n彷佛说：「人家已经尽力了嘛！」\n我拉它，我扯它，甚至想踢它，\n蜗牛受了伤，它流着汗，喘着气，往前爬…\n真奇怪，为什么上帝叫我牵一只蜗牛去散步？\n「上帝啊！为什么？」\n天上一片安静。\n「唉！也许上帝抓蜗牛去了！」\n好吧！松手了！\n反正上帝不管了，我还管什么？\n让蜗牛往前爬，我在后面生闷气。\n咦？我闻到花香，原来这边还有个花园，\n我感到微风，原来夜里的微风这么温柔。\n慢着！我听到鸟叫，我听到虫鸣。\n我看到满天的星斗多亮丽！\n咦？我以前怎么没有这般细腻的体会？\n我忽然想起来了，莫非我错了？\n是上帝叫一只蜗牛牵我去散步。分享：\n教育孩子就像牵着一只蜗牛在散步。\n和孩子一起走过他孩提时代和青春岁月，\n虽然也有被气疯和失去耐心的时候，\n然而，\n孩子却在不知不觉中向我们展示了生命中最初最美好的一面。\n孩子的眼光是率真的，\n孩子的视角是独特的，\n家长又何妨放慢脚步，\n把自己主观的想法放在一边，\n陪着孩子静静体味生活的滋味，\n倾听孩子内心声音在俗世的回响，\n给自己留一点时间，\n从没完没了的生活里探出头，\n这其中成就的，何止是孩子。\n送给所有正处于忙碌中的爸爸妈妈\n","link":"https://idouba.com/learn-and-walk-with-doudou/","section":"posts","tags":["doudou"],"title":"牵一只蜗牛去散步"},{"body":"结合使用整理Oracle的索引，主要权威的来自于Oracle Database Concepts与Oracle Database Performance Tuning Guide\n尝试用最少的字数介绍oracle的几种常用索引原理，主要是想简单分析其存储结构来说明其检索方式，和解释为什们某种索引使用与某种场合。（数据结构中最简单的ArrayList和LinkedList的使用场景）。阐述原因只有一个，就是因为其存储结构决定的。\nB树索引(默认类型) 存储结构： B+树，不多描述。和其他几种关系数据库一样，就是根据索引列(一个或多个)来构造一个B+树来存储索引。非叶子节点两个区域：存储下级子节点的值的范围，和到对应子节点地址（典型B+树的结构），主要作用是导航；叶子节点存储索引的键值和行的ROWID。另外，索引的叶子节点间构成了一个双向链表。类似mysql的myisam引擎的辅助索引，也类似mssql的非聚集索引。\n检索方式： 典型的平衡树的检索，栋根节点开始导航，选择下一个中间节点，直到找到对应的叶节点。需要说明的是，如果检索的所有列都在是索引中，则不用不用检索表，被称为Fast Full Index Scan；如果检索的列不都包含在索引中，则从树上找到索引列对应的key的叶子节点，需要根据其对应的ROWID，再次访问表，根据ROWID关联到其他属性。当Index Range Scan这种检索索引列的某个范围，则不用从根节点开始导航，直接选定开始的叶节点后，直接从叶节点的双向链表就可以完成。只需从根节点导航一次，找到开始的叶节点。\nB+树是一个balance树，所有的叶子都在同一层上，无论根据索引查找表中的哪一条记录，where columnIndex=selectValue中间导航的层数都是相同的，所执行的I/O此次数都是相同的。\n适合应用： 因为树状结构的特征，适合于适合与大量的增、删、改（OLTP），因为树状结构节点分裂、合并等很方便；适合高基数的列（唯一值多）的列索引，数据重叠太多，树状就快变为列表了，其查找功能就不能体现了。\n反向键索引 存储结构： 反向键索引是一种能够将索引键‘反转’的B*Tree索引。通过把键‘反转’，本来连续的键值变得非常‘离散’。当大量数据并行插入的时候，把本来一个索引块上的连续键值分散到不同的索引块上，减少了索引块的争用。\n检索方式： 和普通的B树索引没有差别。\n使用场景 因为存储结构同B树索引，使用场合也同B树索引。但是如WHERE?COL1?\u0026gt;?888这样的Index Range Scan不能用反向索引，因为存储结构决定这种本来连续的已经反向的不连续了。\n位图索引bitmap 存储结构 类似于java的BitSet采用最简单的方式存储某个值在哪些行出现，哪些行不出现。\n如在列性别上建索引。\nValue Row 1 Row 2 Row 3 Row 4 Row 5 Row 6 Row 7 M 1 1 1 1 F 1 1 1 在F行上Row12 row6 row7为1表示这些行的索引列取值是F。搜索一条记录的时候。\n**检索方式 当发出where sex=’F’ 这样的SQL语句时，会去搜索F所在的索引条目，然后扫描该索引条目中的bitmap里所有的bit位。第一个bit位为 1，则说明第一条记录上的C1值为01，于是返回第一条记录所在的ROWID（根据该索引条目里记录的start ROWID加上行号得到该记录所在的ROWID）。然后根据ROWID关联其他属性。\n适用应用： 适用于基数的列(low?cardinality)，因为存储的索引块会比较少。因此不适用创建复合索引，复合索引包含多列，它们的组合一般来说已经是高基数。\n适合读操作多，写操作少的，比较适合用在数据仓库系统里，不适合用在OLTP系统。因为与B*树索引（更改操作仅锁定一行）不同的是，如果一个会话更新了位图索引所索引的数据，会把该位图索引条 目指向所有的行都锁定。无法锁定一个位图索引条目中的一位；而是会锁定整个位图索引条目。倘若有其它会话也要更改同样的这个索引条目指向的行，阻塞。而且更改一个记录涉及到把原记录的bitmap对应的bit设置成0，新值所在的bitmap的bit值设置为1，这样两条记录的操作。\n位图连接索引 在创建位图连接索引时,它是两个表或多个表之间的索引值的连接,连接的结果存储在索引自身中。通过链接使用B表的列来对A表建立索引。本质上还是位图索引。根据Sales对应的cust_id和Customers上cust_id连接在customers表上建立索引。其实就是在customers表上的以个伪列。\n检索方式 询时通过扫描索引(避免两表或多表全表扫描)来获取数据。能够消除查询中的连接操作、因为它实际上已经将连接的结果集保存在索引当中了。\n使用要求： 创建位图连接索引时WHERE 子句中的关联条件列必须是主键或唯一约束。\n基于函数的索引 存储结构 函索索引计算函数或者表达式的值，并保存到索引中。创建的函数索引可以是B树的，也可以是Bitmap的。\n检索方式 当检索语句中包含该函数或者表达式，会使用该函数索引。Inert update等写操作时要更新索引中的函数值。\n适用应用： 用这种索引能提前计算并存储复杂的值，因此可以用来加快现有应用的查询速度，而且不用修改应用中的任何逻辑或查询。\n索引组织表 存储结构 以B树的方式存储表的数据。索引组织表的数据是根据主键排序后的顺序进行排列的，这样就提高了访问的速度。类似于mysql的innodb存储引擎的主索引。\n检索方式 检索方式 定位到主键就定位到了存储位置的数据行。如果直接是根据主键定位则该存储方式也可以理解为一种索引（mssql中聚集索引也是这个意思），如果根据其他列的查询，可以结合在其他列上建立辅助索引（和mysql的Innodb辅助索引是几乎完全相同的思路）\n适用应用： 对于总是通过对主键的精确匹配或范围扫描进行访问的表，直接根据范围定位存储位置，就可以获取对应行的数据。因为索引项和数据存储在一起，无论是基于主键的等值查询还是范围查询都能大大节省磁盘访问时间。ROWID伪列是基于主键值的逻辑rowid，而不是物理rowid，即使表被重新组织过，造成了基表行的迁移，二级索引仍然可用，不需要重建。\n","link":"https://idouba.com/oracle-index-brief/","section":"posts","tags":["索引","Oracle"],"title":"Oracle索引原理精简总结"},{"body":"一、概要描述 shuffle是MapReduce的一个核心过程，因此没有在前面的MapReduce作业提交的过程中描述，而是单独拿出来比较详细的描述。 根据官方的流程图示如下：\n本篇文章中只是想尝试从代码分析来说明在map端是如何将map的输出保存下来等待reduce来取。 在执行每个map task时，无论map方法中执行什么逻辑，最终都是要把输出写到磁盘上。如果没有reduce阶段，则直接输出到hdfs上，如果有有reduce作业，则每个map方法的输出在写磁盘前线在内存中缓存。每个map task都有一个环状的内存缓冲区，存储着map的输出结果，默认100m，在每次当缓冲区快满的时候由一个独立的线程将缓冲区的数据以一个溢出文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有溢出文件做合并，被合并成已分区且已排序的输出文件。然后等待reduce task来拉数据。\n二、 流程描述 在child进程调用到runNewMapper时，会设置output为NewOutputCollector，来负责map的输出。 在map方法的最后，不管经过什么逻辑的map处理，最终一般都要调用到TaskInputOutputContext的write方法，进而调用到设置的output即NewOutputCollector的write方法 NewOutputCollector其实只是对MapOutputBuffer的一个封装，其write方法调用的是MapOutputBuffer的collect方法。 MapOutputBuffer的collect方法中把key和value序列化后存储在一个环形缓存中，如果缓存满了则会调用startspill方法设置信号量，使得一个独立的线程SpillThread可以对缓存中的数据进行处理。 SpillThread线程的run方法中调用sortAndSpill方法对缓存中的数据进行排序后写溢出文件。 当map输出完成后，会调用output的close方法。 在close方法中调用flush方法，对剩余的缓存进行处理，最后调用mergeParts方法，将前面过程的多个溢出文件合并为一个。 ​ Mapreduce shuffle过程之Map输出过程代码流程\n三、代码详细 1. MapTask的runNewMapper方法 注意到有这样一段代码。即当job中只有map没有reduce的时候，这个rg.apache.hadoop.mapreduce.RecordWriter类型的对象 output是一Outputformat中定义的writer，即直接写到输出中。如果是有Reduce，则output是一个NewOutputCollector类型输出。\n1if (job.getNumReduceTasks() == 0) { 2 output = outputFormat.getRecordWriter(taskContext); 3 } else { 4 output = new NewOutputCollector(taskContext, job, umbilical, reporter); 5 } 6 mapperContext = contextConstructor.newInstance(mapper, job, getTaskID(),input, output, committer, reporter, split); 7 input.initialize(split, mapperContext); 8 mapper.run(mapperContext); 和其他的RecordWriter一样，NewOutputCollector也继承自RecordWriter抽象类。除了一个close方法释放资源外，该抽象类定义的最主要的方法就一个void write(K key, V )。即写入key，value。\n2. Mapper的run方法，对每个输出执行map方法。 1 public void run(Context context) throws IOException, InterruptedException { 2 setup(context); 3 while (context.nextKeyValue()) { 4 map(context.getCurrentKey(), context.getCurrentValue(), context); 5 } 6 cleanup(context); 3. Mapper的map方法，默认是直接把key和value写入 1protected void map(KEYIN key, VALUEIN value, 2 Context context) throws IOException, InterruptedException { 3 context.write((KEYOUT) key, (VALUEOUT) value); 4 } 一般使用中会做很多我们需要的操作，如著名的wordcount中，把一行单词切分后，数一（value都设为_one_ = new IntWritable(1)），但最终都是要把结果写入。即调用context.write(key,value)\n1public void map(Object key, Text value, Context context 2 ) throws IOException, InterruptedException { 3 StringTokenizer itr = new StringTokenizer(value.toString()); 4 while (itr.hasMoreTokens()) { 5 word.set(itr.nextToken()); 6 context.write(word, one); 7 } 8 } 4. TaskInputOutputContext的write方法。 调用的是contex中的RecordWriter的write方法。即调用的是NewOutputCollector的write方法。\n1public void write(KEYOUT key, VALUEOUT value 2 ) throws IOException, InterruptedException { 3 output.write(key, value); 4} 5. NewOutputCollector的write方法 1public void write(K key, V value) throws IOException, InterruptedException { 2 collector.collect(key, value, 3 partitioner.getPartition(key, value, partitions)); 4} 从方法名上不难看出提供写数据的是MapOutputCollector\u0026lt;K,V\u0026gt;类型的 collector对象.从NewOutputCollector的构造函数中看到collector的初始化。\n1collector = new MapOutputBuffer\u0026lt;K,V\u0026gt;(umbilical, job, reporter); 6. MapOutputBuffer的构造函数 在了解MapOutputBuffer的collect方法前，先了解下期构造函数，看做了哪些初始化。\n1public MapOutputBuffer(TaskUmbilicalProtocol umbilical, JobConf job, 2 TaskReporter reporter 3 ) throws IOException, ClassNotFoundException { 4 this.job = job; 5 this.reporter = reporter; 6 localFs = FileSystem.getLocal(job); 7 //1）设定map的分区数，即作业 配置中的的reduce数 8 partitions = job.getNumReduceTasks(); 9 rfs = ((LocalFileSystem)localFs).getRaw(); 10 indexCacheList = new ArrayList\u0026lt;SpillRecord\u0026gt;(); 11 12 //2）重要的参数 13 final float spillper = job.getFloat(\u0026#34;io.sort.spill.percent\u0026#34;,(float)0.8); 14 final float recper = job.getFloat(\u0026#34;io.sort.record.percent\u0026#34;,(float)0.05); 15 final int sortmb = job.getInt(\u0026#34;io.sort.mb\u0026#34;, 100); 16 if (spillper \u0026gt; (float)1.0 || spillper \u0026lt; (float)0.0) { 17 throw new IOException(\u0026#34;Invalid \\\u0026#34;io.sort.spill.percent\\\u0026#34;: \u0026#34; + spillper); 18 } 19 if (recper \u0026gt; (float)1.0 || recper \u0026lt; (float)0.01) { 20 throw new IOException(\u0026#34;Invalid \\\u0026#34;io.sort.record.percent\\\u0026#34;: \u0026#34; + recper); 21 } 22 if ((sortmb \u0026amp; 0x7FF) != sortmb) { 23 throw new IOException(\u0026#34;Invalid \\\u0026#34;io.sort.mb\\\u0026#34;: \u0026#34; + sortmb); 24 } 25 //3)sorter,使用其对map的输出在partition内进行内排序。 26 sorter = ReflectionUtils.newInstance( 27 job.getClass(\u0026#34;map.sort.class\u0026#34;, QuickSort.class, IndexedSorter.class), job); 28 LOG.info(\u0026#34;io.sort.mb = \u0026#34; + sortmb); 29 // buffers and accounting 30 //把单位是M的sortmb设定左移20，还原单位为个 31 int maxMemUsage = sortmb \u0026lt;\u0026lt; 20; 32 int recordCapacity = (int)(maxMemUsage * recper); 33 recordCapacity -= recordCapacity % RECSIZE; 34 //输出缓存 35 kvbuffer = new byte[maxMemUsage - recordCapacity]; 36 bufvoid = kvbuffer.length; 37 recordCapacity /= RECSIZE; 38 kvoffsets = new int[recordCapacity]; 39 kvindices = new int[recordCapacity * ACCTSIZE]; 40 softBufferLimit = (int)(kvbuffer.length * spillper); 41 softRecordLimit = (int)(kvoffsets.length * spillper); 42 LOG.info(\u0026#34;data buffer = \u0026#34; + softBufferLimit + \u0026#34;/\u0026#34; + kvbuffer.length); 43 LOG.info(\u0026#34;record buffer = \u0026#34; + softRecordLimit + \u0026#34;/\u0026#34; + kvoffsets.length); 44 // k/v serialization 45 comparator = job.getOutputKeyComparator(); 46 keyClass = (Class\u0026lt;K\u0026gt;)job.getMapOutputKeyClass(); 47 valClass = (Class\u0026lt;V\u0026gt;)job.getMapOutputValueClass(); 48 serializationFactory = new SerializationFactory(job); 49 keySerializer = serializationFactory.getSerializer(keyClass); 50 keySerializer.open(bb); 51 valSerializer = serializationFactory.getSerializer(valClass); 52 valSerializer.open(bb); 53 // counters 54 mapOutputByteCounter = reporter.getCounter(MAP_OUTPUT_BYTES); 55 mapOutputRecordCounter = reporter.getCounter(MAP_OUTPUT_RECORDS); 56 Counters.Counter combineInputCounter = 57 reporter.getCounter(COMBINE_INPUT_RECORDS); 58 combineOutputCounter = reporter.getCounter(COMBINE_OUTPUT_RECORDS); 59 // 4）compression 60 if (job.getCompressMapOutput()) { 61 Class\u0026lt;? extends CompressionCodec\u0026gt; codecClass = 62 job.getMapOutputCompressorClass(DefaultCodec.class); 63 codec = ReflectionUtils.newInstance(codecClass, job); 64 } 65 // 5）combiner是一个NewCombinerRunner类型，调用Job的reducer来对map的输出在map端进行combine。 66 combinerRunner = CombinerRunner.create(job, getTaskID(), 67 combineInputCounter, 68 reporter, null); 69 if (combinerRunner != null) { 70 combineCollector= new CombineOutputCollector\u0026lt;K,V\u0026gt;(combineOutputCounter); 71 } else { 72 combineCollector = null; 73 } 74 //6)启动一个SpillThread线程来 75 minSpillsForCombine = job.getInt(\u0026#34;min.num.spills.for.combine\u0026#34;, 3); 76 spillThread.setDaemon(true); 77 spillThread.setName(\u0026#34;SpillThread\u0026#34;); 78 spillLock.lock(); 79 try { 80 spillThread.start(); 81 while (!spillThreadRunning) { 82 spillDone.await(); 83 } 84 } catch (InterruptedException e) { 85 throw (IOException)new IOException(\u0026#34;Spill thread failed to initialize\u0026#34; 86 ).initCause(sortSpillException); 87 } finally { 88 spillLock.unlock(); 89 } 90 if (sortSpillException != null) { 91 throw (IOException)new IOException(\u0026#34;Spill thread failed to initialize\u0026#34; 92 ).initCause(sortSpillException); 93 } 94} 7. MapOutputBuffer的collect方法。 参数partition是partitioner根据key计算得到的当前key value属于的partition索引。写key和value写入缓存，当缓存满足spill条件时，通过调用startSpill方法设置变量并通过spillReady.signal()，通知spillThread；并等待spill结束（通过spillDone.await()等待）缓冲区的作用是批量收集map结果，减少磁盘IO的影响。key/value对以及Partition的结果都会被写入缓冲区。写入之前，key与value值都会被序列化成字节数组。kvindices保持了记录所属的分区，key在缓冲区开始的位置和value在缓冲区开始的位置，通过kvindices，可以在缓冲区中找到对应的记录。 输出缓冲区中，和kvstart，kvend和kvindex对应的是bufstart，bufend和bufmark。这部分还涉及到变量bufvoid，用于表明实际使用的缓冲区结尾和变量bufmark，用于标记记录的结尾。需要bufmark，是因为key或value的输出是变长的。\n​ Key Value序列化后缓存\n1public synchronized void collect(K key, V value, int partition 2 ) throws IOException { 3 reporter.progress(); 4 if (key.getClass() != keyClass) { 5 throw new IOException(\u0026#34;Type mismatch in key from map: expected \u0026#34; 6 + keyClass.getName() + \u0026#34;, recieved \u0026#34; 7 + key.getClass().getName()); 8 } 9 if (value.getClass() != valClass) { 10 throw new IOException(\u0026#34;Type mismatch in value from map: expected \u0026#34; 11 + valClass.getName() + \u0026#34;, recieved \u0026#34; 12 + value.getClass().getName()); 13 } 14//对kvoffsets的长度取模，暗示我们这是一个环形缓存。 15final int kvnext = (kvindex + 1) % kvoffsets.length; 16 //进入临界区 17spillLock.lock(); 18 try { 19 boolean kvfull; 20 do { 21 if (sortSpillException != null) { 22 throw (IOException)new IOException(\u0026#34;Spill failed\u0026#34; 23 ).initCause(sortSpillException); 24 } 25 // sufficient acct space 26 kvfull = kvnext == kvstart; 27 final boolean kvsoftlimit = ((kvnext \u0026gt; kvend) 28 ? kvnext - kvend \u0026gt; softRecordLimit 29 : kvend - kvnext \u0026lt;= kvoffsets.length - softRecordLimit); 30 if (kvstart == kvend \u0026amp;\u0026amp; kvsoftlimit) { 31 LOG.info(\u0026#34;Spilling map output: record full = \u0026#34; + kvsoftlimit); 32//其实是设置变量并通过spillReady.signal()，通知spillThread；并等待spill结束 33 startSpill(); 34 } 35 if (kvfull) { 36 try { 37 while (kvstart != kvend) { 38//kvstart不等于kvend，表示系统正在spill,等待spillDone信号 39 reporter.progress(); 40 spillDone.await(); 41 } 42 } catch (InterruptedException e) { 43 throw (IOException)new IOException( 44 \u0026#34;Collector interrupted while waiting for the writer\u0026#34; 45 ).initCause(e); 46 } 47 } 48 } while (kvfull); 49 } finally { 50 spillLock.unlock(); 51 } 52 53 try { 54//先对key串行化，然后对value做串行化，临时变量keystart，valstart和valend分别记录了key结果的开始位置，value结果的开始位置和value结果的结束位置。串行化过程中，往缓冲区写是最终调用了Buffer.write方法 55 // serialize key bytes into buffer 56 int keystart = bufindex; 57 keySerializer.serialize(key); 58 if (bufindex \u0026lt; keystart) { 59 //如果key串行化后出现bufindex \u0026lt; keystart，那么会调用BlockingBuffer的reset方法。原因是在spill的过程中需要对\u0026lt;key，value\u0026gt;排序，这种情况下，传递给RawComparator的必须是连续的二进制缓冲区，通过BlockingBuffer.reset方法 会把bufvoid设置为bufmark，缓冲区开始部分往后挪，然后将原来位于bufmark到bufvoid出的结果，拷到缓冲区开始处，这样的话，key串行化的结果就连续存放在缓冲区的最开始处。 60 bb.reset(); 61 keystart = 0; 62 } 63 // serialize value bytes into buffer 64 final int valstart = bufindex; 65 valSerializer.serialize(value); 66 int valend = bb.markRecord(); 67 68 if (partition \u0026lt; 0 || partition \u0026gt;= partitions) { 69 throw new IOException(\u0026#34;Illegal partition for \u0026#34; + key + \u0026#34; (\u0026#34; + 70 partition + \u0026#34;)\u0026#34;); 71 } 72 73 mapOutputRecordCounter.increment(1); 74 mapOutputByteCounter.increment(valend \u0026gt;= keystart 75 ? valend - keystart 76 : (bufvoid - keystart) + valend); 77 78 // update accounting info 79 int ind = kvindex * ACCTSIZE; 80 kvoffsets[kvindex] = ind; 81 kvindices[ind + PARTITION] = partition; 82 kvindices[ind + KEYSTART] = keystart; 83 kvindices[ind + VALSTART] = valstart; 84 kvindex = kvnext; 85 } catch (MapBufferTooSmallException e) { 86 LOG.info(\u0026#34;Record too large for in-memory buffer: \u0026#34; + e.getMessage()); 87//如果value的串行化结果太大，不能一次放入缓冲区 88 spillSingleRecord(key, value, partition); 89 mapOutputRecordCounter.increment(1); 90 return; 91 } 92} 8. MapOutputBuffer.BlockingBuffer的reset()方法. 如果key串行化后出现bufindex \u0026lt; keystart，那么会调用BlockingBuffer的reset方法。原因是在spill的过程中需要对\u0026lt;key，value\u0026gt;排序，这种情况下，传递给RawComparator的必须是连续的二进制缓冲区，通过BlockingBuffer.reset方法当发现key的串行化结果出现不连续的情况时，会把bufvoid设置为bufmark，缓冲区开始部分往后挪，然后将原来位于bufmark到bufvoid出的结果，拷到缓冲区开始处，这样的话，key串行化的结果就连续存放在缓冲区的最开始处。\n​ BlockingBuffer.reset方法\nbufstart前面的缓冲区如果不能放下整个key串行化的结果，，处理的方式是将bufindex置0，然后调用BlockingBuffer内部的out的write方法直接输出，这实际调用了Buffer.write方法，会启动spill过程，最终会成功写入key串行化的结果。\n1protected synchronized void reset() throws IOException { 2 3 int headbytelen = bufvoid - bufmark; 4 bufvoid = bufmark; 5 //当发现key的串行化结果出现不连续的情况时，会把bufvoid设置为bufmark，缓冲区开始部分往后挪，然后将原来位于bufmark到bufvoid出的结果，拷到缓冲区开始处，这样的话，key串行化的结果就连续存放在缓冲区的最开始处。 6 7 if (bufindex + headbytelen \u0026lt; bufstart) { 8 System.arraycopy(kvbuffer, 0, kvbuffer, headbytelen, bufindex); 9 System.arraycopy(kvbuffer, bufvoid, kvbuffer, 0, headbytelen); 10 bufindex += headbytelen; 11 } else { 12 //bufstart前面的缓冲区如果不能够放下整个key串行化的结果，处理的方式是将bufindex置0，然后调用BlockingBuffer内部的out的write方法直接输出 13 byte[] keytmp = new byte[bufindex]; 14 System.arraycopy(kvbuffer, 0, keytmp, 0, bufindex); 15 bufindex = 0; 16 out.write(kvbuffer, bufmark, headbytelen); 17 out.write(keytmp); 18 } 19 } 20 } 9. MapOutputBuffer.Buffer的write方法。 在key和value序列列化的时候，被调用写到缓存中。如果spill线程正在把缓存的数据写溢出文件，则阻塞。\n1public synchronized void write(byte b[], int off, int len) 2 throws IOException { 3 boolean buffull = false; 4 boolean wrap = false; 5 spillLock.lock(); 6 try { 7 do {//循环，直到有足够的空间可以写数据 8 if (sortSpillException != null) { 9 throw (IOException)new IOException(\u0026#34;Spill failed\u0026#34; 10 ).initCause(sortSpillException); 11 } 12 13 // sufficient buffer space? 14 if (bufstart \u0026lt;= bufend \u0026amp;\u0026amp; bufend \u0026lt;= bufindex) { 15 buffull = bufindex + len \u0026gt; bufvoid; 16 wrap = (bufvoid - bufindex) + bufstart \u0026gt; len; 17 } else { 18 // bufindex \u0026lt;= bufstart \u0026lt;= bufend 19 // bufend \u0026lt;= bufindex \u0026lt;= bufstart 20 wrap = false; 21 buffull = bufindex + len \u0026gt; bufstart; 22 } 23 24 if (kvstart == kvend) { 25 // spill thread not running 26 if (kvend != kvindex) { 27 //如果数组中有记录(kvend != kvindex)，那么，根据需要（目前输出空间不足或记录数达到spill条件）启动spill过程 28 final boolean bufsoftlimit = (bufindex \u0026gt; bufend) 29 ? bufindex - bufend \u0026gt; softBufferLimit 30 : bufend - bufindex \u0026lt; bufvoid - softBufferLimit; 31 if (bufsoftlimit || (buffull \u0026amp;\u0026amp; !wrap)) { 32 LOG.info(\u0026#34;Spilling map output: buffer full= \u0026#34; + bufsoftlimit); 33 startSpill(); 34 } 35 } else if (buffull \u0026amp;\u0026amp; !wrap) { 36 // 如果空间不够（buffull \u0026amp;\u0026amp; !wrap），但是缓存中没有记录，表明这个记录非常大，内存缓冲区不能容下这么大的数据量，抛MapBufferTooSmallException异常，直接写文件不用写缓存 37 final int size = ((bufend \u0026lt;= bufindex) 38 ? bufindex - bufend 39 : (bufvoid - bufend) + bufindex) + len; 40 bufstart = bufend = bufindex = bufmark = 0; 41 kvstart = kvend = kvindex = 0; 42 bufvoid = kvbuffer.length; 43 throw new MapBufferTooSmallException(size + \u0026#34; bytes\u0026#34;); 44 } 45 } 46 47 if (buffull \u0026amp;\u0026amp; !wrap) { 48 try { 49 //如果空间不足但是spill在运行，等待spillDone 50 while (kvstart != kvend) { 51 reporter.progress(); 52 spillDone.await(); 53 } 54 } catch (InterruptedException e) { 55 throw (IOException)new IOException( 56 \u0026#34;Buffer interrupted while waiting for the writer\u0026#34; 57 ).initCause(e); 58 } 59 } 60 } while (buffull \u0026amp;\u0026amp; !wrap); 61 } finally { 62 spillLock.unlock(); 63 } 64 //真正把数据写缓存的地方！如果buffull，则写数据会不连续，则写满剩余缓冲区，然后设置bufindex=0，并从bufindex处接着写。否则，就是从bufindex处开始写。 65 if (buffull) { 66 //缓存剩余长度 67 final int gaplen = bufvoid - bufindex; 68 //把剩余的写满 69 System.arraycopy(b, off, kvbuffer, bufindex, gaplen); 70 //剩下长度 71 len -= gaplen; 72 //剩下偏移 73 off += gaplen; 74 //写指针移到开头 75 bufindex = 0; 76 } 77 从指定的开头写 78 System.arraycopy(b, off, kvbuffer, bufindex, len); 79 bufindex += len; 80 } 81 } ​ buffull和wrap条件说明\n如图，对bufful和wrap条件进行说明： 在上面两种情况下，即情况1和情况2，\n1buffull = bufindex + len \u0026gt; bufvoid; 2wrap = (bufvoid - bufindex) + bufstart \u0026gt; len; buffull条件判断为从下次写指针的位置bufindex到缓存结束bufvoid的空间是否有足够的空间容纳写的内容，wrap是图中白颜色部分的空间（前后空白合在一起）是否比输入大，如果是，wrap为true； 情况3和情况4中，\n1wrap = false; 2buffull = bufindex + len \u0026gt; bufstart; buffull判断bufindex到bufstart的空间是否满足条件，而wrap肯定是false。 条件（buffull \u0026amp;\u0026amp; !wrap）满足时，目前的空间不够一次写。\n10. MapOutputBuffer 的spillSingleRecord方法。 如果在collect方法中处理缓存失败，则直接把这条记录些到spill文件中。对应单条记录即使设置了combiner也不用。如果记录非常大，内存缓冲区不能容下这么大的数据量，抛MapBufferTooSmallException异常，直接写文件不用写缓存。\n1private void spillSingleRecord(final K key, final V value, 2 int partition) throws IOException { 3 long size = kvbuffer.length + partitions * APPROX_HEADER_LENGTH; 4 FSDataOutputStream out = null; 5 try { 6 // 创建spill文件 7 final SpillRecord spillRec = new SpillRecord(partitions); 8 final Path filename = mapOutputFile.getSpillFileForWrite(getTaskID(), 9 numSpills, size); 10 out = rfs.create(filename); 11 12 IndexRecord rec = new IndexRecord(); 13 for (int i = 0; i \u0026lt; partitions; ++i) { 14 IFile.Writer\u0026lt;K, V\u0026gt; writer = null; 15 try { 16 long segmentStart = out.getPos(); 17 writer = new IFile.Writer\u0026lt;K,V\u0026gt;(job, out, keyClass, valClass, codec, 18 spilledRecordsCounter); 19 20 if (i == partition) { 21 final long recordStart = out.getPos(); 22 writer.append(key, value); 23 mapOutputByteCounter.increment(out.getPos() - recordStart); 24 } 25 writer.close(); 26 27 // 把偏移记录在index中 28 rec.startOffset = segmentStart; 29 rec.rawLength = writer.getRawLength(); 30 rec.partLength = writer.getCompressedLength(); 31 spillRec.putIndex(rec, i); 32 33 writer = null; 34 } catch (IOException e) { 35 if (null != writer) writer.close(); 36 throw e; 37 } 38 } 39 //如果index满了，则把index也写到index文件中。没满则把该条index记录加入到indexCacheList中，并更新totalIndexCacheMemory。 40 if (totalIndexCacheMemory \u0026gt;= INDEX_CACHE_MEMORY_LIMIT) { 41 // create spill index file 42 Path indexFilename = mapOutputFile.getSpillIndexFileForWrite( 43 getTaskID(), numSpills, 44 partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH); 45 spillRec.writeToFile(indexFilename, job); 46 } else { 47 indexCacheList.add(spillRec); 48 totalIndexCacheMemory += 49 spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH; 50 } 51 ++numSpills; 52 } finally { 53 if (out != null) out.close(); 54 } 55 } 11. MapOutputBuffer的startSpill。 唤醒等待spillReady的线程。\n1private synchronized void startSpill() { 2 LOG.info(\u0026#34;bufstart = \u0026#34; + bufstart + \u0026#34;; bufend = \u0026#34; + bufmark + 3 \u0026#34;; bufvoid = \u0026#34; + bufvoid); 4 LOG.info(\u0026#34;kvstart = \u0026#34; + kvstart + \u0026#34;; kvend = \u0026#34; + kvindex + 5 \u0026#34;; length = \u0026#34; + kvoffsets.length); 6 kvend = kvindex; 7 bufend = bufmark; 8 spillReady.signal(); 9 } 12. SpillThread的run方法。 该Thread会检查内存中的输出缓存区，在满足一定条件的时候将缓冲区中的内容spill到硬盘上。这是一个标准的生产者-消费者模型，MapTask的collect方法是生产者，spillThread是消费者，它们之间同步是通过spillLock（ReentrantLock）和spillLock上的两个条件变量（spillDone和spillReady）完成的。当kvstart == kvend条件成立时，表示没有要spill的记录。\n1public void run() { 2 //临界区 3 spillLock.lock(); 4 spillThreadRunning = true; 5 try { 6 while (true) { 7 spillDone.signal(); 8当kvstart == kvend条件成立时，表示没有要spill的记录 9 while (kvstart == kvend) { 10 spillReady.await(); 11 } 12 try { 13 spillLock.unlock(); 14 //执行操作 15 sortAndSpill(); 16 } catch (Exception e) { 17 sortSpillException = e; 18 } catch (Throwable t) { 19 sortSpillException = t; 20 String logMsg = \u0026#34;Task \u0026#34; + getTaskID() + \u0026#34; failed : \u0026#34; 21 + StringUtils.stringifyException(t); 22 reportFatalError(getTaskID(), t, logMsg); 23 } finally { 24 spillLock.lock(); 25 if (bufend \u0026lt; bufindex \u0026amp;\u0026amp; bufindex \u0026lt; bufstart) { 26 bufvoid = kvbuffer.length; 27 } 28 kvstart = kvend; 29 bufstart = bufend; 30 } 31 } 32 } catch (InterruptedException e) { 33 Thread.currentThread().interrupt(); 34 } finally { 35 spillLock.unlock(); 36 spillThreadRunning = false; 37 } 38 } 39 } 13..MapOutputBuffer的sortAndSpill() 方法 SpillThread线程的run方法中调用sortAndSpill把缓存中的输出写到格式为+ “/spill” + spillNumber + “.out”的spill文件中。索引（kvindices）保持在spill{spill号}.out.index中，数据保存在spill{spill号}.out中\n创建SpillRecord记录，输出文件和IndexRecord记录，然后，需要在kvoffsets上做排序，排完序后顺序访问kvoffsets，也就是按partition顺序访问记录。按partition循环处理排完序的数组，如果没有combiner，则直接输出记录，否则，调用combineAndSpill，先做combin然后输出。循环的最后记录IndexRecord到SpillRecord。\n1private void sortAndSpill() throws IOException, ClassNotFoundException, 2 InterruptedException { 3 //approximate the length of the output file to be the length of the 4 //buffer + header lengths for the partitions 5 long size = (bufend \u0026gt;= bufstart 6 ? bufend - bufstart 7 : (bufvoid - bufend) + bufstart) + 8 partitions * APPROX_HEADER_LENGTH; 9 FSDataOutputStream out = null; 10 try { 11 // 创建溢出文件 12 final SpillRecord spillRec = new SpillRecord(partitions); 13 final Path filename = mapOutputFile.getSpillFileForWrite(getTaskID(), 14 numSpills, size); 15 out = rfs.create(filename); 16 17 final int endPosition = (kvend \u0026gt; kvstart) 18 ? kvend 19 : kvoffsets.length + kvend; 20//使用sorter进行排序, 在内存中进行，参照MapOutputBuffer的compare方法实现的这里的排序也是对序列化的字节做的排序。排序是在kvoffsets上面进行，参照MapOutputBuffer的swap方法实现。 21 sorter.sort(MapOutputBuffer.this, kvstart, endPosition, reporter); 22 int spindex = kvstart; 23 IndexRecord rec = new IndexRecord(); 24 InMemValBytes value = new InMemValBytes(); 25 for (int i = 0; i \u0026lt; partitions; ++i) { 26 IFile.Writer\u0026lt;K, V\u0026gt; writer = null; 27 try { 28 long segmentStart = out.getPos(); 29 writer = new Writer\u0026lt;K, V\u0026gt;(job, out, keyClass, valClass, codec, 30 spilledRecordsCounter); 31 if (combinerRunner == null) { 32 // 如果没有combinner则直接写键值 33 DataInputBuffer key = new DataInputBuffer(); 34 while (spindex \u0026lt; endPosition \u0026amp;\u0026amp; 35 kvindices[kvoffsets[spindex % kvoffsets.length] 36 + PARTITION] == i) { 37 final int kvoff = kvoffsets[spindex % kvoffsets.length]; 38 getVBytesForOffset(kvoff, value); 39 key.reset(kvbuffer, kvindices[kvoff + KEYSTART], 40 (kvindices[kvoff + VALSTART] - 41 kvindices[kvoff + KEYSTART])); 42 //键值写到溢出文件 43 writer.append(key, value); 44 ++spindex; 45 } 46 } else { 47 int spstart = spindex; 48 while (spindex \u0026lt; endPosition \u0026amp;\u0026amp; 49 kvindices[kvoffsets[spindex % kvoffsets.length] 50 + PARTITION] == i) { 51 ++spindex; 52 } 53 //如果设置了combiner，则调用了combine方法后的结果写到IFile中，writer还是先前的writer。减少溢写到磁盘的数据量。 54 if (spstart != spindex) { 55 combineCollector.setWriter(writer); 56 RawKeyValueIterator kvIter = 57 new MRResultIterator(spstart, spindex); 58 combinerRunner.combine(kvIter, combineCollector); 59 } 60 } 61 62 // close the writer 63 writer.close(); 64 65 // record offsets 66 rec.startOffset = segmentStart; 67 rec.rawLength = writer.getRawLength(); 68 rec.partLength = writer.getCompressedLength(); 69 spillRec.putIndex(rec, i); 70 71 writer = null; 72 } finally { 73 if (null != writer) writer.close(); 74 } 75 } 76 77 if (totalIndexCacheMemory \u0026gt;= INDEX_CACHE_MEMORY_LIMIT) { 78 // 写溢出索引文件，格式如+ \u0026#34;/spill\u0026#34; + spillNumber + \u0026#34;.out.index\u0026#34; 79 Path indexFilename = mapOutputFile.getSpillIndexFileForWrite( 80 getTaskID(), numSpills, 81 partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH); 82 spillRec.writeToFile(indexFilename, job); 83 } else { 84 indexCacheList.add(spillRec); 85 totalIndexCacheMemory += 86 spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH; 87 } 88 LOG.info(\u0026#34;Finished spill \u0026#34; + numSpills); 89 ++numSpills; 90 } finally { 91 if (out != null) out.close(); 92 } 93 } 14. MapOutputBuffer的compare方法和swap方法 MapOutputBuffer实现了IndexedSortable接口，从接口命名上就可以猜想到，这个排序不是移动数据，而是移动数据的索引。在这里要排序的其实是kvindices对象，通过移动其记录在kvoffets上的索引来实现。\n如图，表示了写磁盘前Sort的效果。kvindices保持了记录所属的（Reduce）分区，key在缓冲区开始的位置和value在缓冲区开始的位置，通过kvindices，我们可以在缓冲区中找到对应的记录。kvoffets用于在缓冲区满的时候对kvindices的partition进行排序，排完序的结果将输出到输出到本地磁盘上，其中索引（kvindices）保持在spill{spill号}.out.index中，数据保存在spill{spill号}.out中。通过观察MapOutputBuffer的compare知道，先是在partition上排序，然后是在key上排序。\n​ kvindices在kvoffets上排序\n1public int compare(int i, int j) { 2 final int ii = kvoffsets[i % kvoffsets.length]; 3 final int ij = kvoffsets[j % kvoffsets.length]; 4 // 先在partition上排序 5 if (kvindices[ii + PARTITION] != kvindices[ij + PARTITION]) { 6 return kvindices[ii + PARTITION] - kvindices[ij + PARTITION]; 7 } 8 // 然后在可以上排序 9 return comparator.compare(kvbuffer, 10 kvindices[ii + KEYSTART], 11 kvindices[ii + VALSTART] - kvindices[ii + KEYSTART], 12 kvbuffer, 13 kvindices[ij + KEYSTART], 14 kvindices[ij + VALSTART] - kvindices[ij + KEYSTART]); 15 } 16 17 public void swap(int i, int j) { 18 i %= kvoffsets.length; 19 j %= kvoffsets.length; 20 //通过交互在kvoffsets上的索引达到排序效果 21 int tmp = kvoffsets[i]; 22 kvoffsets[i] = kvoffsets[j]; 23 kvoffsets[j] = tmp; 24 } 15. MapOutputBuffer的flush() 方法 Mapper的结果都已经collect了，需要对缓冲区做一些最后的清理，调用flush方法，合并spill{n}文件产生最后的输出。先等待可能的spill过程完成，然后判断缓冲区是否为空，如果不是，则调用sortAndSpill，做最后的spill，然后结束spill线程.\n1public synchronized void flush() throws IOException, ClassNotFoundException, 2 InterruptedException { 3 LOG.info(\u0026#34;Starting flush of map output\u0026#34;); 4 spillLock.lock(); 5 try { 6 while (kvstart != kvend) { 7 reporter.progress(); 8 spillDone.await(); 9 } 10 if (sortSpillException != null) { 11 throw (IOException)new IOException(\u0026#34;Spill failed\u0026#34; 12 ).initCause(sortSpillException); 13 } 14 if (kvend != kvindex) { 15 kvend = kvindex; 16 bufend = bufmark; 17 sortAndSpill(); 18 } 19 } catch (InterruptedException e) { 20 throw (IOException)new IOException( 21 \u0026#34;Buffer interrupted while waiting for the writer\u0026#34; 22 ).initCause(e); 23 } finally { 24 spillLock.unlock(); 25 } 26 assert !spillLock.isHeldByCurrentThread(); 27 28 try { 29 spillThread.interrupt(); 30 spillThread.join(); 31 } catch (InterruptedException e) { 32 throw (IOException)new IOException(\u0026#34;Spill failed\u0026#34; 33 ).initCause(e); 34 } 35 // release sort buffer before the merge 36 kvbuffer = null; 37 mergeParts(); 38 } 16. MapTask.MapOutputBuffer的mergeParts()方法. 从不同溢写文件中读取出来的，然后再把这些值加起来。因为merge是将多个溢写文件合并到一个文件，所以可能也有相同的key存在，在这个过程中如果配置设置过Combiner，也会使用Combiner来合并相同的key。?mapreduce让每个map只输出一个文件，并且为这个文件提供一个索引文件，以记录每个reduce对应数据的偏移量。\n1private void mergeParts() throws IOException, InterruptedException, 2 ClassNotFoundException { 3 // get the approximate size of the final output/index files 4 long finalOutFileSize = 0; 5 long finalIndexFileSize = 0; 6 final Path[] filename = new Path[numSpills]; 7 final TaskAttemptID mapId = getTaskID(); 8 9 for(int i = 0; i \u0026lt; numSpills; i++) { 10 filename[i] = mapOutputFile.getSpillFile(mapId, i); 11 finalOutFileSize += rfs.getFileStatus(filename[i]).getLen(); 12 } 13 if (numSpills == 1) { //如果只有一个spill文件，则重命名为输出的最终文件 14 rfs.rename(filename[0], 15 new Path(filename[0].getParent(), \u0026#34;file.out\u0026#34;)); 16 if (indexCacheList.size() == 0) { 17 rfs.rename(mapOutputFile.getSpillIndexFile(mapId, 0), 18 new Path(filename[0].getParent(),\u0026#34;file.out.index\u0026#34;)); 19 } else { 20 indexCacheList.get(0).writeToFile( 21 new Path(filename[0].getParent(),\u0026#34;file.out.index\u0026#34;), job); 22 } 23 return; 24 } 25 26 // read in paged indices 27 for (int i = indexCacheList.size(); i \u0026lt; numSpills; ++i) { 28 Path indexFileName = mapOutputFile.getSpillIndexFile(mapId, i); 29 indexCacheList.add(new SpillRecord(indexFileName, job)); 30 } 31 32 //make correction in the length to include the sequence file header 33 //lengths for each partition 34 finalOutFileSize += partitions * APPROX_HEADER_LENGTH; 35 finalIndexFileSize = partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH; 36 Path finalOutputFile = mapOutputFile.getOutputFileForWrite(mapId, 37 finalOutFileSize); 38 Path finalIndexFile = mapOutputFile.getOutputIndexFileForWrite( 39 mapId, finalIndexFileSize); 40 41 //The output stream for the final single output file 42 FSDataOutputStream finalOut = rfs.create(finalOutputFile, true, 4096); 43 44 if (numSpills == 0) { 45 //如果没有spill文件，则创建一个 dummy files 46 IndexRecord rec = new IndexRecord(); 47 SpillRecord sr = new SpillRecord(partitions); 48 try { 49 for (int i = 0; i \u0026lt; partitions; i++) { 50 long segmentStart = finalOut.getPos(); 51 Writer\u0026lt;K, V\u0026gt; writer = 52 new Writer\u0026lt;K, V\u0026gt;(job, finalOut, keyClass, valClass, codec, null); 53 writer.close(); 54 rec.startOffset = segmentStart; 55 rec.rawLength = writer.getRawLength(); 56 rec.partLength = writer.getCompressedLength(); 57 sr.putIndex(rec, i); 58 } 59 sr.writeToFile(finalIndexFile, job); 60 } finally { 61 finalOut.close(); 62 } 63 return; 64 } 65 { 66 IndexRecord rec = new IndexRecord(); 67 final SpillRecord spillRec = new SpillRecord(partitions); 68 for (int parts = 0; parts \u0026lt; partitions; parts++) { 69 //在循环内对每个分区分别创建segment然后做merge 70 List\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt; segmentList = 71 new ArrayList\u0026lt;Segment\u0026lt;K, V\u0026gt;\u0026gt;(numSpills); 72 for(int i = 0; i \u0026lt; numSpills; i++) { 73 IndexRecord indexRecord = indexCacheList.get(i).getIndex(parts); 74 75 Segment\u0026lt;K,V\u0026gt; s = 76 new Segment\u0026lt;K,V\u0026gt;(job, rfs, filename[i], indexRecord.startOffset, 77 indexRecord.partLength, codec, true); 78 segmentList.add(i, s); 79 80 if (LOG.isDebugEnabled()) { 81 LOG.debug(\u0026#34;MapId=\u0026#34; + mapId + \u0026#34; Reducer=\u0026#34; + parts + 82 \u0026#34;Spill =\u0026#34; + i + \u0026#34;(\u0026#34; + indexRecord.startOffset + \u0026#34;,\u0026#34; + 83 indexRecord.rawLength + \u0026#34;, \u0026#34; + indexRecord.partLength + \u0026#34;)\u0026#34;); 84 } 85 } 86 87 //merge 88 @SuppressWarnings(\u0026#34;unchecked\u0026#34;) 89 RawKeyValueIterator kvIter = Merger.merge(job, rfs, 90 keyClass, valClass, codec, 91 segmentList, job.getInt(\u0026#34;io.sort.factor\u0026#34;, 100), 92 new Path(mapId.toString()), 93 job.getOutputKeyComparator(), reporter, 94 null, spilledRecordsCounter); 95 96 //write merged output to disk 97 //执行merge，并且把merge结果写到\u0026#34;/file.out\u0026#34;的最终输出中去。 98 long segmentStart = finalOut.getPos(); 99 Writer\u0026lt;K, V\u0026gt; writer = 100 new Writer\u0026lt;K, V\u0026gt;(job, finalOut, keyClass, valClass, codec, 101 spilledRecordsCounter); 102 if (combinerRunner == null || numSpills \u0026lt; minSpillsForCombine) { 103 Merger.writeFile(kvIter, writer, reporter, job); 104 } else { 105 combineCollector.setWriter(writer); 106 combinerRunner.combine(kvIter, combineCollector); 107 } 108 109 //close 110 writer.close(); 111 112 // record offsets 113 //把index写到最终的\u0026#34;/file.out.index\u0026#34;文件中。 114 rec.startOffset = segmentStart; 115 rec.rawLength = writer.getRawLength(); 116 rec.partLength = writer.getCompressedLength(); 117 spillRec.putIndex(rec, parts); 118 } 119 spillRec.writeToFile(finalIndexFile, job); 120 finalOut.close(); 121 for(int i = 0; i \u0026lt; numSpills; i++) { 122 rfs.delete(filename[i],true); 123 } 124 } 125 } ​ 合并前后index文件和spill文件的结构图\n从前面的分析指导，多个partition的都在一个输出文件中，但是按照partition排序的。即把maper输出按照partition分段了。一个partition对应一个reducer，因此一个reducer只要获取一段即可。\n完。 参考： 参考并补充了http://caibinbupt.iteye.com/blog/401374文章中关于内存中索引结构的分析。谢谢。\n","link":"https://idouba.com/hadoop_mapreduce_shuffle_map_output/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】Mapreduce shuffle过程之Map输出过程"},{"body":"","link":"https://idouba.com/tags/hadoop/","section":"tags","tags":null,"title":"hadoop"},{"body":"","link":"https://idouba.com/categories/hadoop/","section":"categories","tags":null,"title":"hadoop"},{"body":"","link":"https://idouba.com/tags/mapreduce/","section":"tags","tags":null,"title":"mapreduce"},{"body":"一、概述 在本篇博文中，试图通过代码了解hadoop job执行的整个流程。即用户提交的mapreduce的jar文件、输入提交到hadoop的集群，并在集群中运行。重点在代码的角度描述整个流程，有些细节描述的并不那么详细。 汇总的代码流程图附件:hadoop_mapreduce_jobsubmit\n二、主要流程 Jobclient通过RPC方式调用到jobtracker的submitJob方法提交作业，包括作业的jar、分片和作业描述。 JobTracker的submitJob方法吧job加入到内存队列中，由独立的线程取出每个JobInProgress的对象调用其initTasks方法，根据传入的作业分片创建对应数量的TaskInProgress类型的maptask和指定数量的Reduce task。 Tasktracker的offerService定时调用jobTracker的heartbeat发心跳给jobtracker报告状态并获取要执行的task。在haeartbeat中其实是通过配置的Taskscheduler来分配task的。 TaskTracker初始化时，会初始化并启动两个TaskLauncher类型的线程，mapLauncher，reduceLauncher。在TaskTracker从JobTracher获取到任务后，对应的会把任务添加到两个TaskLauncher的Queue中。TaskLauncher线程一直会定时检查TaskTracher上面有slot可以运行新的Task，则启动Task。 先把task运行需要的文件解压到本地，并创建根据Task类型（Map或者Reduce）创建一个TaskRunner线程，在TaskRunner中JvmManager调用JvmManagerForType、JvmRunner来启动一个java进程来执行Map或Reduce任务。在TaskRunner线程执行中，会构造一个_java –D Child address port tasked_这 样第一个java命令，单独启动一个java进程。在Child的main函数中通过TaskUmbilicalProtocol协议，从 TaskTracker获得需要执行的Task，并调用Task的run方法来执行。 对于MapTask的的run方法会通过java反射机制构造根据配置 Mapper，InputFormat，mapperContext等对象，然后调用构造的mapper的run方法执行mapper操作。 对于ReduceTask，由ReduceCopier对象的不同线程来获取map输出地址，拷贝输出，merge输出等操作。并利用反射机制根据配置的Reducer类构造一个Reducer实例和运行的上下文。并调用reducer的run方法来执行到用户定义的reduce操作。 三、详细流程 一）JobTracker等相关功能模块初始化(详细) 本来按照流程，第一步骤应该是Jobclient向Jobtracker发起作业提交的请求。为了更好的理解jobtracker是如何接收从jobclient提交的作业，有必要了解jobtracker相关的服务（和功能模块）的初始化过程。即Jobtracker作为一个服务启动起来，包括其附属的其他服务（和功能模块）。以接受jobclient的作业提交，初始化作业，向tasktracker分配任务。\nJobTracker 的main函数中调用其startTracker方法。 在main函数中调用offerService，启动各个子服务项（大部分形态都是线程，有些是其他的初始化，如taskScheduler） ?在startTracker中调用其构造函数，在构造函数中对其中重要的属性根据配置进行初始化。(个人感觉再构造中设置scheduler，在statTracker调用构造的下一句有给Scheduler传JobTracker的引用，有点不自然)。Scheduler和JobTracker实例间，Scheduler包含JobTracker（实际上就是TaskTrackerManager）对象，通过TaskTrackerManager对象获取Hadoop集群的一些信息，如slot总数，QueueManager对象，这些都是调度器中调度算法输入的指标；JobTracker中要包含Scheduler对象，使用Scheduler来为TaskTracker分配task。 在offerService()中启动taskSchedulerexpireTrackersThread retireJobsThread expireLaunchingTaskThread completedJobsStoreThread interTrackerServer等几个线程来共同完成服务。同时调用TaskScheduler的start方法进行初始化。 在FairScheduler调度器的start方法中调用EagerTaskInitializationListenerr的start方法来初始化EagerTaskInitializationListener 在FairScheduler调度器的start方法中调用DefaultTaskSelector的start方法来初始化DefaultTaskSelector，因为该类实现的TaskSelector太简单，start方法里也没有做任何事情。 二）客户端作业提交(详细) Jobclient使用内置的JobSubmissionProtocol 实例jobSubmitClient 和JobTracker交互。向jobtracker请求一个新的作业ID，计算作业的输入分片，并将运行作业所需的资源（包括作业jar文件，配置文件和计算所得的输入分片）复制到jobtracker的文件系统中一个以作业ID命名的目录下。\n通过调用JobTracker的getNewJobId()向jobtracker请求一个新的作业ID 获取job的jar、输入分片、作业描述等几个路径信息，以jobId命名。 其中getSystemDir()是返回jobtracker的系统目录，来放置job相关的文件。包括：mapreduce的jar文件submitJarFile、分片文件submitSplitFile、作业描述文件submitJobFile 检查作业的输出说明，如果没有指定输出目录或输出目录以及存在，则作业不提交。参照org.apache.hadoop.mapreduce.lib.output.FileOutputFormat的checkOutputSpecs方法。如果没有指定，则抛出InvalidJobConfException，文件已经存在则抛出FileAlreadyExistsException 计算作业的输入分片。通过InputFormat的getSplits(job)方法获得作业的split并将split序列化封装为RawSplit。返回split数目，也即代表有多个分片有多少个map。详细参见InputFormat获取Split的方法。 writeNewSplits 方法把输入分片写到JobTracker的job目录下。 将运行作业所需的资源（包括作业jar文件，配置文件和计算所得的输入分片）复制到jobtracker的文件系统中一个以作业ID命名的目录下。 使用句柄JobSubmissionProtocol通过RPC远程调用的submitJob()方法，向JobTracker提交作业。JobTracker作业放入到内存队列中，由作业调度器进行调度。并初始化作业实例。JobTracker创建job成功后会给JobClient传回一个JobStatus对象用于记录job的状态信息，如执行时间、Map和Reduce任务完成的比例等。JobClient会根据这个JobStatus对象创建一个 NetworkedJob的RunningJob对象，用于定时从JobTracker获得执行过程的统计数据来监控并打印到用户的控制台。 三）JobTracker接收作业(详细) JobTracker根据接收到的submitJob()方法调用后，把调用放入到内存队列中，由作业调度器进行调度。并初始化作业实例，从共享文件系统中获取JobClient计算好的输入分片信息，为每个分片创建一个map任务，根据mapred.reduce.task设置来创建指定数量的reduce任务。\nJobClient通过RPC的方式向JobTracker提交作业； 调用JobTracker的submitJob方法。该方法是JobTracker向外提供的供调用的提交作业的接口。 submit方法中调用JobTracker的addJob方法。 在addJob方法中会把作业加入到集合中供调度，并会触发注册的JobInProgressListener的jobAdded事件。由上篇博文的jobtracker相关服务和功能的初始化的FairScheduler的start方法中看到，这里注册的是两个JobInProgressListener。分别是FairScheduler的内部类JobListener和EagerTaskInitializationListener。 FairScheduler的内部类JobListener响应jobAdded事件事件。只是为每个加入的Job创建一个用于FairScheduler调度用的JobInfo对象，并将其和job的对应的存储在Map\u0026lt;JobInProgress, JobInfo\u0026gt; infos集合中。 EagerTaskInitializationListener响应jobAdded事件事件。jobAdded 只是简单的把job加入到一个List类型的 jobInitQueue中。并不直接对其进行初始化，对其中的job的处理由另外线程JobInitManager来做。该线程，一直检查 jobInitQueue是否有作业，有则拿出来从线程池中取一个线程InitJob处理。关于作业的初始化过程专门在下一篇文章中介绍。 四）Job初始化(详细) Jobtracker响应作业提交请求，将提交的作业加入到一个列表中，由单独的线程来对列表中的job进行初始化。至此在Jobtracker一端对提交的job的准备工作就完毕了。\nEagerTaskInitializationListener的 jobAdded方法把JobInProgress类型的job放到List类型的 jobInitQueue中，有个单独的线程会对新加入的每个job进行初始化，其初始化调用的方法就是JobInProgress的方法 initTasks。 在JobInProgress的方法initTasks方法中，会根据传入的作业分片创建对应数量的TaskInProgress类型的maptask，同时会创建TaskInProgress类型的指定数量的reducetask。 TaskInProgress的初始化是由其构造函数和构造函数中调用的init方法完成的。有构造MapTask的构造函数和构造ReduceTask的构造函数。分别是如下。其主要区别在于构造mapTask是要传入输入分片信息的RawSplit，而Reduce Task则不需要。两个构造函数都要调用init方法，进行其他的初始化。 五） TaskTracker获取Task，即jobtracker派发task（详细） tasktracker定时发心跳给jobtracker，并从jobtracker获取要执行的task。jobtracker在分配map任务会考虑数据本地化，对于reduce任务不用考虑本地化。\n​\nTaskTracker在run中调用offerService()方法一直死循环的去连接Jobtracker，先Jobtracker发送心跳，发送自身状态，并从Jobtracker获取任务指令来执行。 在JobTracker的heartbeat方法中，对于来自每一个TaskTracker的心跳请求，根据一定的作业调度策略调用assignTasks方法选择一定Task Scheduler调用对应的LoadManager的canAssignMap方法和canAssignReduce方法以决定是否可以给 tasktracker分配任务。默认的是CapBasedLoad，全局平均分配。即根据全局的任务槽数，全局的map任务数的比值得到一个load系 数，该系数乘以待分配任务的tasktracker的最大map任务数，即是该tasktracker能分配得到的任务数。如果太tracker当前运行 的任务数小于可运行的任务数，则任务可以分配新作业给他。 Scheduler的调用TaskSelector的obtainNewMapTask或者obtainNewReduceTask选择Task。 在DefaultTaskSelector中选择Task的方法其实只是封装了JobInProgress的对应方法。根据待派发Task的TaskTracker根据集群中的TaskTracker数量（clusterSize），运行TraskTracker的服务器数（numUniqueHosts），该Job中map task的平均进度（avgProgress），可以调度map的最大水平（距离其实），选择一个task执行。考虑到map的本地化，选择reducetask时，不用考虑本地化。 JobTracker根据得到的Task构造TaskTrackerAction设置到到HeartbeatResponse返回给TaskTracker。 TaskTracker中将来自JobTracker的任务加入到TaskQueue中等待执行。 六）Tasktracker启动task（详细） TaskTracker初始化时，会初始化并启动两个TaskLauncher类型的线程，mapLauncher，reduceLauncher。在TaskTracker从JobTracher获取到任务后，对应的会把任务添加到两个 TaskLauncher的Queue中，其实是TaskLauncher维护的一个列表List tasksToLaunch。 TaskLauncher线程一直会定时检查TaskTracher上面有slot开业运行新的Task，则启动 Task。在这个过程中，先把task运行需要的文件解压到本地，并创建根据Task类型（Map或者Reduce）创建一个TaskRunner线程， 在TaskRunner中JvmManager调用JvmManagerForType、JvmRunner来启动一个java进程来执行Map或Reduce任务。\ntasktracker启动task\ntasktracker的offerService方法获取到要执行的task后调用addToTaskQueue方法，其实是调用taskrunner的addToTaskQueue方法 TaskLauncher内部维护了一个List tasksToLaunch，只是把task加入到该集合中 taskLauncher是一个线程，在其run方法中从tasksToLaunch集合中取出task来执行，调用Tasktracker的startNewTask方法启动task。 startNewtask方法中调用localizeJob方法把job相关的配置信息和要运行的jar拷贝到tasktracker本地，然后调用taskInProgress的launchTask方法来启动task。 TaskInProgress的launchTask方法先调用localizeTask(task把task相关的配置信息获取到本地。然后创建一个TaskRunner线程来启动task。 在TaskRunner的run方法中构建一个java命令的执行的条件，包括引用类，执行目录等，入口类是Child。然后调用JvmManager 的launchJvm方法来调用。 JvmManager 进而调用 JvmManagerForType的reapJvm，和spawnNewJvm 方法，发起调用。 在JvmManagerForType的spawnNewJvm 方法中创建了一个JvmRunner线程类执行调用。 JvmRunner线程的run调用runChild方法来执行 一个命令行的调用。 七）Tasktracker运行map任务（详细） TaskRunner线程执行中，会构造一个_java –D** Child address port tasked_这 样第一个java命令，单独启动一个java进程。在Child的main函数中通过TaskUmbilicalProtocol协议，从 TaskTracker获得需要执行的Task，并调用Task的run方法来执行，而Task的run方法会通过java反射机制构造 Mapper，InputFormat，mapperContext，然后调用构造的mapper的run方法执行mapper操作。\n​\nChild类根据前面输入的三个参数，即tasktracher的地址、端口、taskid。通过TaskUmbilicalProtocol协议，从TaskTracker获得需要执行的Task，在Child的main函数中调用执行。 在Chilld中，执行Task的run方法。Task 的run方法。是真正执行用户定义的map或者reduce任务的入口，通过TaskUmbilicalProtocol向tasktracker上报执行进度。 在MapTask的run中执行runMapper方法来调用mapper定义的方法。 在runNewMapper方法中构造mapper实例和mapper执行的配置信息。并执行mapper.run方法来调用到用户定义的mapper的方法。 mapper的run方法中，从输入数据中逐一取出调用map方法来处理每一条数据 mapper的map方法是真正用户定义的处理数据的类。也是用户唯一需要定义的方法。 八）TaskTracker运行reduce任务（详细） TaskRunner线程执行中，会构造一个_java –D** Child address port tasked_这样第一个java命令，单独启动一个java进程。在Child的main函数中通过TaskUmbilicalProtocol协议，从TaskTracker获得需要执行的Task，并调用Task的run方法来执行。在ReduceTask而Task的run方法会通过java反射机制构造Reducer，Reducer.Context，然后调用构造的Reducer的run方法执行reduce操作。不同于map任务，在执行reduce任务前，需要把map的输出从map运行的tasktracker上拷贝到reducer运行的tasktracker上。 Reduce需要集群上若干个map任务的输出作为其特殊的分区文件。每个map任务完成的时间可能不同，因此只要有一个任务完成，reduce任务就开始复制其输出。这就是reduce任务的复制阶段。其实是启动若干个MapOutputCopier线程来复制完所有map输出。在复制完成后reduce任务进入排序阶段。这个阶段将由LocalFSMerger或InMemFSMergeThread合并map输出，维持其顺序排序。【即对有序的几个文件进行归并，采用归并排序】在reduce阶段，对已排序输出的每个键都要调用reduce函数，此阶段的输出直接写到文件系统，一般为HDFS上。（如果采用HDFS，由于tasktracker节点也是DataNoe，所以第一个块副本将被写到本地磁盘。 即数据本地化） Map 任务完成后，会通知其父tasktracker状态更新，然后tasktracker通知jobtracker。通过心跳机制来完成。因此jobtracker知道map输出和tasktracker之间的映射关系。Reducer的一个getMapCompletionEvents线程定期询问jobtracker以便获取map输出位置。\n​ Child启动reduce任务\n在ReduceTak中 构建ReduceCopier对象，调用其fetchOutputs方法。 2.在ReduceCopier的fetchOutputs方法中分别构造几个独立的线程。相互配合，并分别独立的完成任务。\n2.1 GetMapEventsThread线程通过RPC询问TaskTracker，对每个完成的Event，获取maptask所在的服务器地址，即MapTask输出的地址，构造URL，加入到mapLocations，供copier线程获取。\n2.2 构造并启动若干个MapOutputCopier线程，通过http协议，把map的输出从远端服务器拷贝的本地，如果可以放在内存中，则存储在内存中调用，否则保存在本地文件。\n2.3 LocalFSMerger对磁盘上的map 输出进行归并。\n2.4 MemFSMergeThread对内存中的map输出进行归并。 3. 根据拷贝到的map输出构造一个raw keyvalue的迭代器，作为reduce的输入。\n调用runNewReducer方法中根据配置的Reducer类构造一个Reducer实例和运行的上下文。并调用reducer的run方法来执行到用户定义的reduce操作。\n在Reducer的run方法中从上下文中取出一个key和该key对应的Value集合（Iterable类型），调用reducer的reduce方法进行处理。\nRecuer的reduce方法是用户定义的处理数据的方法，也是用户唯一需要定义的方法。\n汇总的代码流程图附件：hadoop_mapreduce_jobsubmit\n完。\n","link":"https://idouba.com/hadoop_job_submit_conclusion/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】hadoop作业提交之汇总"},{"body":"一、概要描述\n在上篇博文描述了TaskTracker启动一个独立的java进程来执行Map任务。接上上篇文章，TaskRunner线程执行中，会构造一个java –D** Child address port tasked这样第一个java命令，单独启动一个java进程。在Child的main函数中通过TaskUmbilicalProtocol协议，从TaskTracker获得需要执行的Task，并调用Task的run方法来执行。在ReduceTask而Task的run方法会通过java反射机制构造Reducer，Reducer.Context，然后调用构造的Reducer的run方法执行reduce操作。不同于map任务，在执行reduce任务前，需要把map的输出从map运行的tasktracker上拷贝到reducer运行的tasktracker上。\nReduce需要集群上若干个map任务的输出作为其特殊的分区文件。每个map任务完成的时间可能不同，因此只要有一个任务完成，reduce任务就开始复制其输出。这就是reduce任务的**复制阶段。**其实是启动若干个MapOutputCopier线程来复制完所有map输出。在复制完成后reduce任务进入排序阶段。这个阶段将由LocalFSMerger或InMemFSMergeThread合并map输出，维持其顺序排序。【即对有序的几个文件进行归并，采用归并排序】在reduce阶段，对已排序输出的每个键都要调用reduce函数，此阶段的输出直接写到文件系统，一般为HDFS上。（如果采用HDFS，由于tasktracker节点也是DataNoe，所以第一个块副本将被写到本地磁盘。 即数据本地化）\nMap 任务完成后，会通知其父tasktracker状态更新，然后tasktracker通知jobtracker。通过心跳机制来完成。因此jobtracker知道map输出和tasktracker之间的映射关系。Reducer的一个getMapCompletionEvents线程定期询问jobtracker以便获取map输出位置。\n二、 流程描述\n在ReduceTak中 构建ReduceCopier对象，调用其fetchOutputs方法。\n在ReduceCopier的fetchOutputs方法中分别构造几个独立的线程。相互配合，并分别独立的完成任务。\n2.1 GetMapEventsThread线程通过RPC询问TaskTracker，对每个完成的Event，获取maptask所在的服务器地址，即MapTask输出的地址，构造URL，加入到mapLocations，供copier线程获取。\n2.2构造并启动若干个MapOutputCopier线程，通过http协议，把map的输出从远端服务器拷贝的本地，如果可以放在内存中，则存储在内存中调用，否则保存在本地文件。\n2.3LocalFSMerger对磁盘上的map 输出进行归并。\n2.4nMemFSMergeThread对内存中的map输出进行归并。\n3.根据拷贝到的map输出构造一个raw keyvalue的迭代器，作为reduce的输入。\n调用runNewReducer方法中根据配置的Reducer类构造一个Reducer实例和运行的上下文。并调用reducer的run方法来执行到用户定义的reduce操作。。\n在Reducer的run方法中从上下文中取出一个key和该key对应的Value集合（Iterable类型），调用reducer的reduce方法进行处理。\nRecuer的reduce方法是用户定义的处理数据的方法，也是用户唯一需要定义的方法。\n三、代码详细\n1. Child的main方法每个task进程都会被在单独的进程中执行，这个方法就是这些进程的入口方法。Reduce和map一样都是由该main函数调用。所以此处不做描述，详细见上节Child启动map任务。\n**2. ReduceTask的run方法。**在Child子进程中被调用，执行用户定义的Reduce操作。前面代码逻辑和MapTask类似。通过TaskUmbilicalProtocol向tasktracker上报执行进度。开启线程向TaskTracker上报进度，根据task的不同动作要求执行不同的方法，如jobClean，jobsetup，taskCleanup。对于部分的了解可以产看taskTracker获取Task文章中的 JobTracker的 heartbeat方法处的详细解释。不同于map任务，在执行reduce任务前，需要把map的输出从map运行的tasktracker上拷贝到reducer运行的tasktracker上。\n1@SuppressWarnings(\u0026#34;unchecked\u0026#34;) 2 public void run(JobConf job, final TaskUmbilicalProtocol umbilical) 3 throws IOException, InterruptedException, ClassNotFoundException { 4 job.setBoolean(\u0026#34;mapred.skip.on\u0026#34;, isSkipping()); 5 6 if (isMapOrReduce()) { 7 copyPhase = getProgress().addPhase(\u0026#34;copy\u0026#34;); 8 sortPhase = getProgress().addPhase(\u0026#34;sort\u0026#34;); 9 reducePhase = getProgress().addPhase(\u0026#34;reduce\u0026#34;); 10 } 11 // start thread that will handle communication with parent 12 TaskReporter reporter = new TaskReporter(getProgress(), umbilical); 13 reporter.startCommunicationThread(); 14 boolean useNewApi = job.getUseNewReducer(); 15 initialize(job, getJobID(), reporter, useNewApi); 16 17 // check if it is a cleanupJobTask 18 if (jobCleanup) { 19 runJobCleanupTask(umbilical, reporter); 20 return; 21 } 22 if (jobSetup) { 23 runJobSetupTask(umbilical, reporter); 24 return; 25 } 26 if (taskCleanup) { 27 runTaskCleanupTask(umbilical, reporter); 28 return; 29 } 30 31 // Initialize the codec 32 codec = initCodec(); 33 34 boolean isLocal = \u0026#34;local\u0026#34;.equals(job.get(\u0026#34;mapred.job.tracker\u0026#34;, \u0026#34;local\u0026#34;)); 35 36 //如果不是一个本地执行额模式（就是配置中不是分布式的），则要启动一个ReduceCopier来拷贝Map的输出，即Reduce的输入。 37 if (!isLocal) { 38 reduceCopier = new ReduceCopier(umbilical, job, reporter); 39 if (!reduceCopier.fetchOutputs()) { 40 if(reduceCopier.mergeThrowable instanceof FSError) { 41 LOG.error(\u0026#34;Task: \u0026#34; + getTaskID() + \u0026#34; - FSError: \u0026#34; + 42 StringUtils.stringifyException(reduceCopier.mergeThrowable)); 43 umbilical.fsError(getTaskID(), 44 reduceCopier.mergeThrowable.getMessage()); 45 } 46 throw new IOException(\u0026#34;Task: \u0026#34; + getTaskID() + 47 \u0026#34; - The reduce copier failed\u0026#34;, reduceCopier.mergeThrowable); 48 } 49 } 50 copyPhase.complete(); 51 //拷贝完成后，进入sort阶段。 52 setPhase(TaskStatus.Phase.SORT); 53 statusUpdate(umbilical); 54 55 final FileSystem rfs = FileSystem.getLocal(job).getRaw(); 56 RawKeyValueIterator rIter = isLocal 57 ? Merger.merge(job, rfs, job.getMapOutputKeyClass(), 58 job.getMapOutputValueClass(), codec, getMapFiles(rfs, true), 59 !conf.getKeepFailedTaskFiles(), job.getInt(\u0026#34;io.sort.factor\u0026#34;, 100), 60 new Path(getTaskID().toString()), job.getOutputKeyComparator(), 61 reporter, spilledRecordsCounter, null) 62 : reduceCopier.createKVIterator(job, rfs, reporter); 63 64 // free up the data structures 65 mapOutputFilesOnDisk.clear(); 66 67 sortPhase.complete(); // sort is complete 68 setPhase(TaskStatus.Phase.REDUCE); 69 statusUpdate(umbilical); 70 Class keyClass = job.getMapOutputKeyClass(); 71 Class valueClass = job.getMapOutputValueClass(); 72 RawComparator comparator = job.getOutputValueGroupingComparator(); 73 74 if (useNewApi) { 75 runNewReducer(job, umbilical, reporter, rIter, comparator, 76 keyClass, valueClass); 77 } else { 78 runOldReducer(job, umbilical, reporter, rIter, comparator, 79 keyClass, valueClass); 80 } 81 done(umbilical, reporter); 82 } ReduceCopier类的fetchOutputs方法。该方法负责将map的输出拷贝的reduce端进程处理。从代码上看，启动了一个LocalFSMerger、InMemFSMergeThread、 GetMapEventsThread 和若干个MapOutputCopier线程。几个独立的线程。相互配合，并分别独立的完成任务。 1public boolean fetchOutputs() throws IOException { 2 int totalFailures = 0; 3 int numInFlight = 0, numCopied = 0; 4 DecimalFormat mbpsFormat = new DecimalFormat(\u0026#34;0.00\u0026#34;); 5 final Progress copyPhase = 6 reduceTask.getProgress().phase(); 7 LocalFSMerger localFSMergerThread = null; 8 InMemFSMergeThread inMemFSMergeThread = null; 9 GetMapEventsThread getMapEventsThread = null; 10 11 12 for (int i = 0; i \u0026lt; numMaps; i++) { 13 copyPhase.addPhase(); // add sub-phase per file 14 } 15 16 //1)根据配置的numCopiers数量构造若干个MapOutputCopier拷贝线程，默认是5个，正是这些MapOutputCopier来实施的拷贝任务。 17 copiers = new ArrayList\u0026lt;MapOutputCopier\u0026gt;(numCopiers); 18 19 // start all the copying threads 20 for (int i=0; i \u0026lt; numCopiers; i++) { 21 MapOutputCopier copier = new MapOutputCopier(conf, reporter); 22 copiers.add(copier); 23 24 copier.start(); 25 } 26 27 //start the on-disk-merge thread 2)启动磁盘merge线程（参照后面方法） 28 localFSMergerThread = new LocalFSMerger((LocalFileSystem)localFileSys); 29 //start the in memory merger thread 3)启动内存merge线程（参照后面方法） 30 inMemFSMergeThread = new InMemFSMergeThread(); 31 localFSMergerThread.start(); 32 inMemFSMergeThread.start(); 33 34 // start the map events thread 4)启动merge事件获取线程 35 getMapEventsThread = new GetMapEventsThread(); 36 getMapEventsThread.start(); 37 38 // start the clock for bandwidth measurement 39 long startTime = System.currentTimeMillis(); 40 long currentTime = startTime; 41 long lastProgressTime = startTime; 42 long lastOutputTime = 0; 43 44 // loop until we get all required outputs 45 //5)当获取到的copiedMapOutputs数量小于map数时，说明还没有拷贝完成，则一直执行。在执行中会根据时间进度一直打印输出，表示已经拷贝了多少个map的输出，还有多万未完成。 46 while (copiedMapOutputs.size() \u0026lt; numMaps \u0026amp;\u0026amp; mergeThrowable == null) { 47 48 currentTime = System.currentTimeMillis(); 49 boolean logNow = false; 50 if (currentTime - lastOutputTime \u0026gt; MIN_LOG_TIME) { 51 lastOutputTime = currentTime; 52 logNow = true; 53 } 54 if (logNow) { 55 LOG.info(reduceTask.getTaskID() + \u0026#34; Need another \u0026#34; 56 + (numMaps - copiedMapOutputs.size()) + \u0026#34; map output(s) \u0026#34; 57 + \u0026#34;where \u0026#34; + numInFlight + \u0026#34; is already in progress\u0026#34;); 58 } 59 60 // Put the hash entries for the failed fetches. 61 Iterator\u0026lt;MapOutputLocation\u0026gt; locItr = retryFetches.iterator(); 62 63 while (locItr.hasNext()) { 64 MapOutputLocation loc = locItr.next(); 65 List\u0026lt;MapOutputLocation\u0026gt; locList = 66 mapLocations.get(loc.getHost()); 67 68 // Check if the list exists. Map output location mapping is cleared 69 // once the jobtracker restarts and is rebuilt from scratch. 70 // Note that map-output-location mapping will be recreated and hence 71 // we continue with the hope that we might find some locations 72 // from the rebuild map. 73 if (locList != null) { 74 // Add to the beginning of the list so that this map is 75 //tried again before the others and we can hasten the 76 //re-execution of this map should there be a problem 77 locList.add(0, loc); 78 } 79 } 80 81 if (retryFetches.size() \u0026gt; 0) { 82 LOG.info(reduceTask.getTaskID() + \u0026#34;: \u0026#34; + 83 \u0026#34;Got \u0026#34; + retryFetches.size() + 84 \u0026#34; map-outputs from previous failures\u0026#34;); 85 } 86 // clear the \u0026#34;failed\u0026#34; fetches hashmap 87 retryFetches.clear(); 88 89 // now walk through the cache and schedule what we can 90 int numScheduled = 0; 91 int numDups = 0; 92 93 synchronized (scheduledCopies) { 94 95 // Randomize the map output locations to prevent 96 // all reduce-tasks swamping the same tasktracker 97 List\u0026lt;String\u0026gt; hostList = new ArrayList\u0026lt;String\u0026gt;(); 98 hostList.addAll(mapLocations.keySet()); 99 100 Collections.shuffle(hostList, this.random); 101 102 Iterator\u0026lt;String\u0026gt; hostsItr = hostList.iterator(); 103 104 while (hostsItr.hasNext()) { 105 106 String host = hostsItr.next(); 107 108 List\u0026lt;MapOutputLocation\u0026gt; knownOutputsByLoc = 109 mapLocations.get(host); 110 111 // Check if the list exists. Map output location mapping is 112 // cleared once the jobtracker restarts and is rebuilt from 113 // scratch. 114 // Note that map-output-location mapping will be recreated and 115 // hence we continue with the hope that we might find some 116 // locations from the rebuild map and add then for fetching. 117 if (knownOutputsByLoc == null || knownOutputsByLoc.size() == 0) { 118 continue; 119 } 120 121 //Identify duplicate hosts here 122 if (uniqueHosts.contains(host)) { 123 numDups += knownOutputsByLoc.size(); 124 continue; 125 } 126 127 Long penaltyEnd = penaltyBox.get(host); 128 boolean penalized = false; 129 130 if (penaltyEnd != null) { 131 if (currentTime \u0026lt; penaltyEnd.longValue()) { 132 penalized = true; 133 } else { 134 penaltyBox.remove(host); 135 } 136 } 137 138 if (penalized) 139 continue; 140 141 synchronized (knownOutputsByLoc) { 142 143 locItr = knownOutputsByLoc.iterator(); 144 145 while (locItr.hasNext()) { 146 147 MapOutputLocation loc = locItr.next(); 148 149 // Do not schedule fetches from OBSOLETE maps 150 if (obsoleteMapIds.contains(loc.getTaskAttemptId())) { 151 locItr.remove(); 152 continue; 153 } 154 155 uniqueHosts.add(host); 156 scheduledCopies.add(loc); 157 locItr.remove(); // remove from knownOutputs 158 numInFlight++; numScheduled++; 159 160 break; //we have a map from this host 161 } 162 } 163 } 164 scheduledCopies.notifyAll(); 165 } 166 167 if (numScheduled \u0026gt; 0 || logNow) { 168 LOG.info(reduceTask.getTaskID() + \u0026#34; Scheduled \u0026#34; + numScheduled + 169 \u0026#34; outputs (\u0026#34; + penaltyBox.size() + 170 \u0026#34; slow hosts and\u0026#34; + numDups + \u0026#34; dup hosts)\u0026#34;); 171 } 172 173 if (penaltyBox.size() \u0026gt; 0 \u0026amp;\u0026amp; logNow) { 174 LOG.info(\u0026#34;Penalized(slow) Hosts: \u0026#34;); 175 for (String host : penaltyBox.keySet()) { 176 LOG.info(host + \u0026#34; Will be considered after: \u0026#34; + 177 ((penaltyBox.get(host) - currentTime)/1000) + \u0026#34; seconds.\u0026#34;); 178 } 179 } 180 181 // if we have no copies in flight and we can\u0026#39;t schedule anything 182 // new, just wait for a bit 183 try { 184 if (numInFlight == 0 \u0026amp;\u0026amp; numScheduled == 0) { 185 // we should indicate progress as we don\u0026#39;t want TT to think 186 // we\u0026#39;re stuck and kill us 187 reporter.progress(); 188 Thread.sleep(5000); 189 } 190 } catch (InterruptedException e) { } // IGNORE 191 192 while (numInFlight \u0026gt; 0 \u0026amp;\u0026amp; mergeThrowable == null) { 193 LOG.debug(reduceTask.getTaskID() + \u0026#34; numInFlight = \u0026#34; + 194 numInFlight); 195 //the call to getCopyResult will either 196 //1) return immediately with a null or a valid CopyResult object, 197 // or 198 //2) if the numInFlight is above maxInFlight, return with a 199 // CopyResult object after getting a notification from a 200 // fetcher thread, 201 //So, when getCopyResult returns null, we can be sure that 202 //we aren\u0026#39;t busy enough and we should go and get more mapcompletion 203 //events from the tasktracker 204 CopyResult cr = getCopyResult(numInFlight); 205 206 if (cr == null) { 207 break; 208 } 209 210 if (cr.getSuccess()) { // a successful copy 211 numCopied++; 212 lastProgressTime = System.currentTimeMillis(); 213 reduceShuffleBytes.increment(cr.getSize()); 214 215 long secsSinceStart = 216 (System.currentTimeMillis()-startTime)/1000+1; 217 float mbs = ((float)reduceShuffleBytes.getCounter())/(1024*1024); 218 float transferRate = mbs/secsSinceStart; 219 220 copyPhase.startNextPhase(); 221 copyPhase.setStatus(\u0026#34;copy (\u0026#34; + numCopied + \u0026#34; of \u0026#34; + numMaps 222 + \u0026#34; at \u0026#34; + 223 mbpsFormat.format(transferRate) + \u0026#34; MB/s)\u0026#34;); 224 225 // Note successful fetch for this mapId to invalidate 226 // (possibly) old fetch-failures 227 fetchFailedMaps.remove(cr.getLocation().getTaskId()); 228 } else if (cr.isObsolete()) { 229 //ignore 230 LOG.info(reduceTask.getTaskID() + 231 \u0026#34; Ignoring obsolete copy result for Map Task: \u0026#34; + 232 cr.getLocation().getTaskAttemptId() + \u0026#34; from host: \u0026#34; + 233 cr.getHost()); 234 } else { 235 retryFetches.add(cr.getLocation()); 236 237 // note the failed-fetch 238 TaskAttemptID mapTaskId = cr.getLocation().getTaskAttemptId(); 239 TaskID mapId = cr.getLocation().getTaskId(); 240 241 totalFailures++; 242 Integer noFailedFetches = 243 mapTaskToFailedFetchesMap.get(mapTaskId); 244 noFailedFetches = 245 (noFailedFetches == null) ? 1 : (noFailedFetches + 1); 246 mapTaskToFailedFetchesMap.put(mapTaskId, noFailedFetches); 247 LOG.info(\u0026#34;Task \u0026#34; + getTaskID() + \u0026#34;: Failed fetch #\u0026#34; + 248 noFailedFetches + \u0026#34; from \u0026#34; + mapTaskId); 249 250 // did the fetch fail too many times? 251 // using a hybrid technique for notifying the jobtracker. 252 // a. the first notification is sent after max-retries 253 // b. subsequent notifications are sent after 2 retries. 254 if ((noFailedFetches \u0026gt;= maxFetchRetriesPerMap) 255 \u0026amp;\u0026amp; ((noFailedFetches - maxFetchRetriesPerMap) % 2) == 0) { 256 synchronized (ReduceTask.this) { 257 taskStatus.addFetchFailedMap(mapTaskId); 258 LOG.info(\u0026#34;Failed to fetch map-output from \u0026#34; + mapTaskId + 259 \u0026#34; even after MAX_FETCH_RETRIES_PER_MAP retries... \u0026#34; 260 + \u0026#34; reporting to the JobTracker\u0026#34;); 261 } 262 } 263 // note unique failed-fetch maps 264 if (noFailedFetches == maxFetchRetriesPerMap) { 265 fetchFailedMaps.add(mapId); 266 267 // did we have too many unique failed-fetch maps? 268 // and did we fail on too many fetch attempts? 269 // and did we progress enough 270 // or did we wait for too long without any progress? 271 272 // check if the reducer is healthy 273 boolean reducerHealthy = 274 (((float)totalFailures / (totalFailures + numCopied)) 275 \u0026lt; MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT); 276 277 // check if the reducer has progressed enough 278 boolean reducerProgressedEnough = 279 (((float)numCopied / numMaps) 280 \u0026gt;= MIN_REQUIRED_PROGRESS_PERCENT); 281 282 // check if the reducer is stalled for a long time 283 // duration for which the reducer is stalled 284 int stallDuration = 285 (int)(System.currentTimeMillis() - lastProgressTime); 286 // duration for which the reducer ran with progress 287 int shuffleProgressDuration = 288 (int)(lastProgressTime - startTime); 289 // min time the reducer should run without getting killed 290 int minShuffleRunDuration = 291 (shuffleProgressDuration \u0026gt; maxMapRuntime) 292 ? shuffleProgressDuration 293 : maxMapRuntime; 294 boolean reducerStalled = 295 (((float)stallDuration / minShuffleRunDuration) 296 \u0026gt;= MAX_ALLOWED_STALL_TIME_PERCENT); 297 298 // kill if not healthy and has insufficient progress 299 if ((fetchFailedMaps.size() \u0026gt;= maxFailedUniqueFetches || 300 fetchFailedMaps.size() == (numMaps - copiedMapOutputs.size())) 301 \u0026amp;\u0026amp; !reducerHealthy 302 \u0026amp;\u0026amp; (!reducerProgressedEnough || reducerStalled)) { 303 LOG.fatal(\u0026#34;Shuffle failed with too many fetch failures \u0026#34; + 304 \u0026#34;and insufficient progress!\u0026#34; + 305 \u0026#34;Killing task \u0026#34; + getTaskID() + \u0026#34;.\u0026#34;); 306 umbilical.shuffleError(getTaskID(), 307 \u0026#34;Exceeded MAX_FAILED_UNIQUE_FETCHES;\u0026#34; 308 + \u0026#34; bailing-out.\u0026#34;); 309 } 310 } 311 312 // back off exponentially until num_retries \u0026lt;= max_retries 313 // back off by max_backoff/2 on subsequent failed attempts 314 currentTime = System.currentTimeMillis(); 315 int currentBackOff = noFailedFetches \u0026lt;= maxFetchRetriesPerMap 316 ? BACKOFF_INIT 317 * (1 \u0026lt;\u0026lt; (noFailedFetches - 1)) 318 : (this.maxBackoff * 1000 / 2); 319 penaltyBox.put(cr.getHost(), currentTime + currentBackOff); 320 LOG.warn(reduceTask.getTaskID() + \u0026#34; adding host \u0026#34; + 321 cr.getHost() + \u0026#34; to penalty box, next contact in \u0026#34; + 322 (currentBackOff/1000) + \u0026#34; seconds\u0026#34;); 323 } 324 uniqueHosts.remove(cr.getHost()); 325 numInFlight--; 326 } 327 } 328 329 // all done, inform the copiers to exit 330 exitGetMapEvents= true; 331 try { 332 getMapEventsThread.join(); 333 LOG.info(\u0026#34;getMapsEventsThread joined.\u0026#34;); 334 } catch (Throwable t) { 335 LOG.info(\u0026#34;getMapsEventsThread threw an exception: \u0026#34; + 336 StringUtils.stringifyException(t)); 337 } 338 339 synchronized (copiers) { 340 synchronized (scheduledCopies) { 341 for (MapOutputCopier copier : copiers) { 342 copier.interrupt(); 343 } 344 copiers.clear(); 345 } 346 } 347 348 // copiers are done, exit and notify the waiting merge threads 349 synchronized (mapOutputFilesOnDisk) { 350 exitLocalFSMerge = true; 351 mapOutputFilesOnDisk.notify(); 352 } 353 354 ramManager.close(); 355 356 //Do a merge of in-memory files (if there are any) 357 if (mergeThrowable == null) { 358 try { 359 // Wait for the on-disk merge to complete 360 localFSMergerThread.join(); 361 LOG.info(\u0026#34;Interleaved on-disk merge complete: \u0026#34; + 362 mapOutputFilesOnDisk.size() + \u0026#34; files left.\u0026#34;); 363 364 //wait for an ongoing merge (if it is in flight) to complete 365 inMemFSMergeThread.join(); 366 LOG.info(\u0026#34;In-memory merge complete: \u0026#34; + 367 mapOutputsFilesInMemory.size() + \u0026#34; files left.\u0026#34;); 368 } catch (Throwable t) { 369 LOG.warn(reduceTask.getTaskID() + 370 \u0026#34; Final merge of the inmemory files threw an exception: \u0026#34; + 371 StringUtils.stringifyException(t)); 372 // check if the last merge generated an error 373 if (mergeThrowable != null) { 374 mergeThrowable = t; 375 } 376 return false; 377 } 378 } 379 return mergeThrowable == null \u0026amp;\u0026amp; copiedMapOutputs.size() == numMaps; 380 } MapOutputCopier线程的run方法。从scheduledCopies(List)中取出对象来调用copyOutput方法执行拷贝。通过http协议，把map的输出从远端服务器拷贝的本地，如果可以放在内存中，则存储在内存中调用，否则保存在本地文件。 1public void run() { 2 while (true) { 3 MapOutputLocation loc = null; 4 long size = -1; 5 synchronized (scheduledCopies) { 6 while (scheduledCopies.isEmpty()) { 7 scheduledCopies.wait(); 8 } 9 loc = scheduledCopies.remove(0); 10 } 11 12 start(loc); 13 size = copyOutput(loc); 14 15 16 if (decompressor != null) { 17 CodecPool.returnDecompressor(decompressor); 18 } 19 20 } 5.MapOutputCopier线程的copyOutput方法。map的输出从远端map所在的tasktracker拷贝到reducer任务所在的tasktracker。\n1private long copyOutput(MapOutputLocation loc 2 ) throws IOException, InterruptedException { 3 // 从拷贝的记录中检查是否已经拷贝完成。 4 if (copiedMapOutputs.contains(loc.getTaskId()) || 5 obsoleteMapIds.contains(loc.getTaskAttemptId())) { 6 return CopyResult.OBSOLETE; 7 } 8 TaskAttemptID reduceId = reduceTask.getTaskID(); 9 Path filename = new Path(\u0026#34;/\u0026#34; + TaskTracker.getIntermediateOutputDir( 10 reduceId.getJobID().toString(), 11 reduceId.toString()) 12 + \u0026#34;/map_\u0026#34; + 13 loc.getTaskId().getId() + \u0026#34;.out\u0026#34;); 14 15 //一个拷贝map输出的临时文件。 16 Path tmpMapOutput = new Path(filename+\u0026#34;-\u0026#34;+id); 17 18 //拷贝map输出。 19 MapOutput mapOutput = getMapOutput(loc, tmpMapOutput); 20 if (mapOutput == null) { 21 throw new IOException(\u0026#34;Failed to fetch map-output for \u0026#34; + 22 loc.getTaskAttemptId() + \u0026#34; from \u0026#34; + 23 loc.getHost()); 24 } 25 // The size of the map-output 26 long bytes = mapOutput.compressedSize; 27 28 synchronized (ReduceTask.this) { 29 if (copiedMapOutputs.contains(loc.getTaskId())) { 30 mapOutput.discard(); 31 return CopyResult.OBSOLETE; 32 } 33 // Note that we successfully copied the map-output 34 noteCopiedMapOutput(loc.getTaskId()); 35 return bytes; 36 } 37 38 // 处理map的输出，如果是存储在内存中则添加到（Collections.synchronizedList(new LinkedList\u0026lt;MapOutput\u0026gt;)类型的结合mapOutputsFilesInMemory中，否则如果存储在临时文件中，则冲明明临时文件为正式的输出文件。 39 if (mapOutput.inMemory) { 40 // Save it in the synchronized list of map-outputs 41 mapOutputsFilesInMemory.add(mapOutput); 42 } else { 43 44 tmpMapOutput = mapOutput.file; 45 filename = new Path(tmpMapOutput.getParent(), filename.getName()); 46 if (!localFileSys.rename(tmpMapOutput, filename)) { 47 localFileSys.delete(tmpMapOutput, true); 48 bytes = -1; 49 throw new IOException(\u0026#34;Failed to rename map output \u0026#34; + 50 tmpMapOutput + \u0026#34; to \u0026#34; + filename); 51 } 52 53 synchronized (mapOutputFilesOnDisk) { 54 addToMapOutputFilesOnDisk(localFileSys.getFileStatus(filename)); 55 } 56 } 57 58 // Note that we successfully copied the map-output 59 noteCopiedMapOutput(loc.getTaskId()); 60 } 61 62 return bytes; 63} 6.ReduceTask.ReduceCopier.MapOutputCopier的shuffleInMemory方法。根据上一方法当map的输出可以在内存中存储时会调用该方法。\n1private MapOutput shuffleInMemory(MapOutputLocation mapOutputLoc, 2 URLConnection connection, 3 InputStream input, 4 int mapOutputLength, 5 int compressedLength) 6 throws IOException, InterruptedException { 7 8 //checksum 输入流，读Mpareduce中间文件IFile. 9 IFileInputStream checksumIn = 10 new IFileInputStream(input,compressedLength); 11 12 input = checksumIn; 13 14 // 如果加密，则根据codec来构建一个解密的输入流。 15 if (codec != null) { 16 decompressor.reset(); 17 input = codec.createInputStream(input, decompressor); 18 } 19 20 //把map的输出拷贝到内存的buffer中。 21 byte[] shuffleData = new byte[mapOutputLength]; 22 MapOutput mapOutput = 23 new MapOutput(mapOutputLoc.getTaskId(), 24 mapOutputLoc.getTaskAttemptId(), shuffleData, compressedLength); 25 26 int bytesRead = 0; 27 try { 28 int n = input.read(shuffleData, 0, shuffleData.length); 29 while (n \u0026gt; 0) { 30 bytesRead += n; 31 shuffleClientMetrics.inputBytes(n); 32 33 // indicate we\u0026#39;re making progress 34 reporter.progress(); 35 n = input.read(shuffleData, bytesRead, 36 (shuffleData.length-bytesRead)); 37 } 38 39 LOG.info(\u0026#34;Read \u0026#34; + bytesRead + \u0026#34; bytes from map-output for \u0026#34; + 40 mapOutputLoc.getTaskAttemptId()); 41 42 input.close(); 43 } catch (IOException ioe) { 44 LOG.info(\u0026#34;Failed to shuffle from \u0026#34; + mapOutputLoc.getTaskAttemptId(), 45 ioe); 46 47 // Inform the ram-manager 48 ramManager.closeInMemoryFile(mapOutputLength); 49 ramManager.unreserve(mapOutputLength); 50 51 // Discard the map-output 52 try { 53 mapOutput.discard(); 54 } catch (IOException ignored) { 55 LOG.info(\u0026#34;Failed to discard map-output from \u0026#34; + 56 mapOutputLoc.getTaskAttemptId(), ignored); 57 } 58 mapOutput = null; 59 60 // Close the streams 61 IOUtils.cleanup(LOG, input); 62 63 // Re-throw 64 throw ioe; 65 } 66 67 // Close the in-memory file 68 ramManager.closeInMemoryFile(mapOutputLength); 69 70 // Sanity check 71 if (bytesRead != mapOutputLength) { 72 // Inform the ram-manager 73 ramManager.unreserve(mapOutputLength); 74 75 // Discard the map-output 76 try { 77 mapOutput.discard(); 78 } catch (IOException ignored) { 79 // IGNORED because we are cleaning up 80 LOG.info(\u0026#34;Failed to discard map-output from \u0026#34; + 81 mapOutputLoc.getTaskAttemptId(), ignored); 82 } 83 mapOutput = null; 84 85 throw new IOException(\u0026#34;Incomplete map output received for \u0026#34; + 86 mapOutputLoc.getTaskAttemptId() + \u0026#34; from \u0026#34; + 87 mapOutputLoc.getOutputLocation() + \u0026#34; (\u0026#34; + 88 bytesRead + \u0026#34; instead of \u0026#34; + 89 mapOutputLength + \u0026#34;)\u0026#34; 90 ); 91 } 92 93 // TODO: Remove this after a \u0026#39;fix\u0026#39; for HADOOP-3647 94 if (mapOutputLength \u0026gt; 0) { 95 DataInputBuffer dib = new DataInputBuffer(); 96 dib.reset(shuffleData, 0, shuffleData.length); 97 LOG.info(\u0026#34;Rec #1 from \u0026#34; + mapOutputLoc.getTaskAttemptId() + \u0026#34; -\u0026gt; (\u0026#34; + 98 WritableUtils.readVInt(dib) + \u0026#34;, \u0026#34; + 99 WritableUtils.readVInt(dib) + \u0026#34;) from \u0026#34; + 100 mapOutputLoc.getHost()); 101 } 102 103 return mapOutput; 104 } 7.ReduceTask.ReduceCopier.MapOutputCopier的shuffleToDisk 方法把map输出拷贝到本地磁盘。当map的输出不能再内存中存储时，调用该方法。\n1private MapOutput shuffleToDisk(MapOutputLocation mapOutputLoc, 2 InputStream input, 3 Path filename, 4 long mapOutputLength) 5 throws IOException { 6 // Find out a suitable location for the output on local-filesystem 7 Path localFilename = 8 lDirAlloc.getLocalPathForWrite(filename.toUri().getPath(), 9 mapOutputLength, conf); 10 11 MapOutput mapOutput = 12 new MapOutput(mapOutputLoc.getTaskId(), mapOutputLoc.getTaskAttemptId(), 13 conf, localFileSys.makeQualified(localFilename), 14 mapOutputLength); 15 16 17 // Copy data to local-disk 18 OutputStream output = null; 19 long bytesRead = 0; 20 try { 21 output = rfs.create(localFilename); 22 23 byte[] buf = new byte[64 * 1024]; 24 int n = input.read(buf, 0, buf.length); 25 while (n \u0026gt; 0) { 26 bytesRead += n; 27 shuffleClientMetrics.inputBytes(n); 28 output.write(buf, 0, n); 29 30 // indicate we\u0026#39;re making progress 31 reporter.progress(); 32 n = input.read(buf, 0, buf.length); 33 } 34 35 LOG.info(\u0026#34;Read \u0026#34; + bytesRead + \u0026#34; bytes from map-output for \u0026#34; + 36 mapOutputLoc.getTaskAttemptId()); 37 38 output.close(); 39 input.close(); 40 } catch (IOException ioe) { 41 LOG.info(\u0026#34;Failed to shuffle from \u0026#34; + mapOutputLoc.getTaskAttemptId(), 42 ioe); 43 44 // Discard the map-output 45 try { 46 mapOutput.discard(); 47 } catch (IOException ignored) { 48 LOG.info(\u0026#34;Failed to discard map-output from \u0026#34; + 49 mapOutputLoc.getTaskAttemptId(), ignored); 50 } 51 mapOutput = null; 52 53 // Close the streams 54 IOUtils.cleanup(LOG, input, output); 55 56 // Re-throw 57 throw ioe; 58 } 59 60 // Sanity check 61 if (bytesRead != mapOutputLength) { 62 try { 63 mapOutput.discard(); 64 } catch (Exception ioe) { 65 // IGNORED because we are cleaning up 66 LOG.info(\u0026#34;Failed to discard map-output from \u0026#34; + 67 mapOutputLoc.getTaskAttemptId(), ioe); 68 } catch (Throwable t) { 69 String msg = getTaskID() + \u0026#34; : Failed in shuffle to disk :\u0026#34; 70 + StringUtils.stringifyException(t); 71 reportFatalError(getTaskID(), t, msg); 72 } 73 mapOutput = null; 74 75 throw new IOException(\u0026#34;Incomplete map output received for \u0026#34; + 76 mapOutputLoc.getTaskAttemptId() + \u0026#34; from \u0026#34; + 77 mapOutputLoc.getOutputLocation() + \u0026#34; (\u0026#34; + 78 bytesRead + \u0026#34; instead of \u0026#34; + 79 mapOutputLength + \u0026#34;)\u0026#34; 80 ); 81 } 82 83 return mapOutput; 84 85 } 8.LocalFSMerger线程的run方法。Merge map输出的本地拷贝。\n1public void run() { 2 try { 3 LOG.info(reduceTask.getTaskID() + \u0026#34; Thread started: \u0026#34; + getName()); 4 while(!exitLocalFSMerge){ 5 // TreeSet\u0026lt;FileStatus\u0026gt;(mapOutputFileComparator)中存储了mapout的本地文件集合。 6 synchronized (mapOutputFilesOnDisk) { 7 List\u0026lt;Path\u0026gt; mapFiles = new ArrayList\u0026lt;Path\u0026gt;(); 8 long approxOutputSize = 0; 9 int bytesPerSum = 10 reduceTask.getConf().getInt(\u0026#34;io.bytes.per.checksum\u0026#34;, 512); 11 LOG.info(reduceTask.getTaskID() + \u0026#34;We have \u0026#34; + 12 mapOutputFilesOnDisk.size() + \u0026#34; map outputs on disk. \u0026#34; + 13 \u0026#34;Triggering merge of \u0026#34; + ioSortFactor + \u0026#34; files\u0026#34;); 14 // 1. Prepare the list of files to be merged. This list is prepared 15 // using a list of map output files on disk. Currently we merge 16 // io.sort.factor files into 1. 17 //1. io.sort.factor构造List\u0026lt;Path\u0026gt; mapFiles，准备合并。 synchronized (mapOutputFilesOnDisk) { 18 for (int i = 0; i \u0026lt; ioSortFactor; ++i) { 19 FileStatus filestatus = mapOutputFilesOnDisk.first(); 20 mapOutputFilesOnDisk.remove(filestatus); 21 mapFiles.add(filestatus.getPath()); 22 approxOutputSize += filestatus.getLen(); 23 } 24 } 25 26 27 28 // add the checksum length 29 approxOutputSize += ChecksumFileSystem 30 .getChecksumLength(approxOutputSize, 31 bytesPerSum); 32 33 // 2. 对list中的文件进行合并。 34 Path outputPath = 35 lDirAlloc.getLocalPathForWrite(mapFiles.get(0).toString(), 36 approxOutputSize, conf) 37 .suffix(\u0026#34;.merged\u0026#34;); 38 Writer writer = 39 new Writer(conf,rfs, outputPath, 40 conf.getMapOutputKeyClass(), 41 conf.getMapOutputValueClass(), 42 codec, null); 43 RawKeyValueIterator iter = null; 44 Path tmpDir = new Path(reduceTask.getTaskID().toString()); 45 try { 46 iter = Merger.merge(conf, rfs, 47 conf.getMapOutputKeyClass(), 48 conf.getMapOutputValueClass(), 49 codec, mapFiles.toArray(new Path[mapFiles.size()]), 50 true, ioSortFactor, tmpDir, 51 conf.getOutputKeyComparator(), reporter, 52 spilledRecordsCounter, null); 53 54 Merger.writeFile(iter, writer, reporter, conf); 55 writer.close(); 56 } catch (Exception e) { 57 localFileSys.delete(outputPath, true); 58 throw new IOException (StringUtils.stringifyException(e)); 59 } 60 61 synchronized (mapOutputFilesOnDisk) { 62 addToMapOutputFilesOnDisk(localFileSys.getFileStatus(outputPath)); 63 } 64 LOG.info(reduceTask.getTaskID() + 65 \u0026#34; Finished merging \u0026#34; + mapFiles.size() + 66 \u0026#34; map output files on disk of total-size \u0026#34; + 67 approxOutputSize + \u0026#34;.\u0026#34; + 68 \u0026#34; Local output file is \u0026#34; + outputPath + \u0026#34; of size \u0026#34; + 69 localFileSys.getFileStatus(outputPath).getLen()); 70 } 71 } catch (Throwable t) { 72 LOG.warn(reduceTask.getTaskID() 73 + \u0026#34; Merging of the local FS files threw an exception: \u0026#34; 74 + StringUtils.stringifyException(t)); 75 if (mergeThrowable == null) { 76 mergeThrowable = t; 77 } 78 } 79 } 80} 9.InMemFSMergeThread线程的run方法。\n1 2 public void run() { 3 LOG.info(reduceTask.getTaskID() + \u0026#34; Thread started: \u0026#34; + getName()); 4 try { 5 boolean exit = false; 6 do { 7 exit = ramManager.waitForDataToMerge(); 8 if (!exit) { 9 doInMemMerge(); 10 } 11 } while (!exit); 12 } catch (Throwable t) { 13 LOG.warn(reduceTask.getTaskID() + 14 \u0026#34; Merge of the inmemory files threw an exception: \u0026#34; 15 + StringUtils.stringifyException(t)); 16 ReduceCopier.this.mergeThrowable = t; 17 } 18 } InMemFSMergeThread线程的doInMemMerge方法，\n1private void doInMemMerge() throws IOException{ 2 if (mapOutputsFilesInMemory.size() == 0) { 3 return; 4 } 5 6 TaskID mapId = mapOutputsFilesInMemory.get(0).mapId; 7 8 List\u0026lt;Segment\u0026lt;K, V\u0026gt;\u0026gt; inMemorySegments = new ArrayList\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt;(); 9 long mergeOutputSize = createInMemorySegments(inMemorySegments, 0); 10 int noInMemorySegments = inMemorySegments.size(); 11 12 Path outputPath = mapOutputFile.getInputFileForWrite(mapId, 13 reduceTask.getTaskID(), mergeOutputSize); 14 15 Writer writer = 16 new Writer(conf, rfs, outputPath, 17 conf.getMapOutputKeyClass(), 18 conf.getMapOutputValueClass(), 19 codec, null); 20 21 RawKeyValueIterator rIter = null; 22 try { 23 LOG.info(\u0026#34;Initiating in-memory merge with \u0026#34; + noInMemorySegments + 24 \u0026#34; segments...\u0026#34;); 25 26 rIter = Merger.merge(conf, rfs, 27 (Class\u0026lt;K\u0026gt;)conf.getMapOutputKeyClass(), 28 (Class\u0026lt;V\u0026gt;)conf.getMapOutputValueClass(), 29 inMemorySegments, inMemorySegments.size(), 30 new Path(reduceTask.getTaskID().toString()), 31 conf.getOutputKeyComparator(), reporter, 32 spilledRecordsCounter, null); 33 34 if (combinerRunner == null) { 35 Merger.writeFile(rIter, writer, reporter, conf); 36 } else { 37 combineCollector.setWriter(writer); 38 combinerRunner.combine(rIter, combineCollector); 39 } 40 writer.close(); 41 42 LOG.info(reduceTask.getTaskID() + 43 \u0026#34; Merge of the \u0026#34; + noInMemorySegments + 44 \u0026#34; files in-memory complete.\u0026#34; + 45 \u0026#34; Local file is \u0026#34; + outputPath + \u0026#34; of size \u0026#34; + 46 localFileSys.getFileStatus(outputPath).getLen()); 47 } catch (Exception e) { 48 //make sure that we delete the ondisk file that we created 49 //earlier when we invoked cloneFileAttributes 50 localFileSys.delete(outputPath, true); 51 throw (IOException)new IOException 52 (\u0026#34;Intermediate merge failed\u0026#34;).initCause(e); 53 } 54 55 // Note the output of the merge 56 FileStatus status = localFileSys.getFileStatus(outputPath); 57 synchronized (mapOutputFilesOnDisk) { 58 addToMapOutputFilesOnDisk(status); 59 } 60 } 61 } 11.ReduceCopier.GetMapEventsThread线程的run方法。通过RPC询问TaskTracker，对每个完成的Event，获取maptask所在的服务器地址，即MapTask输出的地址，构造URL，加入到mapLocations，供copier线程获取。\n1public void run() { 2 3 LOG.info(reduceTask.getTaskID() + \u0026#34; Thread started: \u0026#34; + getName()); 4 5 do { 6 try { 7 int numNewMaps = getMapCompletionEvents(); 8 if (numNewMaps \u0026gt; 0) { 9 LOG.info(reduceTask.getTaskID() + \u0026#34;: \u0026#34; + 10 \u0026#34;Got \u0026#34; + numNewMaps + \u0026#34; new map-outputs\u0026#34;); 11 } 12 Thread.sleep(SLEEP_TIME); 13 } 14 catch (InterruptedException e) { 15 LOG.warn(reduceTask.getTaskID() + 16 \u0026#34; GetMapEventsThread returning after an \u0026#34; + 17 \u0026#34; interrupted exception\u0026#34;); 18 return; 19 } 20 catch (Throwable t) { 21 LOG.warn(reduceTask.getTaskID() + 22 \u0026#34; GetMapEventsThread Ignoring exception : \u0026#34; + 23 StringUtils.stringifyException(t)); 24 } 25 } while (!exitGetMapEvents); 26 27 LOG.info(\u0026#34;GetMapEventsThread exiting\u0026#34;); 28 29 } 12.ReduceCopier.GetMapEventsThread线程的getMapCompletionEvents方法。通过RPC询问TaskTracker，对每个完成的Event，获取maptask所在的服务器地址，构造URL，加入到mapLocations。\n1private int getMapCompletionEvents() throws IOException { 2 3 int numNewMaps = 0; 4 5 //RPC调用Tasktracker的getMapCompletionEvents方法，获得MapTaskCompletionEventsUpdate，进而获得TaskCompletionEvents 6 MapTaskCompletionEventsUpdate update = 7 umbilical.getMapCompletionEvents(reduceTask.getJobID(), 8 fromEventId.get(), 9 MAX_EVENTS_TO_FETCH, 10 reduceTask.getTaskID()); 11 TaskCompletionEvent events[] = update.getMapTaskCompletionEvents(); 12 13 // Check if the reset is required. 14 // Since there is no ordering of the task completion events at the 15 // reducer, the only option to sync with the new jobtracker is to reset 16 // the events index 17 if (update.shouldReset()) { 18 fromEventId.set(0); 19 obsoleteMapIds.clear(); // clear the obsolete map 20 mapLocations.clear(); // clear the map locations mapping 21 } 22 23 // Update the last seen event ID 24 fromEventId.set(fromEventId.get() + events.length); 25 26 // Process the TaskCompletionEvents: 27 // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs. 28 // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop 29 // fetching from those maps. 30 // 3. Remove TIPFAILED maps from neededOutputs since we don\u0026#39;t need their 31 // outputs at all. 32 //对每个完成的Event，获取maptask所在的服务器地址，构造URL，加入到mapLocations，供copier线程获取。 33 for (TaskCompletionEvent event : events) { 34 switch (event.getTaskStatus()) { 35 case SUCCEEDED: 36 { 37 URI u = URI.create(event.getTaskTrackerHttp()); 38 String host = u.getHost(); 39 TaskAttemptID taskId = event.getTaskAttemptId(); 40 int duration = event.getTaskRunTime(); 41 if (duration \u0026gt; maxMapRuntime) { 42 maxMapRuntime = duration; 43 // adjust max-fetch-retries based on max-map-run-time 44 maxFetchRetriesPerMap = Math.max(MIN_FETCH_RETRIES_PER_MAP, 45 getClosestPowerOf2((maxMapRuntime / BACKOFF_INIT) + 1)); 46 } 47 URL mapOutputLocation = new URL(event.getTaskTrackerHttp() + 48 \u0026#34;/mapOutput?job=\u0026#34; + taskId.getJobID() + 49 \u0026#34;\u0026amp;map=\u0026#34; + taskId + 50 \u0026#34;\u0026amp;reduce=\u0026#34; + getPartition()); 51 List\u0026lt;MapOutputLocation\u0026gt; loc = mapLocations.get(host); 52 if (loc == null) { 53 loc = Collections.synchronizedList 54 (new LinkedList\u0026lt;MapOutputLocation\u0026gt;()); 55 mapLocations.put(host, loc); 56 } 57 loc.add(new MapOutputLocation(taskId, host, mapOutputLocation)); 58 numNewMaps ++; 59 } 60 break; 61 case FAILED: 62 case KILLED: 63 case OBSOLETE: 64 { 65 obsoleteMapIds.add(event.getTaskAttemptId()); 66 LOG.info(\u0026#34;Ignoring obsolete output of \u0026#34; + event.getTaskStatus() + 67 \u0026#34; map-task: \u0026#39;\u0026#34; + event.getTaskAttemptId() + \u0026#34;\u0026#39;\u0026#34;); 68 } 69 break; 70 case TIPFAILED: 71 { 72 copiedMapOutputs.add(event.getTaskAttemptId().getTaskID()); 73 LOG.info(\u0026#34;Ignoring output of failed map TIP: \u0026#39;\u0026#34; + 74 event.getTaskAttemptId() + \u0026#34;\u0026#39;\u0026#34;); 75 } 76 break; 77 } 78 } 79 return numNewMaps; 80 } 81} 82} 13.ReduceTask.ReduceCopier的createKVIterator方法，从拷贝到的map输出创建RawKeyValueIterator，作为reduce的输入。\n1private RawKeyValueIterator createKVIterator( 2 JobConf job, FileSystem fs, Reporter reporter) throws IOException { 3 4 // merge config params 5 Class\u0026lt;K\u0026gt; keyClass = (Class\u0026lt;K\u0026gt;)job.getMapOutputKeyClass(); 6 Class\u0026lt;V\u0026gt; valueClass = (Class\u0026lt;V\u0026gt;)job.getMapOutputValueClass(); 7 boolean keepInputs = job.getKeepFailedTaskFiles(); 8 final Path tmpDir = new Path(getTaskID().toString()); 9 final RawComparator\u0026lt;K\u0026gt; comparator = 10 (RawComparator\u0026lt;K\u0026gt;)job.getOutputKeyComparator(); 11 12 // segments required to vacate memory 13 List\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt; memDiskSegments = new ArrayList\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt;(); 14 long inMemToDiskBytes = 0; 15 if (mapOutputsFilesInMemory.size() \u0026gt; 0) { 16 TaskID mapId = mapOutputsFilesInMemory.get(0).mapId; 17 inMemToDiskBytes = createInMemorySegments(memDiskSegments, 18 maxInMemReduce); 19 final int numMemDiskSegments = memDiskSegments.size(); 20 if (numMemDiskSegments \u0026gt; 0 \u0026amp;\u0026amp; 21 ioSortFactor \u0026gt; mapOutputFilesOnDisk.size()) { 22 // must spill to disk, but can\u0026#39;t retain in-mem for intermediate merge 23 final Path outputPath = mapOutputFile.getInputFileForWrite(mapId, 24 reduceTask.getTaskID(), inMemToDiskBytes); 25 final RawKeyValueIterator rIter = Merger.merge(job, fs, 26 keyClass, valueClass, memDiskSegments, numMemDiskSegments, 27 tmpDir, comparator, reporter, spilledRecordsCounter, null); 28 final Writer writer = new Writer(job, fs, outputPath, 29 keyClass, valueClass, codec, null); 30 try { 31 Merger.writeFile(rIter, writer, reporter, job); 32 addToMapOutputFilesOnDisk(fs.getFileStatus(outputPath)); 33 } catch (Exception e) { 34 if (null != outputPath) { 35 fs.delete(outputPath, true); 36 } 37 throw new IOException(\u0026#34;Final merge failed\u0026#34;, e); 38 } finally { 39 if (null != writer) { 40 writer.close(); 41 } 42 } 43 LOG.info(\u0026#34;Merged \u0026#34; + numMemDiskSegments + \u0026#34; segments, \u0026#34; + 44 inMemToDiskBytes + \u0026#34; bytes to disk to satisfy \u0026#34; + 45 \u0026#34;reduce memory limit\u0026#34;); 46 inMemToDiskBytes = 0; 47 memDiskSegments.clear(); 48 } else if (inMemToDiskBytes != 0) { 49 LOG.info(\u0026#34;Keeping \u0026#34; + numMemDiskSegments + \u0026#34; segments, \u0026#34; + 50 inMemToDiskBytes + \u0026#34; bytes in memory for \u0026#34; + 51 \u0026#34;intermediate, on-disk merge\u0026#34;); 52 } 53 } 54 55 // segments on disk 56 List\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt; diskSegments = new ArrayList\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt;(); 57 long onDiskBytes = inMemToDiskBytes; 58 Path[] onDisk = getMapFiles(fs, false); 59 for (Path file : onDisk) { 60 onDiskBytes += fs.getFileStatus(file).getLen(); 61 diskSegments.add(new Segment\u0026lt;K, V\u0026gt;(job, fs, file, codec, keepInputs)); 62 } 63 LOG.info(\u0026#34;Merging \u0026#34; + onDisk.length + \u0026#34; files, \u0026#34; + 64 onDiskBytes + \u0026#34; bytes from disk\u0026#34;); 65 Collections.sort(diskSegments, new Comparator\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt;() { 66 public int compare(Segment\u0026lt;K, V\u0026gt; o1, Segment\u0026lt;K, V\u0026gt; o2) { 67 if (o1.getLength() == o2.getLength()) { 68 return 0; 69 } 70 return o1.getLength() \u0026lt; o2.getLength() ? -1 : 1; 71 } 72 }); 73 74 // build final list of segments from merged backed by disk + in-mem 75 List\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt; finalSegments = new ArrayList\u0026lt;Segment\u0026lt;K,V\u0026gt;\u0026gt;(); 76 long inMemBytes = createInMemorySegments(finalSegments, 0); 77 LOG.info(\u0026#34;Merging \u0026#34; + finalSegments.size() + \u0026#34; segments, \u0026#34; + 78 inMemBytes + \u0026#34; bytes from memory into reduce\u0026#34;); 79 if (0 != onDiskBytes) { 80 final int numInMemSegments = memDiskSegments.size(); 81 diskSegments.addAll(0, memDiskSegments); 82 memDiskSegments.clear(); 83 RawKeyValueIterator diskMerge = Merger.merge( 84 job, fs, keyClass, valueClass, diskSegments, 85 ioSortFactor, numInMemSegments, tmpDir, comparator, 86 reporter, false, spilledRecordsCounter, null); 87 diskSegments.clear(); 88 if (0 == finalSegments.size()) { 89 return diskMerge; 90 } 91 finalSegments.add(new Segment\u0026lt;K,V\u0026gt;( 92 new RawKVIteratorReader(diskMerge, onDiskBytes), true)); 93 } 94 return Merger.merge(job, fs, keyClass, valueClass, 95 finalSegments, finalSegments.size(), tmpDir, 96 comparator, reporter, spilledRecordsCounter, null); 97 } 14.ReduceTask的runNewReducer方法。根据配置构造reducer以及其运行的上下文，调用reducer的reduce方法。\n1@SuppressWarnings(\u0026#34;unchecked\u0026#34;) 2 private \u0026lt;INKEY,INVALUE,OUTKEY,OUTVALUE\u0026gt; 3 void runNewReducer(JobConf job, 4 final TaskUmbilicalProtocol umbilical, 5 final TaskReporter reporter, 6 RawKeyValueIterator rIter, 7 RawComparator\u0026lt;INKEY\u0026gt; comparator, 8 Class\u0026lt;INKEY\u0026gt; keyClass, 9 Class\u0026lt;INVALUE\u0026gt; valueClass 10 ) throws IOException,InterruptedException, 11 ClassNotFoundException { 12 //1. 构造TaskContext 13 org.apache.hadoop.mapreduce.TaskAttemptContext taskContext = 14 new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID()); 15 //2. 根据配置的Reducer类构造一个Reducer实例 16 org.apache.hadoop.mapreduce.Reducer\u0026lt;INKEY,INVALUE,OUTKEY,OUTVALUE\u0026gt; reducer = (org.apache.hadoop.mapreduce.Reducer\u0026lt;INKEY,INVALUE,OUTKEY,OUTVALUE\u0026gt;) 17 ReflectionUtils.newInstance(taskContext.getReducerClass(), job); 18 //3. 构造RecordWriter 19 org.apache.hadoop.mapreduce.RecordWriter\u0026lt;OUTKEY,OUTVALUE\u0026gt; output = 20 (org.apache.hadoop.mapreduce.RecordWriter\u0026lt;OUTKEY,OUTVALUE\u0026gt;) 21 outputFormat.getRecordWriter(taskContext); 22 job.setBoolean(\u0026#34;mapred.skip.on\u0026#34;, isSkipping()); 23 24 //4. 构造Context，是Reducer运行的上下文 25 org.apache.hadoop.mapreduce.Reducer.Context 26 reducerContext = createReduceContext(reducer, job, getTaskID(), 27 rIter, reduceInputValueCounter, 28 output, committer, 29 reporter, comparator, keyClass, 30 valueClass); 31 reducer.run(reducerContext); 32 output.close(reducerContext); 33 } 15.抽象类Reducer的run方法。从上下文中取出一个key和该key对应的Value集合（Iterable类型），调用reducer的reduce方法进行处理。\n1public void run(Context context) throws IOException, InterruptedException { 2 setup(context); 3 while (context.nextKey()) { 4 reduce(context.getCurrentKey(), context.getValues(), context); 5 } 6 cleanup(context); 7 } 16.Reducer类的reduce，是用户一般会覆盖来执行reduce处理逻辑的方法。\n1@SuppressWarnings(\u0026#34;unchecked\u0026#34;) 2 protected void reduce(KEYIN key, Iterable\u0026lt;VALUEIN\u0026gt; values, Context context 3 ) throws IOException, InterruptedException { 4 for(VALUEIN value: values) { 5 context.write((KEYOUT) key, (VALUEOUT) value); 6 } 完。\n","link":"https://idouba.com/hadoop_mapreduce_tasktracker_child_reduce/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】Hadoop作业提交之Child启动reduce任务"},{"body":"一、概要描述\n在上篇博文描述了TaskTracker启动一个独立的java进程来执行Map或Reduce任务。在本篇和下篇博文中我们会关注启动的那个入口是org.apache.hadoop.mapred.Child的这个Java进程是如何执行用户定义的map或Reduce任务的。\n接上篇文章，TaskRunner线程执行中，会构造一个_java –D** Child address port tasked_这 样第一个java命令，单独启动一个java进程。在Child的main函数中通过TaskUmbilicalProtocol协议，从 TaskTracker获得需要执行的Task，并调用Task的run方法来执行，而Task的run方法会通过java反射机制构造 Mapper，InputFormat，mapperContext，然后调用构造的mapper的run方法执行mapper操作。\n二、 流程描述\nChild类根据前面输入的三个参数，即tasktracher的地址、端口、taskid。通过TaskUmbilicalProtocol协议，从TaskTracker获得需要执行的Task，在Child的main函数中调用执行。 在Chilld中，执行Task的run方法。Task 的run方法。是真正执行用户定义的map或者reduce任务的入口，通过TaskUmbilicalProtocol向tasktracker上报执行进度。 在MapTask的run中执行runMapper方法来调用mapper定义的方法。 在runNewMapper方法中构造mapper实例和mapper执行的配置信息。并执行mapper.run方法来调用到用户定义的mapper的方法。 mapper的run方法中，从输入数据中逐一取出调用map方法来处理每一条数据 mapper的map方法是真正用户定义的处理数据的类。也是用户唯一需要定义的方法。 三、代码详细\nChild的main方法每个task进程都会被在单独的进程中执行，这个方法就是这些进程的入口方法。观察下载在这个方法中做了哪些事情？ 1)从传入的参数中获得tasktracker的地址、从传入的参数中获得tasktracker的地址\n根据获取的taskTracker的地址和端口通过RPC方式和tasktracker通信，umbilical是作为tasktracker的代理来执行操作。\n根据JvmId从taskTracker查询获取到JvmTask\n执行任务\n1public static void main(String[] args) throws Throwable { 2 LOG.debug(\u0026#34;Child starting\u0026#34;); 3JobConf defaultConf = new JobConf(); 4 5//从传入的参数中获得taskTracker的地址 6String host = args[0]; 7//从传入的参数中获得taskTracker的响应请求的端口。 8 int port = Integer.parseInt(args[1]); 9 InetSocketAddress address = new InetSocketAddress(host, port); 10 final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]); 11 final int SLEEP_LONGER_COUNT = 5; 12 int jvmIdInt = Integer.parseInt(args[3]); 13 JVMId jvmId = new JVMId(firstTaskid.getJobID(),firstTaskid.isMap(),jvmIdInt); 1//通过RPC方式和tasktracker通信，umbilical是作为tasktracker的代理来执行操作。 2TaskUmbilicalProtocol umbilical = 3 (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class, 4 TaskUmbilicalProtocol.versionID, 5 address, 6 defaultConf); 7int numTasksToExecute = -1; //-1 signifies \u0026#34;no limit\u0026#34; 8int numTasksExecuted = 0; 9//for the memory management, a PID file is written and the PID file 10//is written once per JVM. We simply symlink the file on a per task 11//basis later (see below). Long term, we should change the Memory 12//manager to use JVMId instead of TaskAttemptId 13Path srcPidPath = null; 14Path dstPidPath = null; 15int idleLoopCount = 0; 16Task task = null; 17try { 18 while (true) { 19 taskid = null; 20 //根据JvmId从taskTracker查询获取到JvmTask 21 JvmTask myTask = umbilical.getTask(jvmId); 22 if (myTask.shouldDie()) { 23 break; 24 } else { 25 if (myTask.getTask() == null) { 26 taskid = null; 27 if (++idleLoopCount \u0026gt;= SLEEP_LONGER_COUNT) { 28 //we sleep for a bigger interval when we don\u0026#39;t receive 29 //tasks for a while 30 Thread.sleep(1500); 31 } else { 32 Thread.sleep(500); 33 } 34 continue; 35 } 36 } 37 idleLoopCount = 0; 38 task = myTask.getTask(); 39 taskid = task.getTaskID(); 40 isCleanup = task.isTaskCleanupTask(); 41 // reset the statistics for the task 42 FileSystem.clearStatistics(); 43 TaskLog.syncLogs(firstTaskid, taskid, isCleanup); 44 JobConf job = new JobConf(task.getJobFile()); 45 if (job.getBoolean(\u0026#34;task.memory.mgmt.enabled\u0026#34;, false)) { 46 if (srcPidPath == null) { 47 srcPidPath = new Path(task.getPidFile()); 48 } 49 //since the JVM is running multiple tasks potentially, we need 50 //to do symlink stuff only for the subsequent tasks 51 if (!taskid.equals(firstTaskid)) { 52 dstPidPath = new Path(task.getPidFile()); 53 FileUtil.symLink(srcPidPath.toUri().getPath(), 54 dstPidPath.toUri().getPath()); 55 } 56 } 57 //setupWorkDir actually sets up the symlinks for the distributed 58 //cache. After a task exits we wipe the workdir clean, and hence 59 //the symlinks have to be rebuilt. 60 TaskRunner.setupWorkDir(job); 61 62 numTasksToExecute = job.getNumTasksToExecutePerJvm(); 63 assert(numTasksToExecute != 0); 64 TaskLog.cleanup(job.getInt(\u0026#34;mapred.userlog.retain.hours\u0026#34;, 24)); 65 task.setConf(job); 66 defaultConf.addResource(new Path(task.getJobFile())); 67 // use job-specified working directory 68 FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory()); 69 try { 70 //执行任务 71 task.run(job, umbilical); // run the task 72 } finally { 73 TaskLog.syncLogs(firstTaskid, taskid, isCleanup); 74 if (!taskid.equals(firstTaskid) \u0026amp;\u0026amp; 75 job.getBoolean(\u0026#34;task.memory.mgmt.enabled\u0026#34;, false)) { 76 // delete the pid-file\u0026#39;s symlink 77 new File(dstPidPath.toUri().getPath()).delete(); 78 } 79 } 80 if (numTasksToExecute \u0026gt; 0 \u0026amp;\u0026amp; ++numTasksExecuted == numTasksToExecute) { 81 break; 82 } 83 } 84} catch (FSError e) { 85 LOG.fatal(\u0026#34;FSError from child\u0026#34;, e); 86 umbilical.fsError(taskid, e.getMessage()); 87} catch (Throwable throwable) { 88 LOG.warn(\u0026#34;Error running child\u0026#34;, throwable); 89 try { 90 if (task != null) { 91 // do cleanup for the task 92 task.taskCleanup(umbilical); 93 } 94 } catch (Throwable th) { 95 LOG.info(\u0026#34;Error cleaning up\u0026#34; + th); 96 } 97 // Report back any failures, for diagnostic purposes 98 ByteArrayOutputStream baos = new ByteArrayOutputStream(); 99 throwable.printStackTrace(new PrintStream(baos)); 100 if (taskid != null) { 101 umbilical.reportDiagnosticInfo(taskid, baos.toString()); 102 } 103} finally { 104 RPC.stopProxy(umbilical); 105 106} 107 } TaskTracker 的getTask方法。TaskTracker实现了TaskUmbilicalProtocol接扣。getTask是该接口定义的一个方法。是子进程Child调用的根据jvmId获取task。 1public synchronized JvmTask getTask(JVMId jvmId) 2 throws IOException { 3 TaskInProgress tip = jvmManager.getTaskForJvm(jvmId); 4 if (tip == null) { 5 return new JvmTask(null, false); 6 } 7 if (tasks.get(tip.getTask().getTaskID()) != null) { //is task still present 8 LOG.info(\u0026#34;JVM with ID: \u0026#34; + jvmId + \u0026#34; given task: \u0026#34; + 9 tip.getTask().getTaskID()); 10 return new JvmTask(tip.getTask(), false); 11 } else { 12 LOG.info(\u0026#34;Killing JVM with ID: \u0026#34; + jvmId + \u0026#34; since scheduled task: \u0026#34; + 13 tip.getTask().getTaskID() + \u0026#34; is \u0026#34; + tip.taskStatus.getRunState()); 14 return new JvmTask(null, true); 15} 3.Task 的run方法。因为map和reduce的执行逻辑大不相同，先看下MapTask中该方法的实现。是真正执行用户定义的map或者reduce任务的入 口，通过TaskUmbilicalProtocol向tasktracker上报执行进度。开启线程向TaskTracker上报进度，根据task的 不同动作要求执行不同的方法，如jobClean，jobsetup，taskCleanup。对于部分的了解可以产看taskTracker获取Task文章中的JobTracker的 heartbeat方法处的详细解释。\n1public void run(final JobConf job, final TaskUmbilicalProtocol umbilical) 2 throws IOException, ClassNotFoundException, InterruptedException { 3 4 // 开启线程向TaskTracker上报进度 5 TaskReporter reporter = new TaskReporter(getProgress(), umbilical); 6 reporter.startCommunicationThread(); 7 boolean useNewApi = job.getUseNewMapper(); 8 initialize(job, getJobID(), reporter, useNewApi); 9 10 // 根据task的不同动作要求执行不同的方法，如jobClean，jobsetup，taskCleanup 11 if (jobCleanup) { 12 runJobCleanupTask(umbilical, reporter); 13 return; 14 } 15 if (jobSetup) { 16 runJobSetupTask(umbilical, reporter); 17 return; 18 } 19 if (taskCleanup) { 20 runTaskCleanupTask(umbilical, reporter); 21 return; 22 } 23 24 if (useNewApi) { 25 runNewMapper(job, split, umbilical, reporter); 26 } else { 27 runOldMapper(job, split, umbilical, reporter); 28 } 29 done(umbilical, reporter); 30 } TaskReporter的run方法。定时向父进程TaskTracker上报状态和进度。 1public void run() { 2 final int MAX_RETRIES = 3; 3 int remainingRetries = MAX_RETRIES; 4 // get current flag value and reset it as well 5 boolean sendProgress = resetProgressFlag(); 6 while (!taskDone.get()) { 7 try { 8 boolean taskFound = true; // whether TT knows about this task 9 // sleep for a bit 10 try { 11 Thread.sleep(PROGRESS_INTERVAL); 12 } 13 break; 14 } 15 16 if (sendProgress) { 17 // we need to send progress update 18 updateCounters(); 19 taskStatus.statusUpdate(taskProgress.get(), 20 taskProgress.toString(), 21 counters); 22 taskFound = umbilical.statusUpdate(taskId, taskStatus); 23 taskStatus.clearStatus(); 24 } 25 else { 26 // send ping 27 taskFound = umbilical.ping(taskId); 28 } 29 30 // if Task Tracker is not aware of our task ID (probably because it died and 31 // came back up), kill ourselves 32 if (!taskFound) { 33 LOG.warn(\u0026#34;Parent died. Exiting \u0026#34;+taskId); 34 System.exit(66); 35 } 36 37 sendProgress = resetProgressFlag(); 38 remainingRetries = MAX_RETRIES; 39 } 40 catch (Throwable t) { 41 } 42 } 43 } 44 } Task 的Initialize方法初始化后续要执行的几个重要变量。包括JobContext OutputFormat OutputCommitter等，这些都是后续执行中要用到的属性实例。 1public void initialize(JobConf job, JobID id, 2 Reporter reporter, 3 boolean useNewApi) throws IOException, 4 ClassNotFoundException, 5 InterruptedException { 6 jobContext = new JobContext(job, id, reporter); 7 taskContext = new TaskAttemptContext(job, taskId, reporter); 8 if (getState() == TaskStatus.State.UNASSIGNED) { 9 setState(TaskStatus.State.RUNNING); 10 } 11 if (useNewApi) { 12 LOG.debug(\u0026#34;using new api for output committer\u0026#34;); 13 outputFormat = 14 ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), job); 15 committer = outputFormat.getOutputCommitter(taskContext); 16 } else { 17 committer = conf.getOutputCommitter(); 18 } 19 Path outputPath = FileOutputFormat.getOutputPath(conf); 20 if (outputPath != null) { 21 if ((committer instanceof FileOutputCommitter)) { 22 FileOutputFormat.setWorkOutputPath(conf, 23 ((FileOutputCommitter)committer).getTempTaskOutputPath(taskContext)); 24 } else { 25 FileOutputFormat.setWorkOutputPath(conf, outputPath); 26 } 27 } 28 committer.setupTask(taskContext); 29 } Task的 runJobCleanupTask方法。即如果在Task是jobCleanup，则调用OutputCommitter删除输出文件 1protected void runJobCleanupTask(TaskUmbilicalProtocol umbilical, 2 TaskReporter reporter 3 ) throws IOException, InterruptedException { 4 // set phase for this task 5 setPhase(TaskStatus.Phase.CLEANUP); 6 getProgress().setStatus(\u0026#34;cleanup\u0026#34;); 7 statusUpdate(umbilical); 8 // do the cleanup 9 committer.cleanupJob(jobContext); 10 done(umbilical, reporter); 11 } 7.Task的runJobSetupTask。如果Task是setupTask，则调用OutputCommitter，如创建Task要执行的根目录。\n1protected void runJobSetupTask(TaskUmbilicalProtocol umbilical, 2 TaskReporter reporter 3 ) throws IOException, InterruptedException { 4 // do the setup 5 getProgress().setStatus(\u0026#34;setup\u0026#34;); 6 committer.setupJob(jobContext); 7 done(umbilical, reporter); 8 } Task的runTaskCleanupTask。如果Task是taskCleanup，则调用taskCleanup 方法。最终OutputCommitter方法删除task的工作目录。 1protected void runTaskCleanupTask(TaskUmbilicalProtocol umbilical, 2 TaskReporter reporter) 3 throws IOException, InterruptedException { 4 taskCleanup(umbilical); 5 done(umbilical, reporter); 6 } 9.MapTask的runNewMapper方法是我们要重点关注的方法，是真正执行用户定义的map的方法。\n1)根据传入的jobconf构造一个context，包含了job相关的所有配置信息，如后面用到的mapper、inputformat等。\n2)根据配置的mapper类创建一个Mapper实例\n3)根据配置的inputformat创建一个InputFormat实例。\n4)重新够构建InputSplit\n5)创建RecordReader，其实使用的是适配器模式适配了inputFormat的Reader。\n6)构造输出RecordWriter。当没有Reducer时，output是配置的outputFormat的RecordWriter，即直接写输出。如果ruducer数量不为0，则构造一个NewOutputCollector\n7)构造Mapper.Context，封装了刚才配置的所有信息，在map执行时候时候使用。\n8)调用mapper的run方法来执行map动作。\n1@SuppressWarnings(\u0026#34;unchecked\u0026#34;) 2 private \u0026lt;INKEY,INVALUE,OUTKEY,OUTVALUE\u0026gt; 3 void runNewMapper(final JobConf job, 4 final BytesWritable rawSplit, 5 final TaskUmbilicalProtocol umbilical, 6 TaskReporter reporter 7 ) throws IOException, ClassNotFoundException, 8 InterruptedException { 9 // 1. 根据传入的jobconf构造一个context，包含了job相关的所有配置信息，如后面用到的mapper、inputformat等。 10 org.apache.hadoop.mapreduce.TaskAttemptContext taskContext = 11 new org.apache.hadoop.mapreduce.TaskAttemptContext(job, getTaskID()); 12 // 2. 根据配置的mapper类创建一个Mapper实例 13 org.apache.hadoop.mapreduce.Mapper\u0026lt;INKEY,INVALUE,OUTKEY,OUTVALUE\u0026gt; mapper = 14 (org.apache.hadoop.mapreduce.Mapper\u0026lt;INKEY,INVALUE,OUTKEY,OUTVALUE\u0026gt;) 15 ReflectionUtils.newInstance(taskContext.getMapperClass(), job); 16 // 根据配置的input format创建一个InputFormat实例。 17 org.apache.hadoop.mapreduce.InputFormat\u0026lt;INKEY,INVALUE\u0026gt; inputFormat = 18 (org.apache.hadoop.mapreduce.InputFormat\u0026lt;INKEY,INVALUE\u0026gt;) 19 ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job); 20 // 4.重新够构建InputSplit 21 org.apache.hadoop.mapreduce.InputSplit split = null; 22 DataInputBuffer splitBuffer = new DataInputBuffer(); 23 splitBuffer.reset(rawSplit.getBytes(), 0, rawSplit.getLength()); 24 SerializationFactory factory = new SerializationFactory(job); 25 Deserializer\u0026lt; extends org.apache.hadoop.mapreduce.InputSplit\u0026gt; 26 deserializer = 27 (Deserializer\u0026lt; extends org.apache.hadoop.mapreduce.InputSplit\u0026gt;) 28 factory.getDeserializer(job.getClassByName(splitClass)); 29 deserializer.open(splitBuffer); 30 split = deserializer.deserialize(null); 31 32 //5. 创建RecordReader，其实使用的是适配器模式适配了inputFormat的Reader。 33 org.apache.hadoop.mapreduce.RecordReader\u0026lt;INKEY,INVALUE\u0026gt; input = 34 new NewTrackingRecordReader\u0026lt;INKEY,INVALUE\u0026gt; 35 (inputFormat.createRecordReader(split, taskContext), reporter); 36 37 job.setBoolean(\u0026#34;mapred.skip.on\u0026#34;, isSkipping()); 38 org.apache.hadoop.mapreduce.RecordWriter output = null; 39 org.apache.hadoop.mapreduce.Mapper\u0026lt;INKEY,INVALUE,OUTKEY,OUTVALUE\u0026gt;.Context 40 mapperContext = null; 41 try { 42 Constructor\u0026lt;org.apache.hadoop.mapreduce.Mapper.Context\u0026gt; contextConstructor = 43 org.apache.hadoop.mapreduce.Mapper.Context.class.getConstructor 44 (new Class[]{org.apache.hadoop.mapreduce.Mapper.class, 45 Configuration.class, 46 org.apache.hadoop.mapreduce.TaskAttemptID.class, 47 org.apache.hadoop.mapreduce.RecordReader.class, 48 org.apache.hadoop.mapreduce.RecordWriter.class, 49 org.apache.hadoop.mapreduce.OutputCommitter.class, 50 org.apache.hadoop.mapreduce.StatusReporter.class, 51 org.apache.hadoop.mapreduce.InputSplit.class}); 52 53 //6. 构造输出RecordWriter。当没有Reducer时，output是配置的outputFormat的RecordWriter，即直接写输出。如果ruducer数量不为0，则构造一个NewOutputCollector 54 if (job.getNumReduceTasks() == 0) { 55 output = outputFormat.getRecordWriter(taskContext); 56 } else { 57 output = new NewOutputCollector(job, umbilical, reporter); 58 } 59 60 //7.构造Mapper.Context，封装了刚才配置的所有信息，在map执行时候时候使用。 61 mapperContext = contextConstructor.newInstance(mapper, job, getTaskID(), 62 input, output, committer, 63 reporter, split); 64 65 input.initialize(split, mapperContext); 66 //8. 调用mapper的run方法来执行map动作。 67 mapper.run(mapperContext); 68 input.close(); 69 output.close(mapperContext); 70 } catch (NoSuchMethodException e) { 71 throw new IOException(\u0026#34;Can\u0026#39;t find Context constructor\u0026#34;, e); 72 } catch (InstantiationException e) { 73 throw new IOException(\u0026#34;Can\u0026#39;t create Context\u0026#34;, e); 74 } catch (InvocationTargetException e) { 75 throw new IOException(\u0026#34;Can\u0026#39;t invoke Context constructor\u0026#34;, e); 76 } catch (IllegalAccessException e) { 77 throw new IOException(\u0026#34;Can\u0026#39;t invoke Context constructor\u0026#34;, e); 78 } 79 } 10.Mapper的run方法。即对每一个输入的记录执行map方法。一般不会改变，就是拿出输入记录逐条执行map方法。除非要改变记录的执行方式，（如MultithreadedMapper需要多线程来执行），一般该方法不用override。\n1public void run(Context context) throws IOException, InterruptedException { 2 setup(context); 3 while (context.nextKeyValue()) { 4 map(context.getCurrentKey(), context.getCurrentValue(), context); 5 } 6 cleanup(context); 7 } 8} 11.Mapper的map方法。即对每一个输入的记录执行map方法。这个只是默然的map执行方法，把输入不变的输出即可。用户定义的mapper就是override这个方法来按照自己定义的逻辑来处理数据。\n1protected void map(KEYIN key, VALUEIN value, 2 Context context) throws IOException, InterruptedException { 3 context.write((KEYOUT) key, (VALUEOUT) value); 4 } 完。\n","link":"https://idouba.com/hadoop_mapreduce_tasktracker_child_map/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】hadoop作业提交之Child启动map任务"},{"body":"一、概要描述\n在上篇博文描 述了TaskTracker从Jobtracker如何从JobTracker获取到要执行的Task。在从JobTracker获取到 LaunchTaskAction后，执行addToTaskQueue方法来把要执行的Task加入到queue。在本篇博文中，我们来关注下该方法 后，TaskTracker怎么来处理这些Task。\n实际上，TaskTracker初始化时，会初始化并启动两个TaskLauncher类型的线程，mapLauncher，reduceLauncher。在TaskTracker从JobTracher获取到任务后，对应的会把任务添加到两个 TaskLauncher的Queue中，其实是TaskLauncher维护的一个列表List tasksToLaunch。\nTaskLauncher线程一直会定时检查TaskTracher上面有slot开业运行新的Task，则启动 Task。在这个过程中，先把task运行需要的文件解压到本地，并创建根据Task类型（Map或者Reduce）创建一个TaskRunner线程， 在TaskRunner中JvmManager调用JvmManagerForType、JvmRunner来启动一个java进程来执行Map或Reduce任务。\n本文只是介绍到启动一个java进程，至于是什么样的java进程，对于maptask和reducetask分别是怎么执行的，在后面的child启动maptask，和child启动reducetask 会比较详细的介绍。\n二、 流程描述\ntasktracker的offerService方法获取到要执行的task后调用addToTaskQueue方法，其实是调用taskrunner的addToTaskQueue方法 TaskLauncher内部维护了一个List tasksToLaunch，只是把task加入到该 taskLauncher是一个线程，在其run方法中从tasksToLaunch集合中取出task来执行，调用Tasktracker的startNewTask方法启动task。 startNewtask方法中调用localizeJob方法把job相关的配置信息和要运行的jar拷贝到tasktracker本地，然后调用taskInProgress的launchTask方法来启动task。 TaskInProgress的launchTask方法先调用localizeTask(task把task相关的配置信息获取到本地。然后创建一个TaskRunner线程来启动task。 在TaskRunner的run方法中构建一个java命令的执行的条件，包括引用类，执行目录等，入口类是Child。然后调用JvmManager 的launchJvm方法来调用。 JvmManager 进而调用 JvmManagerForType的reapJvm，和spawnNewJvm 方法，发起调用. 在JvmManagerForType的spawnNewJvm 方法中创建了一个JvmRunner线程类执行调用. JvmRunner线程的run反复调用runChild方法来执行 一个命令行的调用。 三、代码详细\nTaskTracker的 addToTaskQueue方法。 接上文的最后一个方法的在heartbeat中把根据jobtracker的指令把需要launch的task调用addToTaskQueue方法加入task queue。\n1//根据task的类型不同加入到不同的launcher中。 2private void addToTaskQueue(LaunchTaskAction action) { 3if (action.getTask().isMapTask()) { 4mapLauncher.addToTaskQueue(action); 5}else { 6reduceLauncher.addToTaskQueue(action); 7} 8} TaskLauncher 的addToTaskQueue方法，即把要launch的task加入到TaskLauncher内维护的一个列表List tasksToLaunch;中。 1public void addToTaskQueue(LaunchTaskAction action) { 2 synchronized (tasksToLaunch) { 3 TaskInProgress tip = registerTask(action, this); 4 tasksToLaunch.add(tip); 5 tasksToLaunch.notifyAll(); 6 } 7} TaskLauncher线程的run方法。TaskLauncher是一个线程。一直检查task列表中有数据，取出一个来执行。 1public void run() { 2 TaskInProgress tip; 3 synchronized (tasksToLaunch) { 4 while (tasksToLaunch.isEmpty()) { 5 tasksToLaunch.wait(); 6 } 7 //get the TIP 8 tip = tasksToLaunch.remove(0); 9 //wait for a slot to run 10 synchronized (numFreeSlots) { 11 while (numFreeSlots.get() == 0) { 12 numFreeSlots.wait(); 13 } 14 LOG.info(\u0026#34;In TaskLauncher, current free slots : \u0026#34; + numFreeSlots.get()+ 15 \u0026#34; and trying to launch \u0026#34;+tip.getTask().getTaskID()); 16 numFreeSlots.set(numFreeSlots.get() - 1); 17 assert (numFreeSlots.get() \u0026gt;= 0); 18 } 19 20 //got a free slot. launch the task 21 startNewTask(tip); 22 return; // ALL DONE 23 } 24 } 25 } TaskTracker的startNewTask 启动一个新task。该方法的主要代码就一句。 1localizeJob(tip); TaskTracker的localizeJob方法。 初始化job的目录 1private void localizeJob(TaskInProgress tip) throws IOException { 2 Path localJarFile = null; 3 Task t = tip.getTask(); 4 JobID jobId = t.getJobID(); 5 Path jobFile = new Path(t.getJobFile()); 6 FileStatus status = null; 7 long jobFileSize = -1; 8 status = systemFS.getFileStatus(jobFile); 9 jobFileSize = status.getLen(); 10 11 Path localJobFile = lDirAlloc.getLocalPathForWrite( 12 getLocalJobDir(jobId.toString()) 13 + Path.SEPARATOR + \u0026#34;job.xml\u0026#34;, 14 jobFileSize, fConf); 15 RunningJob rjob = addTaskToJob(jobId, tip); 16 synchronized (rjob) { 17 if (!rjob.localized) { 18 19 FileSystem localFs = FileSystem.getLocal(fConf); 20 21 systemFS.copyToLocalFile(jobFile, localJobFile); 22 JobConf localJobConf = new JobConf(localJobFile); 23 Path workDir = lDirAlloc.getLocalPathForWrite( 24 (getLocalJobDir(jobId.toString()) 25 + Path.SEPARATOR + \u0026#34;work\u0026#34;), fConf); 26 27 System.setProperty(\u0026#34;job.local.dir\u0026#34;, workDir.toString()); 28 localJobConf.set(\u0026#34;job.local.dir\u0026#34;, workDir.toString()); 29 30 //把job的jar文件拷贝到本地文件系统并且解压。 31 String jarFile = localJobConf.getJar(); 32 33 Path jarFilePath = new Path(jarFile); 34 status = systemFS.getFileStatus(jarFilePath); 35 jarFileSize = status.getLen(); 36 37 //保证释放的目录容量有5倍的jar文件大小 38 localJarFile = new Path(lDirAlloc.getLocalPathForWrite( 39 getLocalJobDir(jobId.toString()) 40 + Path.SEPARATOR + \u0026#34;jars\u0026#34;, 41 5 * jarFileSize, fConf), \u0026#34;job.jar\u0026#34;); 42 43 //把jar文件拷贝到本地 44 systemFS.copyToLocalFile(jarFilePath, localJarFile); 45 localJobConf.setJar(localJarFile.toString()); 46 OutputStream out = localFs.create(localJobFile); 47 localJobConf.writeXml(out); 48 49 // also unjar the job.jar files 50 RunJar.unJar(new File(localJarFile.toString()), 51 new File(localJarFile.getParent().toString())); 52 } 53 rjob.keepJobFiles = ((localJobConf.getKeepTaskFilesPattern() != null) || 54 localJobConf.getKeepFailedTaskFiles()); 55 rjob.localized = true; 56 rjob.jobConf = localJobConf; 57 } 58 } 59 launchTaskForJob(tip, new JobConf(rjob.jobConf)); 60} TaskTracker的addTaskToJob方法。只是把job和task的关系加入到runningJobs中。 1private RunningJob addTaskToJob(JobID jobId, 2 TaskInProgress tip) { 3 synchronized (runningJobs) { 4 RunningJob rJob = null; 5 if (!runningJobs.containsKey(jobId)) { 6 rJob = new RunningJob(jobId); 7 rJob.localized = false; 8 rJob.tasks = new HashSet\u0026lt;TaskInProgress\u0026gt;(); 9 runningJobs.put(jobId, rJob); 10 } else { 11 rJob = runningJobs.get(jobId); 12 } 13 synchronized (rJob) { 14 rJob.tasks.add(tip); 15 } 16 runningJobs.notify(); //notify the fetcher thread 17 return rJob; 18 } 19 } TaskTracker的launchTaskForJob方法。调用TaskInprogress的launchTask方法。 1private void launchTaskForJob(TaskInProgress tip, JobConf jobConf) { 2 synchronized (tip) { 3 tip.setJobConf(jobConf); 4 tip.launchTask(); 5 } 6 } TaskIProgress的 launchTask方法。 1public synchronized void launchTask() throws IOException { 2 if (this.taskStatus.getRunState() == TaskStatus.State.UNASSIGNED || 3 this.taskStatus.getRunState() == TaskStatus.State.FAILED_UNCLEAN || 4 this.taskStatus.getRunState() == TaskStatus.State.KILLED_UNCLEAN) { 5 localizeTask(task); 6 if (this.taskStatus.getRunState() == TaskStatus.State.UNASSIGNED) { 7 this.taskStatus.setRunState(TaskStatus.State.RUNNING); 8 } 9 //创建一个Runner来运行。 10 this.runner = task.createRunner(TaskTracker.this, this); 11 this.runner.start(); 12 this.taskStatus.setStartTime(System.currentTimeMillis()); 13 } 9.TaskinProgress的localizeTask方法。把Task相关的文件拷贝到本地。\n1private void localizeTask(Task task) throws IOException{ 2 3 Path localTaskDir = 4 lDirAlloc.getLocalPathForWrite( 5 TaskTracker.getLocalTaskDir(task.getJobID().toString(), 6 task.getTaskID().toString(), task.isTaskCleanupTask()), 7 defaultJobConf ); 8 9 FileSystem localFs = FileSystem.getLocal(fConf); 10 // create symlink for ../work if it already doesnt exist 11 String workDir = lDirAlloc.getLocalPathToRead( 12 TaskTracker.getLocalJobDir(task.getJobID().toString()) 13 + Path.SEPARATOR 14 + \u0026#34;work\u0026#34;, defaultJobConf).toString(); 15 String link = localTaskDir.getParent().toString() 16 + Path.SEPARATOR + \u0026#34;work\u0026#34;; 17 File flink = new File(link); 18 if (!flink.exists()) 19 FileUtil.symLink(workDir, link); 20 21 // 创建task的工作目录 22 Path cwd = lDirAlloc.getLocalPathForWrite( 23 getLocalTaskDir(task.getJobID().toString(), 24 task.getTaskID().toString(), task.isTaskCleanupTask()) 25 + Path.SEPARATOR + MRConstants.WORKDIR, 26 defaultJobConf); 27 28 Path localTaskFile = new Path(localTaskDir, \u0026#34;job.xml\u0026#34;); 29 task.setJobFile(localTaskFile.toString()); 30 localJobConf.set(\u0026#34;mapred.local.dir\u0026#34;, 31 fConf.get(\u0026#34;mapred.local.dir\u0026#34;)); 32 33 localJobConf.set(\u0026#34;mapred.task.id\u0026#34;, task.getTaskID().toString()); 34 } 35 OutputStream out = localFs.create(localTaskFile); 36 localJobConf.writeXml(out); 37 38 task.setConf(localJobConf); 39 } 10.Task是个抽象类，两个子类分别是MapTask和ReduceTask。先关注Map的TaskRunner。\n1public TaskRunner createRunner(TaskTracker tracker, 2 TaskTracker.TaskInProgress tip) { 3 return new MapTaskRunner(tip, tracker, this.conf); 4 } TaskRunner线程的Run方法，有420行代码！主要作用是根据配置信息，构造java命令，启动一个java进程。 拼接一个java指令，启动一个单独的java进程来执行每一个map或者reduce任务。这个java命令的class是Child。即这个java进程最终调用的是Child类的main函数。\n12.JvmManager的 launchJvm方法。在TaskRunner的run方法，是构造一个java命令的参数，调用JvmManager的launchJvm方法执行。\n1public void launchJvm(TaskRunner t, JvmEnv env) { 2 if (t.getTask().isMapTask()) { 3 mapJvmManager.reapJvm(t, env); 4 } else { 5 reduceJvmManager.reapJvm(t, env); 6 } 7 } JvmManagerForType的reapJvm方法 1private synchronized void reapJvm( 2 TaskRunner t, JvmEnv env) { 3 if (t.getTaskInProgress().wasKilled()) { 4 //如果task被杀死则直接返回 5 return; 6 } 7 boolean spawnNewJvm = false; 8 JobID jobId = t.getTask().getJobID(); 9 //检查是否有空闲的槽，如果小于最大jvm数，则重新开启一个jvm，不让你从现有job的空闲jvm中选择一个，或者杀死另外job的空闲jvm 10 int numJvmsSpawned = jvmIdToRunner.size(); 11 JvmRunner runnerToKill = null; 12 if (numJvmsSpawned \u0026gt;= maxJvms) { 13 //go through the list of JVMs for all jobs. 14 Iterator\u0026lt;Map.Entry\u0026lt;JVMId, JvmRunner\u0026gt;\u0026gt; jvmIter = 15 jvmIdToRunner.entrySet().iterator(); 16 17 while (jvmIter.hasNext()) { 18 JvmRunner jvmRunner = jvmIter.next().getValue(); 19 JobID jId = jvmRunner.jvmId.getJobId(); 20 //look for a free JVM for this job; if one exists then just break 21 if (jId.equals(jobId) \u0026amp;\u0026amp; !jvmRunner.isBusy() \u0026amp;\u0026amp; !jvmRunner.ranAll()){ 22 setRunningTaskForJvm(jvmRunner.jvmId, t); //reserve the JVM 23 LOG.info(\u0026#34;No new JVM spawned for jobId/taskid: \u0026#34; + 24 jobId+\u0026#34;/\u0026#34;+t.getTask().getTaskID() + 25 \u0026#34;. Attempting to reuse: \u0026#34; + jvmRunner.jvmId); 26 return; 27 } 28 29 if ((jId.equals(jobId) \u0026amp;\u0026amp; jvmRunner.ranAll()) || 30 (!jId.equals(jobId) \u0026amp;\u0026amp; !jvmRunner.isBusy())) { 31 runnerToKill = jvmRunner; 32 spawnNewJvm = true; 33 } 34 } 35 } else { 36 spawnNewJvm = true; 37 } 38 39 if (spawnNewJvm) { 40 if (runnerToKill != null) { 41 LOG.info(\u0026#34;Killing JVM: \u0026#34; + runnerToKill.jvmId); 42 runnerToKill.kill(); 43 } 44 spawnNewJvm(jobId, env, t); 45 return; 46 } 47} JvmManagerForType的spawnNewJvm方法。重新启动一个jvm。 1private void spawnNewJvm(JobID jobId, JvmEnv env, 2 TaskRunner t) { 3 JvmRunner jvmRunner = new JvmRunner(env,jobId); 4 jvmIdToRunner.put(jvmRunner.jvmId, jvmRunner); 5 jvmRunner.setDaemon(true); 6 jvmRunner.setName(\u0026#34;JVM Runner \u0026#34; + jvmRunner.jvmId + \u0026#34; spawned.\u0026#34;); 7 setRunningTaskForJvm(jvmRunner.jvmId, t); 8 LOG.info(jvmRunner.getName()); 9 jvmRunner.start(); 10 } JvmRunner线程的run方法。 1public void run() { 2 runChild(env); 3 } JvmRunner线程的runChild方法。其中掉ShellCommandExecutor的execute方法。 ShellCommandExecutor封装了shell执行。即把前面步骤构造的JvmEnv类型的执行信息分装成一个字符串列表，使用该列表构造一 个ShellCommandExecutor来执行命令。 1public void runChild(JvmEnv env) { 2env.vargs.add(Integer.toString(jvmId.getId())); 3 List\u0026lt;String\u0026gt; wrappedCommand = 4 TaskLog.captureOutAndError(env.setup, env.vargs, env.stdout, env.stderr, 5 env.logSize, env.pidFile); 6 shexec = new ShellCommandExecutor(wrappedCommand.toArray(new String[0]), 7 env.workDir, env.env); 8 shexec.execute(); 9} 完.\n","link":"https://idouba.com/hadoop_mapreduce_tasktracker_launch_task/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】hadoop作业提交之TaskTracker 启动task"},{"body":"一、概要描述\n在上一篇博文中主要描述了JobTracker和其几个服务（或功能）模块的接收到提交的job后的一些处理。其中很重要的一部分就作业的初始化。因为代码片段图的表达问题，本应该在上篇描述的内容，分开在本篇描述。\n二、 流程描述\n代码也接上文的最后一个方法EagerTaskInitializationListener的 jobAdded方法把JobInProgress类型的job放到List类型的 jobInitQueue中，有个单独的线程会对新加入的每个job进行初始化，其初始化调用的方法就是JobInProgress的方法 initTasks。\n在JobInProgress的方法initTasks方法中，会根据传入的作业分片创建对应数量的TaskInProgress类型的maptask，同时会创建TaskInProgress类型的指定数量的reducetask。\nTaskInProgress的初始化是由其构造函数和构造函数中调用的init方法完成的。\n三、代码详细\n1. EagerTaskInitializationListener的内部InitJob线程的run方法。调用JobInProgress的初始化方法。\n1static class InitJob implements Runnable { 2 private JobInProgress job; 3 public InitJob(JobInProgress job) { 4 this.job = job; 5 } 6public void run() 7 { 8 job.initTasks(); 9 } 10 } 2. JobInProgress 类的initTasks方法。\n主要流程：\n1）根据读入的split确定map的数量，每个split一个map\n2）如果Task数大于该jobTracker支持的最大task数，则抛出异常。\n3）根据split的数量初始化maps\n4）如果没有split，表示job已经成功结束。\n根据指定的reduce数量numReduceTasks创建reduce task 6）计算并且最少剩下多少map task ，才可以开始Reduce task。默认是总的map task的5%，即大部分Map task完成后，就可以开始reduce task了。\n//1） 根据读入的split确定map的数量，每个split一个map\n1String jobFile = profile.getJobFile(); 2 Path sysDir = new Path(this.jobtracker.getSystemDir()); 3 FileSystem fs = sysDir.getFileSystem(conf); 4 DataInputStream splitFile = 5 fs.open(new Path(conf.get(\u0026#34;mapred.job.split.file\u0026#34;))); 6 JobClient.RawSplit[] splits; 7 splits = JobClient.readSplitFile(splitFile); 8 numMapTasks = splits.length; //2）如果Task数大于该jobTracker支持的最大task数，则抛出异常。\n1int maxTasks = jobtracker.getMaxTasksPerJob(); 2 if (maxTasks \u0026gt; 0 \u0026amp;\u0026amp; numMapTasks + numReduceTasks \u0026gt; maxTasks) { 3 throw new IOException( 4 \u0026#34;The number of tasks for this job \u0026#34; + 5 (numMapTasks + numReduceTasks) + 6 \u0026#34; exceeds the configured limit \u0026#34; + maxTasks); 7 } //3）根据split的数量初始化maps\n1 maps = new TaskInProgress[numMapTasks]; 2 for(int i=0; i \u0026lt; numMapTasks; ++i) { 3 inputLength += splits[i].getDataLength(); 4 maps[i] = new TaskInProgress(jobId, jobFile, 5 splits[i], 6 jobtracker, conf, this, i); 7 } 8 LOG.info(\u0026#34;Input size for job \u0026#34;+ jobId + \u0026#34; = \u0026#34; + inputLength); 9 if (numMapTasks \u0026gt; 0) { 10 LOG.info(\u0026#34;Split info for job:\u0026#34; + jobId + \u0026#34; with \u0026#34; + 11 splits.length + \u0026#34; splits:\u0026#34;); 12 nonRunningMapCache = createCache(splits, maxLevel); 13 } 14 15 this.launchTime = System.currentTimeMillis(); //4）如果没有split，表示job已经成功结束。\n1 if (numMapTasks == 0) { 2 //设定作业的完成时间避免下次还会判断。 3 this.finishTime = this.launchTime; 4 status.setSetupProgress(1.0f); 5 status.setMapProgress(1.0f); 6 status.setReduceProgress(1.0f); 7 status.setCleanupProgress(1.0f); 8 status.setRunState(JobStatus.SUCCEEDED); 9 tasksInited.set(true); 10 JobHistory.JobInfo.logInited(profile.getJobID(), 11 this.launchTime, 0, 0); 12 JobHistory.JobInfo.logFinished(profile.getJobID(), 13 this.finishTime, 0, 0, 0, 0, 14 getCounters()); 15 return; 16 } //5) 根据指定的reduce数量numReduceTasks创建reduce task\n1 this.reduces = new TaskInProgress[numReduceTasks]; 2 for (int i = 0; i \u0026lt; numReduceTasks; i++) { 3 reduces[i] = new TaskInProgress(jobId, jobFile, 4 numMapTasks, i, 5 jobtracker, conf, this); 6 nonRunningReduces.add(reduces[i]); 7 } // 6）计算最少剩下多少map task ，才可以开始Reduce task。默认是总的map task的5%，即大部分Map task完成后，就可以开始reduce task了。 1 completedMapsForReduceSlowstart = 2 (int)Math.ceil( 3 (conf.getFloat(\u0026#34;mapred.reduce.slowstart.completed.maps\u0026#34;, 4 DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART) * 5 numMapTasks)); 6 7 tasksInited.set(true); 8 } 3. TaskInProgress的构造函数\n有构造MapTask的构造函数和构造ReduceTask的构造函数。分别是如下。其主要区别在于构造mapTask是要传入输入分片信息的RawSplit，而Reduce Task则不需要。两个构造函数都要调用init方法，进行其他的初始化。\n1public TaskInProgress(JobID jobid, String jobFile, 2 RawSplit rawSplit, 3 JobTracker jobtracker, JobConf conf, 4 JobInProgress job, int partition) { 5 this.jobFile = jobFile; 6 this.rawSplit = rawSplit; 7 this.jobtracker = jobtracker; 8 this.job = job; 9 this.conf = conf; 10 this.partition = partition; 11 this.maxSkipRecords = SkipBadRecords.getMapperMaxSkipRecords(conf); 12 setMaxTaskAttempts(); 13 init(jobid); 14 } 1public TaskInProgress(JobID jobid, String jobFile, 2 int numMaps, 3 int partition, JobTracker jobtracker, JobConf conf, 4 JobInProgress job) { 5 this.jobFile = jobFile; 6 this.numMaps = numMaps; 7 this.partition = partition; 8 this.jobtracker = jobtracker; 9 this.job = job; 10 this.conf = conf; 11 this.maxSkipRecords = SkipBadRecords.getReducerMaxSkipGroups(conf); 12 setMaxTaskAttempts(); 13 init(jobid); 14 } TaskInProgress的init方法。初始化写map和reduce类型task都需要的初始化信息。 1void init(JobID jobId) { 2 this.startTime = System.currentTimeMillis(); 3 this.id = new TaskID(jobId, isMapTask(), partition); 4 this.skipping = startSkipping(); 5 } 完。\n","link":"https://idouba.com/hadoop_mapreduce_job_init/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】hadoop作业提交之Job初始化"},{"body":"一、概要描述\n在上上一篇博文和上一篇博文中 分别描述了jobTracker和其服务（功能）模块初始化完成后，接收JobClient提交的作业，并进行初始化。本文着重描 述，JobTracker如何选择作业的Task分发到TaskTracker。本文只是描述一个TaskTracker如何从JobTracker获取 Task任务。Task任务在TaskTracker如何执行将在后面博文中描述。\n二、 流程描述\nTaskTracker在run中调用offerService()方法一直死循环的去连接Jobtracker，先Jobtracker发送心跳，发送自身状态，并从Jobtracker获取任务指令来执行。 在JobTracker的heartbeat方法中，对于来自每一个TaskTracker的心跳请求，根据一定的作业调度策略调用assignTasks方法选择一定Task Scheduler调用对应的LoadManager的canAssignMap方法和canAssignReduce方法以决定是否可以给 tasktracker分配任务。默认的是CapBasedLoad，全局平均分配。即根据全局的任务槽数，全局的map任务数的比值得到一个load系 数，该系数乘以待分配任务的tasktracker的最大map任务数，即是该tasktracker能分配得到的任务数。如果太tracker当前运行 的任务数小于可运行的任务数，则任务可以分配新作业给他。（图中缺失了LoadManager的表达，也画不下了，就不加了。在代码详细分析中有） Scheduler的调用TaskSelector的obtainNewMapTask或者obtainNewReduceTask选择Task。 在DefaultTaskSelector中选择Task的方法其实只是封装了JobInProgress的对应方法。 JobTracker根据得到的Task构造TaskTrackerAction设置到到HeartbeatResponse返回给TaskTracker。 TaskTracker中将来自JobTracker的任务加入到TaskQueue中等待执行。 三、代码详细\n1. TaskTracker的入口函数main\n1JobConf conf=new JobConf(); 2 // enable the server to track time spent waiting on locks 3 ReflectionUtils.setContentionTracing 4 (conf.getBoolean(\u0026#34;tasktracker.contention.tracking\u0026#34;, false)); 5 new TaskTracker(conf).run(); TaskTracker的构造函数 1maxCurrentMapTasks = conf.getInt( 2 \u0026#34;mapred.tasktracker.map.tasks.maximum\u0026#34;, 2); 3maxCurrentReduceTasks = conf.getInt( 4 \u0026#34;mapred.tasktracker.reduce.tasks.maximum\u0026#34;, 2); 5this.jobTrackAddr = JobTracker.getAddress(conf); 6 7//启动httpserver 展示tasktracker状态。 8this.server = new HttpServer(\u0026#34;task\u0026#34;, httpBindAddress, httpPort, 9 httpPort == 0, conf); 10server.start(); 11this.httpPort = server.getPort(); 12//初始化方法 13initialize(); TaskTracker的initialize方法，完成TaskTracker的初始化工作。 主要流程\n1)检查可以创建本地文件夹\n2)清理或者初始化需要用到的实例集合变量\n3)初始化RPC服务器，接受task的请求。\n4)清除临时文件\n5)jobtracker的代理，负责处理和jobtracker的交互，通过RPC方式。\n6)一个线程，获取map完成事件。\n7)初始化内存管理\n8)分别启动map和reduce的tasklauncher\n1synchronized void initialize() 2 { 3 //检查可以创建本地文件夹 4 checkLocalDirs(this.fConf.getLocalDirs()); 5 fConf.deleteLocalFiles(SUBDIR); 6 //清理或者初始化需要用到的实例集合变量 7 this.tasks.clear(); 8 this.runningTasks = new LinkedHashMap\u0026lt;TaskAttemptID, TaskInProgress\u0026gt;(); 9 this.runningJobs = new TreeMap\u0026lt;JobID, RunningJob\u0026gt;(); 10 this.jvmManager = new JvmManager(this); 11 //初始化RPC服务器，接受task的请求。 12 this.taskReportServer = 13 RPC.getServer(this, bindAddress, tmpPort, 2 * max, false, this.fConf); 14 this.taskReportServer.start(); 15 // 清除临时文件 16 DistributedCache.purgeCache(this.fConf); 17 cleanupStorage(); 18 19 //jobtracker的代理，负责处理和jobtracker的交互，通过RPC方式。 20 this.jobClient = (InterTrackerProtocol) 21 RPC.waitForProxy(InterTrackerProtocol.class, 22 InterTrackerProtocol.versionID, 23 jobTrackAddr, this.fConf); 24 25 //一个线程，获取map完成事件。 26 this.mapEventsFetcher = new MapEventsFetcherThread(); 27 mapEventsFetcher.setDaemon(true); 28 mapEventsFetcher.setName( 29 \u0026#34;Map-events fetcher for all reduce tasks \u0026#34; + \u0026#34;on \u0026#34; + taskTrackerName); 30 mapEventsFetcher.start(); 31 //初始化内存管理 32 initializeMemoryManagement(); 33 //分别启动map和reduce的tasklauncher 34 mapLauncher = new TaskLauncher(maxCurrentMapTasks); 35 reduceLauncher = new TaskLauncher(maxCurrentReduceTasks); 36 mapLauncher.start(); 37 reduceLauncher.start(); 38 39} TaskTracker 的run方法，在其中一直尝试执行offerService方法 1public void run() 2{ 3 while (running \u0026amp;\u0026amp; !staleState \u0026amp;\u0026amp; !shuttingDown \u0026amp;\u0026amp; !denied) { 4State osState = offerService(); 5} 6} 5. TaskTracker 的offerService方法\n通过RPC调用获得Jobtracker的系统目录。\n发送心跳并且获取Jobtracker的应答\n从JobTrackeer的应答中获取指令\n不同的指令类型执行不同的动作\n对于要launch的task加入到taskQueue中去\n对于清理动作，加入待清理的task集合，会有线程自动清理\n杀死那些过久未反馈进度的task\n当磁盘空间不够时，杀死某些task以腾出空间\n1State offerService() 2 { 3 //通过RPC调用获得Jobtracker的系统目录。 4 String dir = jobClient.getSystemDir(); 5 if (dir == null) { 6 throw new IOException(\u0026#34;Failed to get system directory\u0026#34;); 7 } 8 systemDirectory = new Path(dir); 9 systemFS = systemDirectory.getFileSystem(fConf); 10 } 11 // 发送心跳并且获取Jobtracker的应答 12 HeartbeatResponse heartbeatResponse = transmitHeartBeat(now); 13 //从JobTrackeer的应答中获取指令 14 TaskTrackerAction[] actions = heartbeatResponse.getActions(); 15 //不同的指令类型执行不同的动作 16 if (actions != null){ 17 for(TaskTrackerAction action: actions) { 18 //对于要launch的task加入到taskQueue中去 19 if (action instanceof LaunchTaskAction) {addToTaskQueue((LaunchTaskAction)action); } else if (action instanceof CommitTaskAction) { 20 CommitTaskAction commitAction = (CommitTaskAction)action; 21 if (!commitResponses.contains(commitAction.getTaskID())) {commitResponses.add(commitAction.getTaskID());} 22 //加入待清理的task集合，会有线程自动清理 23 } else {tasksToCleanup.put(action); 24 } 25 } 26 } 27 //杀死那些过久未反馈进度的task 28 markUnresponsiveTasks(); 29 //当磁盘空间不够时，杀死某些task以腾出空间 30 killOverflowingTasks(); 31 } 6. TaskTracker的 transmitHeartBeat方法，定时向JobTracker发心跳。其实是通过RPC的方式向调用Jobtracker的heartbeat方法。\n1private HeartbeatResponse transmitHeartBeat(long now) 2{ 3boolean askForNewTask; 4long localMinSpaceStart; 5synchronized (this) { 6//判断该Tasktracker是否可以接受新的task，依赖于 7 askForNewTask = (status.countMapTasks() \u0026lt; maxCurrentMapTasks || 8 status.countReduceTasks() \u0026lt; maxCurrentReduceTasks) \u0026amp;\u0026amp; 9 acceptNewTasks; 10 localMinSpaceStart = minSpaceStart; 11 } 12if (askForNewTask) { 13 checkLocalDirs(fConf.getLocalDirs()); 14//判断本地空间是否足够，以决定是否接受新的task 15 askForNewTask = enoughFreeSpace(localMinSpaceStart); 16 long freeDiskSpace = getFreeSpace(); 17 long totVmem = getTotalVirtualMemoryOnTT(); 18 long totPmem = getTotalPhysicalMemoryOnTT(); 19 status.getResourceStatus().setAvailableSpace(freeDiskSpace); status.getResourceStatus().setTotalVirtualMemory(totVmem); status.getResourceStatus().setTotalPhysicalMemory(totPmem); status.getResourceStatus().setMapSlotMemorySizeOnTT(mapSlotMemorySizeOnTT); status.getResourceStatus().setReduceSlotMemorySizeOnTT(reduceSlotSizeMemoryOnTT); 20} 21//通过jobclient通过RPC的方式向调用Jobtracker的heartbeat方法。 22HeartbeatResponse heartbeatResponse = jobClient.heartbeat(status, ustStarted,justInited, askForNewTask, heartbeatResponseId); 23} 6. JobTracker的 heartbeat方法。Jobtracker 接受并处理 tasktracker上报的状态，在返回的应答信息中指示tasktracker完成启停job或启动某个task的动作。\n动作类型类 描述 CommitTaskAction 指示Task保存输出，即提交 KillJobAction 杀死属于这个Job的任何一个Task KillTaskAction 杀死指定的Task LaunchTaskAction 开启某个task ReinitTrackerAction 重新初始化taskTracker 主要流程如下：\nacceptTaskTracker(status)方法通过查询inHostsList(status) \u0026amp;\u0026amp; !inExcludedHostsList确认Tasktracker是否在JobTracker的允许列表中。\n当得知TaskTracker重启的标记，从jobtracker的潜在故障名单中移除该tasktracker\n如果initialContact为否表示这次心跳请求不是该taskTracker第一次连接jobtracker，但是如果在jobtracker的 trackerToHeartbeatResponseMap记录中没有之前的响应记录，则说明发生了笔记严重的错误。发送指令给tasktracker 要求其重新初始化。\n如果这是有问题的tasktracker重新接回来的第一个心跳，则通知recoveryManager recoveryManager从的recoveredTrackers列表中移除该tracker以表示该tracker又正常的接回来了。\n如果initialContact != true 并且 revHeartbeatResponse != null表示上一个心跳应答存在，但是tasktracker表示第一次请求，则说上一个initialContact请求的应答丢失了，未传送到 tasktracker。则只是简单的把原来的应答重发一下即可。\n构造应答的Id，是递加的。\n处理心跳，其实就是在jobTracker端更新该tasktracker的状态\n检查tasktracker可以运行新的task\n调用JobTracker配置的taskSceduler来调度task给对应的TaskTracker。从submit到JobTracker的Job列表中选择每个job的每个Task，适合交给该TaskTracker调度的Task\n把分配的Task加入到expireLaunchingTasks，监视并处理其是否超时。\n根据调度器发获得要启动的task构造LaunchTaskAction，通知taskTracker启动这些task。\n把属于该tasktracker的，job已经结束的task加入到killTasksList，发送到tasktracker杀死。即结束那些在tasktracker上已经结束了的作业的task，不管作业是完成还失败。\n判定哪些作业需要清理的，构造Action加入到action列表中。trackerToJobsToCleanup是一个结合，当job gc的时候，调用 finalizeJob进而调用 addJobForCleanup 把作业加入到trackerToJobsToCleanup中\n判定那些task可以提交输出，构造action加入到action列表。\n计算下一次心跳的间隔，设置到应答消息中。\n把上面这些Action设置到response中返回。\n把本次应答保存到trackerToHeartbeatResponseMap中\n1public synchronized HeartbeatResponse heartbeat(TaskTrackerStatus status, 2 boolean restarted, 3 boolean initialContact, 4 boolean acceptNewTasks, 5 short responseId) 6 throws IOException { 7 8 //1) acceptTaskTracker(status)方法通过查询inHostsList(status) \u0026amp;\u0026amp; !inExcludedHostsList确认Tasktracker是否在JobTracker的允许列表中。 9 if (!acceptTaskTracker(status)) { 10 throw new DisallowedTaskTrackerException(status); 11 } 12 String trackerName = status.getTrackerName(); 13 long now = System.currentTimeMillis(); 14 boolean isBlacklisted = false; 15 if (restarted) { 16 //2)当得知TaskTracker重启的标记，从jobtracker的潜在故障名单中移除该tasktracker 17 faultyTrackers.markTrackerHealthy(status.getHost()); 18 } else { 19 isBlacklisted = 20 faultyTrackers.shouldAssignTasksToTracker(status.getHost(), now); 21 } 22 23 HeartbeatResponse prevHeartbeatResponse =trackerToHeartbeatResponseMap.get(trackerName); 24 boolean addRestartInfo = false; 25 26 if (initialContact != true) { 27 //3)如果initialContact为否表示这次心跳请求不是该taskTracker第一次连接jobtracker，但是如果在jobtracker的trackerToHeartbeatResponseMap记录中没有之前的响应记录，则说明发生了笔记严重的错误。发送指令给tasktracker要求其重新初始化。 28 if (prevHeartbeatResponse == null) { 29 // This is the first heartbeat from the old tracker to the newly 30 // started JobTracker 31 //4)如果这是有问题的tasktracker重新接回来的第一个心跳，则通知recoveryManager 32 if (hasRestarted()) { 33 addRestartInfo = true; 34 // recoveryManager从的recoveredTrackers列表中移除该tracker以表示该tracker又正常的接回来了。 35 recoveryManager.unMarkTracker(trackerName); 36 } else { 37 //发送指令让tasktracker重新初始化。 38 return new HeartbeatResponse(responseId, 39 new TaskTrackerAction[] {new ReinitTrackerAction()}); 40 } 41 42 } else { 43 44 //如果initialContact != true 并且 revHeartbeatResponse != null表示上一个心跳应答存在，但是tasktracker表示第一次请求，则说上一个initialContact请求的应答丢失了，未传送到tasktracker。则只是简单的把原来的应答重发一下即可。 45 if (prevHeartbeatResponse.getResponseId() != responseId) { 46 LOG.info(\u0026#34;Ignoring \u0026#39;duplicate\u0026#39; heartbeat from \u0026#39;\u0026#34; + 47 trackerName + \u0026#34;\u0026#39;; resending the previous \u0026#39;lost\u0026#39; response\u0026#34;); 48 return prevHeartbeatResponse; 49 } 50 } 51 } 52 53 // 应答的Id是递加的。 54 short newResponseId = (short)(responseId + 1); 55 status.setLastSeen(now); 56 //处理心跳，其实就是在jobTracker端更新该tasktracker的状态 57 if (!processHeartbeat(status, initialContact)) { 58 if (prevHeartbeatResponse != null) { 59 trackerToHeartbeatResponseMap.remove(trackerName); 60 } 61 return new HeartbeatResponse(newResponseId, 62 new TaskTrackerAction[] {new ReinitTrackerAction()}); 63 } 64 65 // 检查tasktracker可以运行新的task 66 if (recoveryManager.shouldSchedule() \u0026amp;\u0026amp; acceptNewTasks \u0026amp;\u0026amp; !isBlacklisted) { 67 TaskTrackerStatus taskTrackerStatus = getTaskTracker(trackerName); 68 if (taskTrackerStatus == null) { 69 } else { 70 List\u0026lt;Task\u0026gt; tasks = getSetupAndCleanupTasks(taskTrackerStatus); 71 if (tasks == null ) { 72 //2调用JobTracker配置的taskSceduler来调度task给对应的TaskTracker。从submit到JobTracker的Job列表中选择每个job的每个Task，适合交给该TaskTracker调度的Task 73 74 tasks = taskScheduler.assignTasks(taskTrackerStatus);} 75 if (tasks != null) { 76 //把分配的Task加入到expireLaunchingTasks，监视并处理其是否超时。 77 for (Task task : tasks) { 78 Object expireLaunchingTasks; 79 expireLaunchingTasks.addNewTask(task.getTaskID()); 80 actions.add(new LaunchTaskAction(task)); 81 } 82 } 83 } 84 } 85 86 //把属于该tasktracker的，job已经结束的task加入到killTasksList，发送到tasktracker杀死。即结束那些在tasktracker上已经结束了的作业的task，不管作业是完成还失败。 87 List\u0026lt;TaskTrackerAction\u0026gt; killTasksList = getTasksToKill(trackerName); 88 if (killTasksList != null) { 89 actions.addAll(killTasksList); 90 } 91 92 //判定哪些作业需要清理。finalizeJob-\u0026gt; addJobForCleanup 当gc一个job的时候，会调用以上方法把其加入到trackerToJobsToCleanup中 93 List\u0026lt;TaskTrackerAction\u0026gt; killJobsList = getJobsForCleanup(trackerName); 94 if (killJobsList != null) { 95 actions.addAll(killJobsList); 96 97 //判定那些task可以提交输出。 98 List\u0026lt;TaskTrackerAction\u0026gt; commitTasksList = getTasksToSave(status); 99 if (commitTasksList != null) { 100 actions.addAll(commitTasksList); 101 } 102 103 //calculate next heartbeat interval and put in heartbeat response 104 //计算下一次心跳的间隔，设置到应答消息中。 105 int nextInterval = getNextHeartbeatInterval(); 106 response.setHeartbeatInterval(nextInterval); 107 108 //把上面这些Action设置到response中返回。 109 response.setActions(actions.toArray(new TaskTrackerAction[actions.size()])); 110 //把本次应答保存到trackerToHeartbeatResponseMap中 111 trackerToHeartbeatResponseMap.put(trackerName, response); 112 return response; 113 114 } 7.FairScheduler的assignTasks方法。JobTracker就是调用该方法来实现作业的分配的。\n主要流程如下：\n1)分别计算可运行的maptask和reducetask总数\n2)ClusterStatus 维护了当前Map/Reduce作业框架的总体状况。根据ClusterStatus计算得到获得map task的槽数，reduce task的槽数。\n3)调用LoadManager方法决定是否可以为该tasktracker分配任务(默认CapBasedLoadManager方法根据全局的任务槽数， 全局的map任务数的比值得到一个load系数，该系数乘以待分配任务的tasktracker的最大map任务数，即是该tasktracker能分配 得到的任务数。如果太tracker当前运行的任务数小于可运行的任务数，则任务可以分配新作业给他)\n4)从job列表中找出那些job需要运行map或reduce任务，加到List candidates集合中\n5)对candidates集合中的job排序，对每个job调用taskSelector的obtainNewMapTask或者 obtainNewReduceTask方法获取要执行的task。把所以的task放到task集合中返回。从而实现了作业Job的任务Task分配。\n6)并对candidates集合中的每个job，更新Jobinfo信息，即其正在运行的task数，需要运行的task数，以便其后续调度用。\n1 public synchronized List\u0026lt;Task\u0026gt; assignTasks(TaskTrackerStatus tracker) 2 throws IOException { 3 if (!initialized) // Don\u0026#39;t try to assign tasks if we haven\u0026#39;t yet started up 4 return null; 5 6 oolMgr.reloadAllocsIfNecessary(); 7 8 // 分别计算可运行的maptask和reducetask总数 9 int runnableMaps = 0; 10 int runnableReduces = 0; 11 for (JobInProgress job: infos.keySet()) { 12 runnableMaps += runnableTasks(job, TaskType.MAP); 13 runnableReduces += runnableTasks(job, TaskType.REDUCE); 14 } 15 16 // ClusterStatus 维护了当前Map/Reduce作业框架的总体状况。 17 ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus(); 18 //计算得到获得map task的槽数，reduce task的槽数。 19 int totalMapSlots = getTotalSlots(TaskType.MAP, clusterStatus); 20 int totalReduceSlots = getTotalSlots(TaskType.REDUCE, clusterStatus); 21 22 //从job列表中找出那些job需要运行map或reduce任务，加到List\u0026lt;JobInProgress\u0026gt; candidates集合中 23 ArrayList\u0026lt;Task\u0026gt; tasks = new ArrayList\u0026lt;Task\u0026gt;(); 24 TaskType[] types = new TaskType[] {TaskType.MAP, TaskType.REDUCE}; 25 for (TaskType taskType: types) { 26 boolean canAssign = (taskType == TaskType.MAP) 27 //CapBasedLoadManager方法根据全局的任务槽数，全局的map任务数的比值得到一个load系数，该系数乘以待分配任务的tasktracker的最大map任务数，即是该tasktracker能分配得到的任务数。如果太tracker当前运行的任务数小于可运行的任务数，则任务可以分配新作业给他 28 loadMgr.canAssignMap(tracker, runnableMaps, totalMapSlots) : 29 loadMgr.canAssignReduce(tracker, runnableReduces, totalReduceSlots); 30 if (canAssign) { 31 List\u0026lt;JobInProgress\u0026gt; candidates = new ArrayList\u0026lt;JobInProgress\u0026gt;(); 32 for (JobInProgress job: infos.keySet()) { 33 if (job.getStatus().getRunState() == JobStatus.RUNNING \u0026amp;\u0026amp; 34 neededTasks(job, taskType) \u0026gt; 0) { 35 candidates.add(job); 36 } 37 } 38 //对candidates集合中的job排序，对每个job调用taskSelector的obtainNewMapTask或者obtainNewReduceTask方法获取要执行的task。把所以的task放到task集合中返回。 39 // Sort jobs by deficit (for Fair Sharing) or submit time (for FIFO) 40 Comparator\u0026lt;JobInProgress\u0026gt; comparator = useFifo 41 new FifoJobComparator() : new DeficitComparator(taskType); 42 Collections.sort(candidates, comparator); 43 for (JobInProgress job: candidates) { 44 Task task = (taskType == TaskType.MAP 45 taskSelector.obtainNewMapTask(tracker, job) : 46 taskSelector.obtainNewReduceTask(tracker, job)); 47 if (task != null) { 48 //并对candidates集合中的每个job，更新Jobinfo信息，即其正在运行的task数，需要运行的task数。 49 JobInfo info = infos.get(job); 50 if (taskType == TaskType.MAP) { 51 info.runningMaps++; 52 info.neededMaps--; 53 } else { 54 info.runningReduces++; 55 info.neededReduces--; 56 } 57 tasks.add(task); 58 if (!assignMultiple) 59 return tasks; 60 break; 61 } 62 } 63 } 64 } 65 66 // If no tasks were found, return null 67 return tasks.isEmpty() null : tasks; 68 } **8.CapBasedLoadManager的canAssignMap方法和canAssignReduce方法。**一 种简单的算法在FairScheduler中用来决定是否可以给某个tasktracker分配maptask或者reducetask。总体思路是对于 某种类型的task，map或者reduce，考虑jobtracker管理的mapreduce集群全部的任务数，和全部的任务槽数，和该 tasktracker上面当前的任务数，以决定是否给他分配任务。如对于maptask，根据全局的任务槽数，全局的map任务数的比值得到一个 load系数，该系数乘以待分配任务的tasktracker的最大map任务数，即是该tasktracker能分配得到的任务数。如果太 tracker当前运行的任务数小于可运行的任务数，则任务可以分配新作业给他。reducetask同理。即尽量做到全局平均。\n1int getCap(int totalRunnableTasks, int localMaxTasks, int totalSlots) { 2 double load = ((double)totalRunnableTasks) / totalSlots; 3 return (int) Math.ceil(localMaxTasks * Math.min(1.0, load)); 4 } 5 6 @Override 7 public boolean canAssignMap(TaskTrackerStatus tracker, 8 int totalRunnableMaps, int totalMapSlots) { 9 return tracker.countMapTasks() \u0026lt; getCap(totalRunnableMaps, 10 tracker.getMaxMapTasks(), totalMapSlots); 11 } 12 13 @Override 14 public boolean canAssignReduce(TaskTrackerStatus tracker, 15 int totalRunnableReduces, int totalReduceSlots) { 16 return tracker.countReduceTasks() \u0026lt; getCap(totalRunnableReduces, 17 tracker.getMaxReduceTasks(), totalReduceSlots); 18 } 9.DefaultTaskSelector继承自TaskSelector，其两个方法其实只是对jobInprogress得封装，没有做什么特别的事情。\n1@Override 2 public Task obtainNewMapTask(TaskTrackerStatus taskTracker, JobInProgress job) 3 throws IOException { 4 ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus(); 5 int numTaskTrackers = clusterStatus.getTaskTrackers(); 6 return job.obtainNewMapTask(taskTracker, numTaskTrackers, 7 taskTrackerManager.getNumberOfUniqueHosts()); 8 } 9 10 @Override 11 public Task obtainNewReduceTask(TaskTrackerStatus taskTracker, JobInProgress job) 12 throws IOException { 13 ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus(); 14 int numTaskTrackers = clusterStatus.getTaskTrackers(); 15 return job.obtainNewReduceTask(taskTracker, numTaskTrackers, 16 taskTrackerManager.getNumberOfUniqueHosts()); 17 } 10. JobInProgress的obtainNewMapTask方法。其实主要逻辑是在findNewMapTask方法中实现。\n1public synchronized Task obtainNewMapTask(TaskTrackerStatus tts, 2 int clusterSize, 3 int numUniqueHosts 4 ) throws IOException { 5 6 int target = findNewMapTask(tts, clusterSize, numUniqueHosts, anyCacheLevel, 7 status.mapProgress()); 8 9 Task result = maps[target].getTaskToRun(tts.getTrackerName()); 10 if (result != null) { 11 addRunningTaskToTIP(maps[target], result.getTaskID(), tts, true); 12 } 13 14 return result; 15 } 11 JobInProgress的findNewMapTask方法。\n根据待派发Task的TaskTracker根据集群中的TaskTracker数量（clusterSize），运行TraskTracker的服务器数（numUniqueHosts），该Job中map task的平均进度（avgProgress），可以调度map的最大水平（距离其实），选择一个task执行。考虑到map的本地化。\n1private synchronized int findNewMapTask(final TaskTrackerStatus tts, 2 final int clusterSize, 3 final int numUniqueHosts, 4 final int maxCacheLevel, 5 final double avgProgress) { 6 String taskTracker = tts.getTrackerName(); 7 TaskInProgress tip = null; 8 9 //1）更新TaskTracker总数。 10 this.clusterSize = clusterSize; 11 12 //2）如果这个TraskTracker上面之前有很多map都会失败，则返回标记，不分配给他。 13 if (!shouldRunOnTaskTracker(taskTracker)) { 14 return -1; 15 16 //3） 检查该TaskTracker有足够的资源运行。估算output的方法有点意思，根据（job现有的map数+当前job的map数）*已完成map数*2*已完成的map的输出size/已经完成map的输入size，即根据完成估算总数。 17 long outSize = resourceEstimator.getEstimatedMapOutputSize(); 18 long availSpace = tts.getResourceStatus().getAvailableSpace(); 19 if(availSpace \u0026lt; outSize) { 20 LOG.warn(\u0026#34;No room for map task. Node \u0026#34; + tts.getHost() + 21 \u0026#34; has \u0026#34; + availSpace + 22 \u0026#34; bytes free; but we expect map to take \u0026#34; + outSize); 23 return -1; 24 } 25 26 // For scheduling a map task, we have two caches and a list (optional) 27 // I) one for non-running task 28 // II) one for running task (this is for handling speculation) 29 // III) a list of TIPs that have empty locations (e.g., dummy splits), 30 // the list is empty if all TIPs have associated locations 31 32 // First a look up is done on the non-running cache and on a miss, a look 33 // up is done on the running cache. The order for lookup within the cache: 34 // 1. from local node to root [bottom up] 35 // 2. breadth wise for all the parent nodes at max level 36 37 // We fall to linear scan of the list (III above) if we have misses in the 38 // above caches 39 40 //4）获得jobTracker所在的Node 41 Node node = jobtracker.getNode(tts.getHost()); 42 43 // I) Non-running TIP : 44 //5） 从未运行的作业集合中选择一个nonRunningMapCache 加入到运行集合runningMapCache中。加入时根据待添加的Task的split的位置信息，在runningMapCache中保存Node和Task集合的对应关系。 45 46 // 1. check from local node to the root [bottom up cache lookup] 47 // i.e if the cache is available and the host has been resolved 48 // (node!=null) 49 if (node != null) { 50 Node key = node; 51 int level = 0; 52 // maxCacheLevel might be greater than this.maxLevel if findNewMapTask is 53 // called to schedule any task (local, rack-local, off-switch or speculative) 54 // tasks or it might be NON_LOCAL_CACHE_LEVEL (i.e. -1) if findNewMapTask is 55 // (i.e. -1) if findNewMapTask is to only schedule off-switch/speculative 56 // tasks 57 //从taskTracker本地开始由近至远查找要加入的Task 到runningMapCache中。 58 int maxLevelToSchedule = Math.min(maxCacheLevel, maxLevel); 59 for (level = 0;level \u0026lt; maxLevelToSchedule; ++level) { 60 List \u0026lt;TaskInProgress\u0026gt; cacheForLevel = nonRunningMapCache.get(key); 61 if (cacheForLevel != null) { 62 tip = findTaskFromList(cacheForLevel, tts, 63 numUniqueHosts,level == 0); 64 if (tip != null) { 65 // 把该map任务加入到runningMapCache 66 scheduleMap(tip); 67 return tip.getIdWithinJob(); 68 } 69 } 70 key = key.getParent(); 71 } 72 73 // Check if we need to only schedule a local task (node-local/rack-local) 74 if (level == maxCacheLevel) { 75 return -1; 76 } 77 } 78 79 //2. Search breadth-wise across parents at max level for non-running 80 // TIP if 81 // - cache exists and there is a cache miss 82 // - node information for the tracker is missing (tracker\u0026#39;s topology 83 // info not obtained yet) 84 85 // collection of node at max level in the cache structure 86 Collection\u0026lt;Node\u0026gt; nodesAtMaxLevel = jobtracker.getNodesAtMaxLevel(); 87 88 // get the node parent at max level 89 Node nodeParentAtMaxLevel = 90 (node == null) null : JobTracker.getParentNode(node, maxLevel - 1); 91 92 for (Node parent : nodesAtMaxLevel) { 93 94 // skip the parent that has already been scanned 95 if (parent == nodeParentAtMaxLevel) { 96 continue; 97 } 98 99 List\u0026lt;TaskInProgress\u0026gt; cache = nonRunningMapCache.get(parent); 100 if (cache != null) { 101 tip = findTaskFromList(cache, tts, numUniqueHosts, false); 102 if (tip != null) { 103 // Add to the running cache 104 scheduleMap(tip); 105 106 // remove the cache if empty 107 if (cache.size() == 0) { 108 nonRunningMapCache.remove(parent); 109 } 110 LOG.info(\u0026#34;Choosing a non-local task \u0026#34; + tip.getTIPId()); 111 return tip.getIdWithinJob(); 112 } 113 } 114 } 115 116 //搜索非本地Map 117 tip = findTaskFromList(nonLocalMaps, tts, numUniqueHosts, false); 118 if (tip != null) { 119 // Add to the running list 120 scheduleMap(tip); 121 122 LOG.info(\u0026#34;Choosing a non-local task \u0026#34; + tip.getTIPId()); 123 return tip.getIdWithinJob(); 124 } 125 126 // 127 // II) Running TIP : 128 // 129 130 if (hasSpeculativeMaps) { 131 long currentTime = System.currentTimeMillis(); 132 133 // 1. Check bottom up for speculative tasks from the running cache 134 if (node != null) { 135 Node key = node; 136 for (int level = 0; level \u0026lt; maxLevel; ++level) { 137 Set\u0026lt;TaskInProgress\u0026gt; cacheForLevel = runningMapCache.get(key); 138 if (cacheForLevel != null) { 139 tip = findSpeculativeTask(cacheForLevel, tts, 140 avgProgress, currentTime, level == 0); 141 if (tip != null) { 142 if (cacheForLevel.size() == 0) { 143 runningMapCache.remove(key); 144 } 145 return tip.getIdWithinJob(); 146 } 147 } 148 key = key.getParent(); 149 } 150 } 151 152 // 2. Check breadth-wise for speculative tasks 153 154 for (Node parent : nodesAtMaxLevel) { 155 // ignore the parent which is already scanned 156 if (parent == nodeParentAtMaxLevel) { 157 continue; 158 } 159 160 Set\u0026lt;TaskInProgress\u0026gt; cache = runningMapCache.get(parent); 161 if (cache != null) { 162 tip = findSpeculativeTask(cache, tts, avgProgress, 163 currentTime, false); 164 if (tip != null) { 165 // remove empty cache entries 166 if (cache.size() == 0) { 167 runningMapCache.remove(parent); 168 } 169 LOG.info(\u0026#34;Choosing a non-local task \u0026#34; + tip.getTIPId() 170 + \u0026#34; for speculation\u0026#34;); 171 return tip.getIdWithinJob(); 172 } 173 } 174 } 175 176 // 3. Check non-local tips for speculation 177 tip = findSpeculativeTask(nonLocalRunningMaps, tts, avgProgress, 178 currentTime, false); 179 if (tip != null) { 180 LOG.info(\u0026#34;Choosing a non-local task \u0026#34; + tip.getTIPId() 181 + \u0026#34; for speculation\u0026#34;); 182 return tip.getIdWithinJob(); 183 } 184 } 185 186 return -1; 187 188 } 12 JobInProgress的obtainNewReduceTask方法返回一个ReduceTask，实际调用的是findNewReduceTask方法。\n1public synchronized Task obtainNewReduceTask(TaskTrackerStatus tts, 2 int clusterSize, 3 int numUniqueHosts 4 ) throws IOException { 5 //判定有足够的map已经完成。， 6 if (!scheduleReduces()) { 7 return null; 8 } 9 10 int target = findNewReduceTask(tts, clusterSize, numUniqueHosts, 11 status.reduceProgress()); 12 Task result = reduces[target].getTaskToRun(tts.getTrackerName()); 13 if (result != null) { 14 addRunningTaskToTIP(reduces[target], result.getTaskID(), tts, true); 15 } 16 17 return result; 18 } 13 JobInProgress的findNewReduceTask方法，为指定的TaskTracker选择Reduce task。不用考虑本地化。\n1private synchronized int findNewReduceTask(TaskTrackerStatus tts, 2 int clusterSize, 3 int numUniqueHosts, 4 double avgProgress) { 5 String taskTracker = tts.getTrackerName(); 6 TaskInProgress tip = null; 7 8 // Update the last-known clusterSize 9 this.clusterSize = clusterSize; 10 // 该taskTracker可用性符合要求 11 if (!shouldRunOnTaskTracker(taskTracker)) { 12 return -1; 13 } 14 15//估算Reduce的输入，根据map的总输出来和reduce的个数来计算。 16 long outSize = resourceEstimator.getEstimatedReduceInputSize(); 17 long availSpace = tts.getResourceStatus().getAvailableSpace(); 18 if(availSpace \u0026lt; outSize) { 19 LOG.warn(\u0026#34;No room for reduce task. Node \u0026#34; + taskTracker + \u0026#34; has \u0026#34; + 20 availSpace + 21 \u0026#34; bytes free; but we expect reduce input to take \u0026#34; + outSize); 22 23 return -1; //see if a different TIP might work better. 24 } 25 26 // 1. check for a never-executed reduce tip 27 // reducers don\u0026#39;t have a cache and so pass -1 to explicitly call that out 28 tip = findTaskFromList(nonRunningReduces, tts, numUniqueHosts, false); 29 if (tip != null) { 30 scheduleReduce(tip); 31 return tip.getIdWithinJob(); 32 } 33 34 // 2. check for a reduce tip to be speculated 35 if (hasSpeculativeReduces) { 36 tip = findSpeculativeTask(runningReduces, tts, avgProgress, 37 System.currentTimeMillis(), false); 38 if (tip != null) { 39 scheduleReduce(tip); 40 return tip.getIdWithinJob(); 41 } 42 } 43 44 return -1; 45 } 14 TaskTracker 的addToTaskQueue方法。对于要launch的task加入到taskQueue中去，不同类型的Task有不同类型额launcher。\n1private void addToTaskQueue(LaunchTaskAction action) { 2 if (action.getTask().isMapTask()) { 3 mapLauncher.addToTaskQueue(action); 4 } else { 5 reduceLauncher.addToTaskQueue(action); 6 } 7} 完。\n","link":"https://idouba.com/hadoop_mapreduce_tasktracker_retrieve_task/","section":"posts","tags":["hadoop","java","mapreduce","source"],"title":"【hadoop代码笔记】hadoop作业提交之TaskTracker获取Task"},{"body":"","link":"https://idouba.com/tags/source/","section":"tags","tags":null,"title":"source"},{"body":"一、概要描述\n本文重点描述在JobTracker一端接收作业、调度作业等几个模块的初始化工作。想过模块的介绍会在其他文章中比较详细的描述。受理作业提交在下一篇文章中会进行描述。\n为了表达的尽可能清晰一点只是摘录出影响逻辑流转的主要代码。重点强调直接的协作调用，每个内部完成的逻辑（一直可以更细的说明、有些细节可能自己也理解并不深刻:-(）在后续会描述。\n主要包括JobTracker、TaskScheduler（此处以FairScheduler为例）、JobInProgressListener（以用的较多的EagerTaskInitializationListener为例）、TaskSelector(以最简单的DefaultTaskSelector为例)等。\n二、 流程描述\nJobTracker 的main函数中调用其startTracker方法。 在mai函数中调用offerService，启动各个子服务项（大部分形态都是线程，有些是其他的初始化，如taskScheduler） 在startTracker中调用其构造函数，在构造函数中对其中重要的属性根据配置进行初始化。()个人感觉再构造中设置scheduler，在statTracker调用构造的下一句有给Scheduler传JobTracker的引用，有点不自然) 在offerService()中启动taskSchedulerexpireTrackersThread retireJobsThread expireLaunchingTaskThread completedJobsStoreThread interTrackerServer等几个线程来共同完成服务。同时调用TaskScheduler的start方法进行初始化。 在FairScheduler调度器的start方法中调用EagerTaskInitializationListenerr的start方法来初始化EagerTaskInitializationListener 在FairScheduler调度器的start方法中调用DefaultTaskSelector的start方法来初始化DefaultTaskSelector，因为该类实现的TaskSelector太简单，start方法里也没有做任何事情。 JobTracker等相关功能模块初始化\n三、 代码详述\n1. JobTracker 的入口main函数。主要是实例化一个JobTracker类，然后调用offerService方法做事情。\n在Jobtracker的main函数中去掉记日志和异常捕获外关键代码就一下两行。\n1 JobTracker tracker = startTracker(new JobConf()); 2 tracker.offerService(); 2. JobTracker 的startTracker方法。 调用JobTracker的构造函数，完成初始化工作。\n1JobTracker result = null; 2 while (true) { 3 try { 4 result = new JobTracker(conf); 5 result.taskScheduler.setTaskTrackerManager(result); 6 Thread.sleep(1000); 7 } 8 9 JobEndNotifier.startNotifier(); 10 return result; 3. JobTracker的构造方法JobTracker(JobConf conf)。是一个有两三屏的长的方法。值得关注下，当然jobtracker服务运维的有些部分会适当忽略，着重看处理作业的部分。(其实这样的说法也 不太对，Jobtracker的主要甚至是唯一的作用就是处理提交的job)\n主要的工作有：\n1)创建一个初始化一个队列管理器，一个HadoopMapReduce作业可以配置一个或者多个Queue，依赖于其使用的作业调度器Scheduler 2)根据配置创建一个调度器 3)创建一个RPC Server,其中handlerCount是RPC server服务端处理请求的Handler线程的数量，默认是10。详细机制参照RPC机制描述。 4)创建一个创建一个HttpServer，用于JobTracker的信息发布。 5)创建一个RecoveryManager，用于JobTracker重启时候恢复 6)创建一个CompletedJobStatusStore，用户持久化作业状态。\n1//初始化一个队列管理器，一个HadoopMapReduce作业可以配置一个或者多个Queue，依赖于其使用的作业调度器Scheduler 2queueManager = new QueueManager(this.conf); 3// 根据 conf的配置创建一个调度器 4Class\u0026lt; extends TaskScheduler\u0026gt; schedulerClass = conf.getClass(\u0026#34;mapred.jobtracker.taskScheduler\u0026#34;,JobQueueTaskScheduler.class, TaskScheduler.class); 5taskScheduler = (TaskScheduler) ReflectionUtils.newInstance(schedulerClass, conf); 6//创建一个RPC Server，作用见上节详细描述 7InetSocketAddress addr = getAddress(conf); 8this.localMachine = addr.getHostName(); 9this.port = addr.getPort(); 10int handlerCount = conf.getInt(\u0026#34;mapred.job.tracker.handler.count\u0026#34;, 10); 11//其中handlerCount是RPC server服务端处理请求的Handler线程的数量，默认是10 12this.interTrackerServer = RPC.getServer(this, addr.getHostName(), addr.getPort(), handlerCount, false, conf); 13//创建一个HttpServer 14infoServer = new HttpServer(\u0026#34;job\u0026#34;, infoBindAddress, tmpInfoPort, tmpInfoPort == 0, conf); 15infoServer.addServlet(\u0026#34;reducegraph\u0026#34;, \u0026#34;/taskgraph\u0026#34;, TaskGraphServlet.class); 16infoServer.start(); 17//用于重启时候恢复 18recoveryManager = new RecoveryManager(); 19//初始化 the job status store，用户持久化作业状态 20 completedJobStatusStore = new CompletedJobStatusStore(conf,fs); 4. Jobtracker的offerService方法。把她相关的子服务（大部分是线程）启动，其他的相关的初始化。 1）启动任务调度器。 2）在每次启动时候，恢复需要恢复的作业 3）启动expireTrackersThread，其实是启动ExpireTrackers类型的一个线程。 this.expireTrackersThread = new Thread(this.expireTrackers, expireTrackers”); 4）启动retireJobsThread ，其实是启动RetireJobs类型的一个线程.删除完成的过期job 5）启动expireLaunchingTaskThread，查分配的task未返回报告的使之为过期。 6）启动CompletedJobStatusStore，负责job信息的持久化或者读出。 7）启动RPC 服务，接收客户端端的RPC请求\n1//启动任务调度器。 2taskScheduler.start(); 3//恢复需要恢复的作业,不深入进行看了。 4recoveryManager.recover(); 5//启动expireTrackersThread，其实是启动ExpireTrackers类型的一个线程。this.expireTrackersThread = new Thread(this.expireTrackers, expireTrackers\u0026#34;); 6this.expireTrackersThread.start(); 7//启动retireJobsThread ，其实是启动RetireJobs类型的一个线程.删除完成的过期job 8 9this.retireJobsThread = new Thread(this.retireJobs, \u0026#34;retireJobs\u0026#34;); 10this.retireJobsThread.start(); 11//检查分配的task未返回报告的使之为过期。 12expireLaunchingTaskThread.start(); 13//启动CompletedJobStatusStore，负责job信息的持久化或者读出。 14completedJobsStoreThread.start(); 15//启动RPC 服务，接收客户端端的RPC请求 16this.interTrackerServer.start(); 5. TaskScheduler（FairScheduler）的Start方法。Scheduler相关的初始化。\n1)调用用EagerTaskInitializationListener的Start方法，启动一个守护线程来初始化其jobInitQueue中的Job（JobInprogress） 2)向taskTrackerManager（其实就是JobTracker）注册JobInProgressListener，响应Job相关的动作，如典型的jobAdded方法。eagerInitListener响 应JobAdded方法，是把加入的job放到自己的管理的队列中，启动线程去初始化；jobListener是该类的内部类，其jobAdded方法是 构造job的调度信息JobInfo，并把每个job和对应的调度信息加入到实例变量Map\u0026lt;JobInProgress, JobInfo\u0026gt; infos中，供调度时使用。 3)初始化PoolManager 4)根据配置，初始化一个 LoadManager，在scheduler中决定某个tasktracker是否可以得到一个新的Task，不同的LoadManager有不同的算 法。一般默认的是CapBasedLoadManager，根据每个Node的最大可接受数量平均分配。 5)构造一个TaskSelector 6) 一个线程调用FairScheduler的update方法来以一定间隔来更新作业权重、运行待运行的task数等状态信息以便FairScheduler调度用。 7) 注册到infoserver中，可以通过web查看其信息。\n1// 1)调用用EagerTaskInitializationListener的Start方法，启动一个守护线程来初始化其jobInitQueue中的Job（JobInprogress） 2 Configuration conf = getConf(); 3 this.eagerInitListener = new EagerTaskInitializationListener(conf); 4 5 eagerInitListener.start(); 6 // 2)向taskTrackerManager（其实就是JobTracker）注册JobInProgressListener，响应Job相关的动作，如典型的jobAdded方法。eagerInitListener响应JobAdded方法，是把加入的job放到自己的管理的队列中，启动线程去初始化；jobListener是该类的内部类，其jobAdded方法是构造job的调度信息JobInfo，并把每个job和对应的调度信息加入到实例变量Map\u0026lt;JobInProgress, 7 // JobInfo\u0026gt; infos中，供调度时使用。 8 taskTrackerManager.addJobInProgressListener(eagerInitListener); 9 taskTrackerManager.addJobInProgressListener(jobListener); 10 11 // 3)初始化PoolManager 12 poolMgr = new PoolManager(conf); 13 // 4)根据配置，初始化一个LoadManager，在scheduler中决定某个tasktracker是否可以得到一个新的Task，不同的LoadManager有不同的算法。一般默认的是CapBasedLoadManager，根据每个Node的最大可接受数量平均分配。 14 loadMgr = (LoadManager) ReflectionUtils.newInstance(conf.getClass( 15 \u0026#34;mapred.fairscheduler.loadmanager\u0026#34;, CapBasedLoadManager.class, 16 LoadManager.class), conf); 17 loadMgr.setTaskTrackerManager(taskTrackerManager); 18 loadMgr.start(); 19 20 // 5)构造一个TaskSelector 21 taskSelector = (TaskSelector) ReflectionUtils.newInstance(conf 22 .getClass(\u0026#34;mapred.fairscheduler.taskselector\u0026#34;, 23 DefaultTaskSelector.class, TaskSelector.class), conf); 24 taskSelector.setTaskTrackerManager(taskTrackerManager); 25 taskSelector.start(); 26 Class\u0026lt;\u0026gt; weightAdjClass = conf.getClass( 27 \u0026#34;mapred.fairscheduler.weightadjuster\u0026#34;, null); 28 if (weightAdjClass != null) { 29 weightAdjuster = (WeightAdjuster) ReflectionUtils.newInstance( 30 weightAdjClass, conf); 31 } 32 assignMultiple = conf.getBoolean(\u0026#34;mapred.fairscheduler.assignmultiple\u0026#34;, 33 false); 34 sizeBasedWeight = conf.getBoolean( 35 \u0026#34;mapred.fairscheduler.sizebasedweight\u0026#34;, false); 36 initialized = true; 37 running = true; 38 lastUpdateTime = clock.getTime(); 39 // 6) 一个线程调用FairScheduler的update方法来以一定间隔来更新作业权重、运行待运行的task数等状态信息以便FairScheduler调度用。 40 if (runBackgroundUpdates) 41 new UpdateThread().start(); 42 // 7) 注册到infoserver中，可以通过web查看其信息。 43 if (taskTrackerManager instanceof JobTracker) { 44 JobTracker jobTracker = (JobTracker) taskTrackerManager; 45 HttpServer infoServer = jobTracker.infoServer; 46 infoServer.setAttribute(\u0026#34;scheduler\u0026#34;, this); 47 infoServer.addServlet(\u0026#34;scheduler\u0026#34;, \u0026#34;/scheduler\u0026#34;, 48 FairSchedulerServlet.class); 49 } 6. JobInProgressListener(EagerTaskInitializationListener)的start方法。初始化一个线程，检查器jobqueue上的job进行初始化。\n1this.jobInitManagerThread = new Thread(jobInitManager, \u0026#34;jobInitManager\u0026#34;); 2 jobInitManagerThread.setDaemon(true); 3 this.jobInitManagerThread.start(); 7. TaskSelector(DefaultTaskSelector)的start方法。在父类TaskSelector和子类DefaultTaskSelector都没有做任何事情，因为DefaultTaskSelector的实现的主要业务方法只是简单封装，在该类中没有保存任何状态的信息，也不用其他子服务之类的来完成，因此没有初始化内容。但是其他方式的TaskSelector可能会有，因此父类中定义了个start方法。\n1public void start() throws IOException { 2 // do nothing 3 } 完。\n","link":"https://idouba.com/hadoop_job_submit_service_init/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记hadoop作业提交之JobTracker等相关功能模块初始化"},{"body":"一、概要描述\n仅仅描述向Hadoop提交作业的第一步，即调用Jobclient的submitJob方法，向Hadoop提交作业。\n二、 流程描述\nJobclient使用内置的JobSubmissionProtocol 实例jobSubmitClient 和JobTracker交互，最主要是提交作业、获取作业执行信息等。\n在JobClient中作业提交的主要过程如下：\n1）通过调用JobTracker的getNewJobId()向jobtracker请求一个新的作业ID 2）获取job的jar、输入分片、作业描述等几个路径信息，以jobId命名。 3）其中getSystemDir()是返回jobtracker的系统目录，来放置job相关的文件。包括：mapreduce的jar文件submitJarFile、分片文件submitSplitFile、作业描述文件submitJobFile 4）检查作业的输出说明，如果没有指定输出目录或输出目录以及存在，则作业不提交。参照org.apache.hadoop.mapreduce.lib.output.FileOutputFormat的checkOutputSpecs方法。如果没有指定，则抛出InvalidJobConfException，文件已经存在则抛出FileAlreadyExistsException 5）计算作业的输入分片。通过InputFormat的getSplits(job)方法获得作业的split并将split序列化封装为RawSplit。返回split数目，也即代表有多个分片有多少个map。详细参见InputFormat获取Split的方法。 6）writeNewSplits 方法把输入分片写到JobTracker的job目录下。 7）将运行作业所需的资源（包括作业jar文件，配置文件和计算所得的输入分片）复制到jobtracker的文件系统中一个以作业ID命名的目录下。 8）使用句柄JobSubmissionProtocol通过RPC远程调用的submitJob()方法，向JobTracker提交作业。JobTracker作业放入到内存队列中，由作业调度器进行调度。并初始化作业实例。JobTracker创建job成功后会给JobClient传回一个JobStatus对象 用于记录job的状态信息，如执行时间、Map和Reduce任务完成的比例等。JobClient会根据这个JobStatus对象创建一个 NetworkedJob的RunningJob对象，用于定时从JobTracker获得执行过程的统计数据来监控并打印到用户的控制台。\n三、代码详细\nJobclient ：JobClient是向JobTracker提交作业的接口，可以理解为Hadoop的Mapreduce作业框架向用户开放的作业提交入口。可以提交作业，监视作业状态等\nJobSubmissionProtocol（为什么0.20.1的javadoc中找不到这个接口，虽然0.20.1 0.20.2代码中都是相同的用法，知道2.2.0貌似重命名为被ClientProtocol替换）：JobClient和JobTracker进行通信的一个协议。JobClient实际上是用这个句柄来提交锁业并且监视作业的执行状况。\n这个接口有两个实现：LocalJobRunner(conf)当mapred-site.xml中的mapred.job.tracker值为local是为此对象。表示在单机上执行；如果为一个地址的话则是 JobTracker的对象，表示分布式执 行。\n详细可参照JobClient中 的初始化代码：\n1 /** 2 *如果是非local的就会 连接到指定的JobTracker 3 */ 4 public void init(JobConf conf) throws IOException { 5 String tracker = conf.get(\u0026#34;mapred.job.tracker\u0026#34;, \u0026#34;local\u0026#34;); 6 if (\u0026#34;local\u0026#34;.equals(tracker)) { 7 this.jobSubmitClient = new LocalJobRunner(conf); 8 } else { 9 this.jobSubmitClient = createRPCProxy(JobTracker.getAddress(conf), conf); 10 } 11 } 12 13 /* 14 * RPC不是本次主题重点，可参照后续发表的专题内容 15 */ 16 private JobSubmissionProtocol createRPCProxy(InetSocketAddress addr, 17 Configuration conf) throws IOException { 18 return (JobSubmissionProtocol) RPC.getProxy(JobSubmissionProtocol.class, 19 JobSubmissionProtocol.versionID, addr, getUGI(conf), conf, 20 NetUtils.getSocketFactory(conf, JobSubmissionProtocol.class)); 21 } 通过代码来了解流程，了解如何调用JobClient向Hadoop集群提交作业。\n1 public RunningJob submitJob(JobConf job) throws FileNotFoundException, 2 IOException { 3 try { 4 return submitJobInternal(job); 5 } catch (InterruptedException ie) { 6 throw new IOException(\u0026#34;interrupted\u0026#34;, ie); 7 } catch (ClassNotFoundException cnfe) { 8 throw new IOException(\u0026#34;class not found\u0026#34;, cnfe); 9 } 10 } 实际方法的执行是submitJobInternal方法。着重看下这个方法的内部执行。主要的逻辑部分比较详细的进行了注释\n1public RunningJob submitJobInternal(JobConf job) 2 throws FileNotFoundException, ClassNotFoundException, 3 InterruptedException, IOException { 4 5 // 1）通过调用JobTracker的getNewJobId()向jobtracker请求一个新的作业ID 6 JobID jobId = jobSubmitClient.getNewJobId(); 7 // 2）获取job的jar、输入分片、作业描述等几个路径信息，以jobId命名。 8 // 3）其中getSystemDir()是返回jobtracker的系统目录，来放置job相关的文件。包括：mapreduce的jar文件submitJarFile、分片文件submitSplitFile、作业描述文件submitJobFile 9 10 Path submitJobDir = new Path(getSystemDir(), jobId.toString()); 11 Path submitJarFile = new Path(submitJobDir, \u0026#34;job.jar\u0026#34;); 12 Path submitSplitFile = new Path(submitJobDir, \u0026#34;job.split\u0026#34;); 13 configureCommandLineOptions(job, submitJobDir, submitJarFile); 14 Path submitJobFile = new Path(submitJobDir, \u0026#34;job.xml\u0026#34;); 15 int reduces = job.getNumReduceTasks(); 16 JobContext context = new JobContext(job, jobId); 17 18 // Check the output specification 19 // 4）检查作业的输出说明，如果没有指定输出目录或输出目录以及存在，则作业不提交。参照org.apache.hadoop.mapreduce.lib.output.FileOutputFormat的checkOutputSpecs方法。如果没有指定，则抛出InvalidJobConfException，文件已经存在则抛出FileAlreadyExistsException 20 21 if (reduces == 0 ? job.getUseNewMapper() : job.getUseNewReducer()) { 22 org.apache.hadoop.mapreduce.OutputFormat\u0026lt;?, ?\u0026gt; output = ReflectionUtils 23 .newInstance(context.getOutputFormatClass(), job); 24 output.checkOutputSpecs(context); 25 } else { 26 job.getOutputFormat().checkOutputSpecs(fs, job); 27 } 28 29 // 5）计算作业的输入分片。详细参见FormatInputFormat获取Split的方法。 30 // 6）writeNewSplits 方法把输入分片写到JobTracker的job目录下，名称是submitSplitFile 31 // job.split名称。 32 // 7）将运行作业所需的资源（包括作业jar文件，配置文件和计算所得的输入分片）复制到jobtracker的文件系统中一个以作业ID命名的目录下。 33 34 // Create the splits for the job 35 LOG.debug(\u0026#34;Creating splits at \u0026#34; + fs.makeQualified(submitSplitFile)); 36 int maps; 37 if (job.getUseNewMapper()) { 38 maps = writeNewSplits(context, submitSplitFile); 39 } else { 40 maps = writeOldSplits(job, submitSplitFile); 41 } 42 job.set(\u0026#34;mapred.job.split.file\u0026#34;, submitSplitFile.toString()); 43 job.setNumMapTasks(maps); 44 45 // Write job file to JobTracker\u0026#39;s fs 46 FSDataOutputStream out = FileSystem.create(fs, submitJobFile, 47 new FsPermission(JOB_FILE_PERMISSION)); 48 49 try { 50 job.writeXml(out); 51 } finally { 52 out.close(); 53 } 54 55 // 8）使用句柄JobSubmissionProtocol通过RPC远程调用的submitJob()方法，向JobTracker提交作业。JobTracker根据接收到的submitJob()方法调用后，把调用放入到内存队列中，由作业调度器进行调度。并初始化作业实例。 56 57 JobStatus status = jobSubmitClient.submitJob(jobId); 58 if (status != null) { 59 return new NetworkedJob(status); 60 } else { 61 throw new IOException(\u0026#34;Could not launch job\u0026#34;); 62 } 63 } 1/** 2 * JobTracker.submitJob() kicks off a new job. 3 * 4 * Create a \u0026#39;JobInProgress\u0026#39; object, which contains both JobProfile 5 * and JobStatus. Those two sub-objects are sometimes shipped outside 6 * of the JobTracker. But JobInProgress adds info that\u0026#39;s useful for 7 * the JobTracker alone. 8 */ 9 public synchronized JobStatus submitJob(JobID jobId) throws IOException { 10 if(jobs.containsKey(jobId)) { 11 //job already running, don\u0026#39;t start twice 12 return jobs.get(jobId).getStatus(); 13 } 14 15 JobInProgress job = new JobInProgress(jobId, this, this.conf); 16 17 String queue = job.getProfile().getQueueName(); 18 if(!(queueManager.getQueues().contains(queue))) { 19 new CleanupQueue().addToQueue(conf,getSystemDirectoryForJob(jobId)); 20 throw new IOException(\u0026#34;Queue \\\u0026#34;\u0026#34; + queue + \u0026#34;\\\u0026#34; does not exist\u0026#34;); 21 } 22 23 // check for access 24 try { 25 checkAccess(job, QueueManager.QueueOperation.SUBMIT_JOB); 26 } catch (IOException ioe) { 27 LOG.warn(\u0026#34;Access denied for user \u0026#34; + job.getJobConf().getUser() 28 + \u0026#34;. Ignoring job \u0026#34; + jobId, ioe); 29 new CleanupQueue().addToQueue(conf, getSystemDirectoryForJob(jobId)); 30 throw ioe; 31 } 32 33 return addJob(jobId, job); 34 } ","link":"https://idouba.com/hadoop_jobclient_submit/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】Hadoop作业提交之客户端作业提交"},{"body":"一、概要描述\n在上一篇博文中主要描述了JobTracker接收作业的几个服务（或功能）模块的初始化过程。本节将介绍这些服务（或功能）是如何接收到提交的job。本来作业的初始化也可以在本节内描述，但是涉及到JobInProgress的初始化过程放在一张图上太拥挤，就分开到下一篇文章中描述。\n二、 流程描述\nJobClient通过RPC的方式向JobTracker提交作业； 调用JobTracker的submitJob方法。该方法是JobTracker向外提供的供调用的提交作业的接口。 submit方法中调用JobTracker的addJob方法。 在addJob方法中会把作业加入到集合中供调度，并会触发注册的JobInProgressListener的jobAdded事件。由上篇博文的jobtracker相关服务和功能的初始化的FairScheduler的start方法中看到，这里注册的是两个JobInProgressListener。分别是FairScheduler的内部类JobListener和EagerTaskInitializationListener。 FairScheduler的内部类JobListener响应jobAdded事件事件。只是为每个加入的Job创建一个用于FairScheduler调度用的JobInfo对象，并将其和job的对应的存储在Map\u0026lt;JobInProgress, JobInfo\u0026gt; infos集合中。 EagerTaskInitializationListener响应jobAdded事件事件。jobAdded 只是简单的把job加入到一个List类型的 jobInitQueue中。并不直接对其进行初始化，对其中的job的处理由另外线程JobInitManager来做。该线程，一直检查 jobInitQueue是否有作业，有则拿出来从线程池中取一个线程InitJob处理。关于作业的初始化过程专门在下一篇文章中介绍。 JobTracker接收作业提交\n三、代码详细\n1. JobClient的submitJob方法，调用submitJobInternal方法。\n主要流程：\n1）通过调用JobTracker的getNewJobId()向jobtracker请求一个新的作业ID\n2）获取job的jar、输入分片、作业描述等几个路径信息，以jobId命名。\n3）其中getSystemDir()是返回jobtracker的系统目录，来放置job相关的文件。包括：mapreduce的jar文件submitJarFile、分片文件submitSplitFile、作业描述文件submitJobFile\n4）检查作业的输出说明，如果没有指定输出目录或输出目录以及存在，则作业不提交。参照org.apache.hadoop.mapreduce.lib.output.FileOutputFormat的checkOutputSpecs方法。如果没有指定，则抛出InvalidJobConfException，文件已经存在则抛出FileAlreadyExistsException\n5）计算作业的输入分片。通过InputFormat的getSplits(job)方法获得作业的split并将split序列化封装为RawSplit。返回split数目，也即代表有多个分片有多少个map。详细参见InputFormat获取Split的方法。\n6）writeNewSplits 方法把输入分片写到JobTracker的job目录下。\n7）将运行作业所需的资源（包括作业jar文件，配置文件和计算所得的输入分片）复制到jobtracker的文件系统中一个以作业ID命名的目录下。\n8） 使用句柄JobSubmissionProtocol通过RPC远程调用的submitJob()方法，向JobTracker提交作业。 JobTracker作业放入到内存队列中，由作业调度器进行调度。并初始化作业实例。JobTracker创建job成功后会给JobClient传回 一个JobStatus对象用于记录job的状态信息，如执行时间、Map和Reduce任务完成的比例等。JobClient会根据这个 JobStatus对象创建一个 NetworkedJob的RunningJob对象，用于定时从JobTracker获得执行过程的统计数据来监控并打印到用户的控制台。\n1 public RunningJob submitJobInternal(JobConf job) 2 throws FileNotFoundException, ClassNotFoundException, 3 InterruptedException, IOException { 4 5 // 1）通过调用JobTracker的getNewJobId()向jobtracker请求一个新的作业ID 6 JobID jobId = jobSubmitClient.getNewJobId(); 7 // 2）获取job的jar、输入分片、作业描述等几个路径信息，以jobId命名。 8 // 3）其中getSystemDir()是返回jobtracker的系统目录，来放置job相关的文件。包括：mapreduce的jar文件submitJarFile、分片文件submitSplitFile、作业描述文件submitJobFile 9 10 Path submitJobDir = new Path(getSystemDir(), jobId.toString()); 11 Path submitJarFile = new Path(submitJobDir, \u0026#34;job.jar\u0026#34;); 12 Path submitSplitFile = new Path(submitJobDir, \u0026#34;job.split\u0026#34;); 13 configureCommandLineOptions(job, submitJobDir, submitJarFile); 14 Path submitJobFile = new Path(submitJobDir, \u0026#34;job.xml\u0026#34;); 15 int reduces = job.getNumReduceTasks(); 16 JobContext context = new JobContext(job, jobId); 17 18 // Check the output specification 19 // 4）检查作业的输出说明，如果没有指定输出目录或输出目录以及存在，则作业不提交。参照org.apache.hadoop.mapreduce.lib.output.FileOutputFormat的checkOutputSpecs方法。如果没有指定，则抛出InvalidJobConfException，文件已经存在则抛出FileAlreadyExistsException 20 21 if (reduces == 0 ? job.getUseNewMapper() : job.getUseNewReducer()) { 22 org.apache.hadoop.mapreduce.OutputFormat\u0026lt;, \u0026gt; output = ReflectionUtils 23 .newInstance(context.getOutputFormatClass(), job); 24 output.checkOutputSpecs(context); 25 } else { 26 job.getOutputFormat().checkOutputSpecs(fs, job); 27 } 28 29 // 5）计算作业的输入分片。详细参见FormatInputFormat获取Split的方法。 30 // 6）writeNewSplits 方法把输入分片写到JobTracker的job目录下，名称是submitSplitFile 31 // job.split名称。 32 // 7）将运行作业所需的资源（包括作业jar文件，配置文件和计算所得的输入分片）复制到jobtracker的文件系统中一个以作业ID命名的目录下。 33 34 // Create the splits for the job 35 LOG.debug(\u0026#34;Creating splits at \u0026#34; + fs.makeQualified(submitSplitFile)); 36 int maps; 37 if (job.getUseNewMapper()) { 38 maps = writeNewSplits(context, submitSplitFile); 39 } else { 40 maps = writeOldSplits(job, submitSplitFile); 41 } 42 job.set(\u0026#34;mapred.job.split.file\u0026#34;, submitSplitFile.toString()); 43 job.setNumMapTasks(maps); 44 45 // Write job file to JobTracker\u0026#39;s fs 46 FSDataOutputStream out = FileSystem.create(fs, submitJobFile, 47 new FsPermission(JOB_FILE_PERMISSION)); 48 49 try { 50 job.writeXml(out); 51 } finally { 52 out.close(); 53 } 54 55 // 8）使用句柄JobSubmissionProtocol通过RPC远程调用的submitJob()方法，向JobTracker提交作业。JobTracker根据接收到的submitJob()方法调用后，把调用放入到内存队列中，由作业调度器进行调度。并初始化作业实例。 56 57 JobStatus status = jobSubmitClient.submitJob(jobId); 58 if (status != null) { 59 return new NetworkedJob(status); 60 } else { 61 throw new IOException(\u0026#34;Could not launch job\u0026#34;); 62 } 63 } 2. JobTracker的submitJob方法，是JobTracker向外提供的供调用的提交作业的接口。\n1public synchronized JobStatus submitJob(JobID jobId) throws IOException { 2if(jobs.containsKey(jobId)) { 3 //检查Job已经存在，则仅仅返回其status 4 return jobs.get(jobId).getStatus(); 5} 6//不存在，则创建该job 的JobInProgress 实例， 7JobInProgress job = new JobInProgress(jobId, this, this.conf); 8String queue = job.getProfile().getQueueName(); 9 new CleanupQueue().addToQueue(conf,getSystemDirectoryForJob(jobId)); 10 } 11// check for access 12checkAccess(job, QueueManager.QueueOperation.SUBMIT_JOB); 13// 检查内存是否够用 14checkMemoryRequirements(job); 15return addJob(jobId, job); 16} 3. JobTracker的addJob方法，把作业加入到集合中供调度。其中jobs 是Map\u0026lt;JobID, JobInProgress\u0026gt;类型，维护着加入进来的JobInProgress job。\n1private synchronized JobStatus addJob(JobID jobId, JobInProgress job) { 2totalSubmissions++; 3synchronized (jobs) { 4 synchronized (taskScheduler) { 5//将job实例加入到Map\u0026lt;JobID, JobInProgress\u0026gt; jobs 集合中， 6 jobs.put(job.getProfile().getJobID(), job); 7//并触发所有注册的JobInProgressListener，通知其一个新Job添加进来了，让各个Listener响应各自的动作。 8 for (JobInProgressListener listener : jobInProgressListeners) { 9 try { 10 listener.jobAdded(job); 11 } catch (IOException ioe) { 12 LOG.warn(\u0026#34;Failed to add and so skipping the job : \u0026#34; 13 + job.getJobID() + \u0026#34;. Exception : \u0026#34; + ioe); 14 } 15 } 16 } 17 } 18 myInstrumentation.submitJob(job.getJobConf(), jobId); 19 return job.getStatus(); 20 } 4.FairScheduler.JobListener的jobAdded方法。jobAdded方法是JobInProgressListener中定义的在JobTracker中响应job变化的方法。在这个方法中，只是为每个加入的Job创建一个用于FairScheduler调度用的JobInfo对象，并将其和job的对应的存储在Map\u0026lt;JobInProgress, JobInfo\u0026gt; infos集合中。\n1@Override 2 public void jobAdded(JobInProgress job) { 3 synchronized (FairScheduler.this) { 4 poolMgr.addJob(job); 5 JobInfo info = new JobInfo(); 6 infos.put(job, info); 7 update(); 8 } 9 } 5. EagerTaskInitializationListener的jobAdded方法。这个方法其实在前面文章中介绍过，在EagerTaskInitializationListener中，jobAdded 只是简单的把job加入到一个List类型的 jobInitQueue中。并不直接对其进行初始化，对其中的job的处理由另外线程JobInitManager来做。该线程，一直检查 jobInitQueue是否有作业，有则拿出来从线程池中取一个线程InitJob处理。关于作业的初始化过程专门在下一篇文章中介绍。\n1@Override 2 public void jobAdded(JobInProgress job) { 3 synchronized (jobInitQueue) { 4 jobInitQueue.add(job); 5 resortInitQueue(); 6 jobInitQueue.notifyAll(); 7 } 8 9 } 完。\n","link":"https://idouba.com/hadoop_mapreduce_jobadded/","section":"posts","tags":["hadoop","java","mapreduce","source"],"title":"【hadoop代码笔记】hadoop作业提交之JobTracker接收作业提交"},{"body":"一、概述\n继承自JobInProgressListener，实现了jobAdded，jobRemoved，jobUpdated方法。哦，不能说实现，应该说继承，JobInProgressListener居然是个抽象类，看着怎么这样的listener也应该是个interface。\n在该listener被注册后，就响应jobAdded，jobRemoved，jobUpdated动作。在EagerTaskInitializationListener中，响应这三种动作来维护内部的一个job列表（List jobInitQueue），并启动线程对job列表中的job异步的进行初始化。\n二、主要代码逻辑\n在job被添加到JobTracker时，注册的Lister会响应该方法。即当有作业提交到JobTracker时，该方法会把JIP加到jobInitQueue列表中，并且根据作业优先级和启动时间来调整其顺序。 jobInitManagerThread会一直产看jobInitManagerThread列表中的job，逐一取出来初始化其task。 三、主要成员\n1 private JobInitManager jobInitManager = new JobInitManager(); //一个job初始化线程，关注job队列jobInitQueue，取出进行初始化 2 private Thread jobInitManagerThread; // JobInitManager线程 3 private List\u0026lt;JobInProgress\u0026gt; jobInitQueue = new ArrayList\u0026lt;JobInProgress\u0026gt;(); //响应lister的几种方法，维护的job队列 4 private ExecutorService threadPool; //一个线程池，里面的一个线程取一个job进行初始化 5 private int numThreads; //线程池的线程数，可配置 四、主要方法\n1. EagerTaskInitializationListener的jobAdded方法：\n首先关注的代码片段是该listener的jobAdded方法，前面说过，在FairScheduler的start方法中（taskTrackerManager.addJobInProgressListener(eagerInitListener)）会把EagerTaskInitializationListener注册到JobTracker，在jobTracker中加入job的时候（addJob被调用），触发其上所有的jobListener的jobAdded方法。\n在EagerTaskInitializationListener中，jobAdded只是简单的把job加入到一个List类型的 jobInitQueue中。并不直接对其进行初始化，对其中的job的处理由另外线程来做。\n1@Override 2 public void jobAdded(JobInProgress job) { 3 synchronized (jobInitQueue) { 4 jobInitQueue.add(job); 5 resortInitQueue(); 6 jobInitQueue.notifyAll(); 7 } 8 9 } 2. JobInitManager类：\n一个线程，对jobInitQueue上保存的每个Job启动一个线程来执行初始化工作。在其run方法中会一直检查jobInitQueue是否有作业，有则拿出来从线程池中取一个线程处理。\n1class JobInitManager implements Runnable { 2 3 public void run() { 4 JobInProgress job = null; 5 while (true) { 6 try { 7 synchronized (jobInitQueue) { 8 while (jobInitQueue.isEmpty()) { 9 jobInitQueue.wait(); 10 } 11 job = jobInitQueue.remove(0); 12 } 13 threadPool.execute(new InitJob(job)); 14 } catch (InterruptedException t) { 15 LOG.info(\u0026#34;JobInitManagerThread interrupted.\u0026#34;); 16 break; 17 } 18 } 19 LOG.info(\u0026#34;Shutting down thread pool\u0026#34;); 20 threadPool.shutdownNow(); 21 } 22 } 3. InitJob 一个线程类定义，真正处理每一个job的初始化。其实调用的是job的初始化方法(JobInProgress initTasks)\n1static class InitJob implements Runnable { 2 private JobInProgress job; 3 public InitJob(JobInProgress job) { 4 this.job = job; 5 } 6 7 public void run() 8 { 9 job.initTasks(); 10 } 11 } 完。\n","link":"https://idouba.com/eagertaskinitializationlistener/","section":"posts","tags":["hadoop","java","mapreduce"],"title":"【hadoop代码笔记】Hadoop作业提交中EagerTaskInitializationListener的作用"},{"body":"Hadoop的各个服务间，客户端和服务间的交互采用RPC方式。关于这种机制介绍的资源很多，也不难理解，这里不做背景介绍。只是尝试从Jobclient向JobTracker提交作业这个最简单的客户端服务器交互的代码中，去跟踪和了解下RPC是怎么被使用的。不同于准备发表博客时搜索的几篇博文，试图通过一种具体的场景来介绍，属于比较初级。其他DataNode和Namenode之间，Tasktracker和JobTracker之间的交互基本也都一样。为了引用的代码篇幅尽可能少，忽略了代码中写日志（包括Metrics）、某些判断等辅助代码。\n1 RPC客户端请求（从JobClient的jobSubmitClient 入手） Jobclient包含一个JobSubmissionProtocol jobSubmitClient类型的句柄，从作业提交一节的介绍中看到Jobclient的计划所有重要操作都是通过jobSubmitClient来完成的。包括\nJobSubmissionProtocol Outline\n所有这些方法都在JobSubmissionProtocol接口中定义。在0.20.1的时候已经到Version 20了，在2.2.0好像到了Version 40了,说明功能一直在增强。 客户端的某个方法调用如何会调用到服务端的方法呢？在客户端机器上调用JobClient的getAllJobs(),怎么调用到了服务端JobTracker的getAllJobs()。这也是我尝试讲明白的核心内容。为了体现代码的一步一步分析总结在最后。可能循序渐进的作用没起到，还会笔记读起来笔记乱，感受有点不太好可能:-(。 首先看客户端JobClient中的jobSubmitClient初始化方法。在JobClient的init方法中判断不是local的方式则会调用createRPCProxy方法，进而调用RPC的getProxy方法。方法连接对应IP的服务器。比较客户端和服务端的RPC版本一致，返回一个JobSubmissionProtocol类型的句柄，抛出VersionMismatch异常。\n1private JobSubmissionProtocol createRPCProxy(InetSocketAddress addr, 2 Configuration conf) throws IOException { 3 return (JobSubmissionProtocol) RPC.getProxy(JobSubmissionProtocol.class, 4 JobSubmissionProtocol.versionID, addr, getUGI(conf), conf, 5 NetUtils.getSocketFactory(conf, JobSubmissionProtocol.class)); 6 } 7 public static VersionedProtocol getProxy(Class\u0026lt; \u0026gt; protocol, 8 long clientVersion, InetSocketAddress addr, UserGroupInformation ticket, 9 Configuration conf, SocketFactory factory) throws IOException { 10 11 VersionedProtocol proxy = 12 (VersionedProtocol) Proxy.newProxyInstance( 13 protocol.getClassLoader(), new Class[] { protocol }, 14 new Invoker(addr, ticket, conf, factory)); 15 long serverVersion = proxy.getProtocolVersion(protocol.getName(), 16 clientVersion); 17 if (serverVersion == clientVersion) { 18 return proxy; 19 } else { 20 throw new VersionMismatch(protocol.getName(), clientVersion, 21 serverVersion); 22 } 23} 注意到调用了java的反射代理在构建VersionedProtocol的时候ProxynewProxyInstance方法初始化了一个Invoker类型的对象。 该对象是org.apache.hadoop.ipc.RPC.包下Server类的一个内部类。\n1static class Invoker implements InvocationHandler 这下明白了！基于java的reflect机制提供的一种Proxy使用方式。InvocationHandler这个Interface的作用就是把proxy 上的方法调用派发到实现了InvocationHandler的类上来。即Jobclient上中jobSubmitClient的任何调用都会派发到这个Invoker上来。\n那么Invoker中做了什么事情呢？Invoker类实现了InvocationHandler接口定义的唯一的invoke方法。只是把传入的调用信息，包括方面名，方法参数封装为一个invocation对象，调用用Client client对象的call方法来执行操作。\n1public Object invoke(Object proxy, Method method, Object[] args) 2 throws Throwable { 3 ObjectWritable value = (ObjectWritable) 4 client.call(new Invocation(method, args), address, 5 method.getDeclaringClass(), ticket); 6 return value.get();} 了解Client的call方法，该方法的主要作用是把参数发送给指定服务端地址上的IPC server。并获取结果。构建一个Call对象，封装了请求参数（其实是Invocation封装了方法和参数的对象），创建一个连接到IPC服务器的connection，然后发送出去。（client发送请求还是有些业务的，包括Client下的几个内部类的工作，在此略去）\n1public Writable call(Writable param, InetSocketAddress addr, 2 Class\u0026lt; \u0026gt; protocol, UserGroupInformation ticket) 3 { 4 Call call = new Call(param); 5 Connection connection = getConnection(addr, protocol, ticket, call); 6 connection.sendParam(call); // send the parameter 7 return call.value; 8 } 客户端的主要过程总结如下：客户端Jobclient的创建一个JobSubmissionProtocol jobSubmitClient，jobSubmitClient的所有请求都会通过Invoker封装成一个请求，通过Client的call方法发送到服务端。\n2 RPC服务端处理(看Jobtracker的interTrackerServer响应请求) 接下来看法服务器是如何接收请求，Client的call将请求发送到什么样的服务器？服务器如何解释这些请求，如何响应请求的。\n服务端JobTracker实现了JobSubmissionProtocol接口，因此提供了JobSubmissionProtocol定义的所有方法\n1public class JobTracker implements MRConstants, InterTrackerProtocol, 2 JobSubmissionProtocol, TaskTrackerManager, RefreshAuthorizationPolicyProtocol 在JobTracker内包含一个类型org.apache.hadoop.ipc.Server的的实例interTrackerServer ，该实例其实是响应客户端的的RPC调用的服务实例。\n1this.interTrackerServer = RPC.getServer(this, addr.getHostName(), addr.getPort(), handlerCount, false, conf); 查看RPC的getServer方法\n1public static Server getServer(final Object instance, final String bindAddress, final int port, 2 final int numHandlers, 3 final boolean verbose, Configuration conf) 4 throws IOException { 5 return new Server(instance, conf, bindAddress, port, numHandlers, verbose); 6 } 再往下看其实Server的构造函数，就是在某个Ip和端口上监听，响应客户端发起的请求。多么典型的客户端服务器模式呀。代码看上去多么想Socket通信那一套呀。看到了bindAddress，看到了port，还看到socketSendBufferSize。没错！\n1protected Server(String bindAddress, int port, 2 Class\u0026lt; extends Writable\u0026gt; paramClass, int handlerCount, 3 Configuration conf, String serverName) 4 throws IOException { 5 this.bindAddress = bindAddress; 6 this.conf = conf; 7 this.port = port; 8 this.paramClass = paramClass; 9 this.handlerCount = handlerCount; 10 this.socketSendBufferSize = 0; 11 this.maxQueueSize = handlerCount * MAX_QUEUE_SIZE_PER_HANDLER; 12 this.callQueue = new LinkedBlockingQueue\u0026lt;Call\u0026gt;(maxQueueSize); 13 this.maxIdleTime = 2*conf.getInt(\u0026#34;ipc.client.connection.maxidletime\u0026#34;, 1000); 14 this.maxConnectionsToNuke = conf.getInt(\u0026#34;ipc.client.kill.max\u0026#34;, 10); 15 this.thresholdIdleConnections = conf.getInt(\u0026#34;ipc.client.idlethreshold\u0026#34;, 4000); 16 17 // Start the listener here and let it bind to the port 18 listener = new Listener(); 19 this.port = listener.getAddress().getPort(); 20 this.rpcMetrics = new RpcMetrics(serverName, 21 Integer.toString(this.port), this); 22 this.tcpNoDelay = conf.getBoolean(\u0026#34;ipc.server.tcpnodelay\u0026#34;, false); 23 24 // Create the responder here 25 responder = new Responder(); 26 } 同时不小心注意到Server类的outline阵容还是很宏大的，除了一长串的方法外，还包括Call, Connection, Handler,Listener, responder 五个内部类，猜就是这些协作来完成Server的服务响应处理。\nipc_server outline 同时注意到Server中包含的如下几个重要的实例\n1private BlockingQueue\u0026lt;Call\u0026gt; callQueue; // queued calls 2 private List\u0026lt;Connection\u0026gt; connectionList = 3 Collections.synchronizedList(new LinkedList\u0026lt;Connection\u0026gt;()); 4 private Listener listener = null; 5 private Responder responder = null; 6 private Handler[] handlers = null; 再看看Server的start（）方法\n1public synchronized void start() throws IOException { 2 responder.start(); 3 listener.start(); 4 handlers = new Handler[handlerCount]; 5 6 for (int i = 0; i \u0026lt; handlerCount; i++) { 7 handlers[i] = new Handler(i); 8 handlers[i].start(); 9 } 10} 其中，在Server的构造函数中看到了两个差不多能猜到其功能的东西：Listener \u0026amp; Responder。从命名上几乎就能猜到，他们分别是监听用户请求和响应用户请求的线程？应该是线程吧？居然猜对了！\n先看下Listener。构造函数如下\n1public Listener() throws IOException { 2 address = new InetSocketAddress(bindAddress, port); 3 // Create a new server socket and set to non blocking mode 4 acceptChannel = ServerSocketChannel.open(); 5 acceptChannel.configureBlocking(false); 6 7 // Bind the server socket to the local host and port 8 bind(acceptChannel.socket(), address, backlogLength); 9 port = acceptChannel.socket().getLocalPort(); //Could be an ephemeral port 10 // create a selector; 11 selector= Selector.open(); 12 13 // Register accepts on the server socket with the selector. 14 acceptChannel.register(selector, SelectionKey.OP_ACCEPT); 15} 重点看下线程的业务方法，即其run方法做了些啥。方法虽然很长，但是业务很典型，在服务端监听，收到数据就接收。\n1public void run() { 2 SERVER.set(Server.this); 3 while (running) { 4 SelectionKey key = null; 5 selector.select(); 6 Iterator\u0026lt;SelectionKey\u0026gt; iter = selector.selectedKeys().iterator(); 7 while (iter.hasNext()) { 8 key = iter.next(); 9 iter.remove(); 10 try { 11 if (key.isValid()) { 12 if (key.isAcceptable()) 13 doAccept(key); 14 else if (key.isReadable()) 15 doRead(key); 16 } 17 18 } 接着看下接受数据的 doAccept(SelectionKey key)和doRead(SelectionKey key)方法。\ndoAccept做的事情是把每一个数据连接的请求绑定到一个Connection对象上，并把Connection全部添加到connectionList集合中；doRead做的事情是对每个Connection执行readAndProcess操作。\n1void doAccept(SelectionKey key) 2 { 3 Connection c = null; 4 ServerSocketChannel server = (ServerSocketChannel) key.channel(); 5 // accept up to 10 connections 6 for (int i=0; i\u0026lt;10; i++) { 7 SocketChannel channel = server.accept(); 8 SelectionKey readKey = channel.register(selector, SelectionKey.OP_READ); 9 c = new Connection(readKey, channel, System.currentTimeMillis()); 10 readKey.attach(c); 11 synchronized (connectionList) { 12 connectionList.add(numConnections, c); 13 numConnections++; 14 } 15 } 16 17 void doRead(SelectionKey key) 18 { 19 Connection c = (Connection)key.attachment(); 20 c.setLastContact(System.currentTimeMillis()); 21 count = c.readAndProcess(); 22 } 需要关注下org.apache.hadoop.ipc.Server.Connection类。重点看listener doRead 中调用的readAndProcess方法\n1data = ByteBuffer.allocate(dataLength); 2 count = channelRead(channel, data); 3 4 if (headerRead) { 5 processData(); 6 data = null; 7 return count; 8 } else { 9 processHeader(); 10 headerRead = true; 11 data = null; 12 } 13 authorize(user, header); 其中的processHeader()作用是解析出通信的protocol类\n1header.readFields(in); 2String protocolClassName = header.getProtocol(); 3protocol = getProtocolClass(header.getProtocol(), conf); processData的主要代码如下：\n1int id = dis.readInt(); 2Writable param = ReflectionUtils.newInstance(paramClass, conf); param.readFields(dis); 3 Call call = new Call(id, param, this); 4callQueue.put(call); 读取调用Id，从读取的数据中构建参数，并构造一个Call对象，放置到BlockingQueue 类型的集合中callQueue。 至此Listener的所有功能就是接收客户端发起的请求，构造Call对象并放置到队列中等待处理。 接下来是发送响应的Responder类。 重点是processResponse，是真正的写response的地方，即把执行结果写会对应的channel。\n1private boolean processResponse(LinkedList\u0026lt;Call\u0026gt; responseQueue, 2 boolean inHandler) 3{ 4call = responseQueue.removeFirst(); 5 SocketChannel channel = call.connection.channel; 6 int numBytes = channelWrite(channel, call.response); 7} Handler是处理请求的线程。是对队列中的每个call进行处理的类。前面看Server包含的实例的时候看到了，Server包含一个Handler数组，在Server的start方法中启动了Listener，Responder线程，同时初始化了handlerCount个Handler线程并且启动。 主要还是看run方法。主要是从请求队列callQueue中逐个取出call了，并进行处理。处理过程即，对每个call，执行Server的call[ 方法，（实际的call方法是从org.apache.hadoop.ipc.RPC.Server，继承了org.apache.hadoop.ipc.Server，不是一个Server哦！这个在后面RPC类中会讲到）并调用Responder方法doRespond，把结果返回。\n1while (running) 2{ 3 final Call call = callQueue.take(); // pop the queue; maybe blocked here 4 CurCall.set(call); 5 value = Subject.doAs(call.connection.user, 6 new PrivilegedExceptionAction\u0026lt;Writable\u0026gt;() { 7 @Override 8 public Writable run() throws Exception { 9 // make the call 10 return call(call.connection.protocol, 11 call.param, call.timestamp); 12 } 13 } 14 ); 15 CurCall.set(null); 16 setupResponse(buf, call, 17 (error == null) Status.SUCCESS : Status.ERROR, 18 value, errorClass, error); 19 responder.doRespond(call); 20 } 调用的setupResponse方法\n1private void setupResponse(ByteArrayOutputStream response, 2 Call call, Status status, 3 Writable rv, String errorClass, String error) 4 5DataOutputStream out = new DataOutputStream(response); 6 out.writeInt(call.id); // write call id 7 out.writeInt(status.state); // write status 8 rv.write(out); 9call.setResponse(ByteBuffer.wrap(response.toByteArray())); 核心就一句，把执行结果写到Call中去。 顺便看下上面方法调用的Responder的doRespond方法，即把经过handler处理的带有结果的call放到对应的响应队列中，等待responder线程来逐个返回给客户端，注意看到一个，如果队列中只有一个对象时，直接调用processResponse触发把结果翻过给客户端。\n1void doRespond(Call call) throws IOException { 2 { 3 call.connection.responseQueue.addLast(call); 4 if (call.connection.responseQueue.size() == 1) { 5 processResponse(call.connection.responseQueue, true); 6 } 7 } 8} 即handler完成call之后就开始向客户端写call结果，但是结果可能太多，无法通过一次性发送完毕，而发送之后还要等待client接受完毕才能再发，如果现在handler在那里等待客户端接受完毕，然后再发，效率不高。解决办法是handler处理完毕之后，只向client发送一次处理结果。如果这一次将处理结果发送完毕，接下来就没有response的事情了，如果没有发送完毕，接下来response负责将剩下的处理结果发送给客户端。这样handler的并发量会大一些。详细可参照Responder线程的run方法和\n1writeSelector.select(PURGE_INTERVAL); 2 Iterator\u0026lt;SelectionKey\u0026gt; iter = writeSelector.selectedKeys().iterator(); 3 while (iter.hasNext()) { 4 SelectionKey key = iter.next(); 5 iter.remove(); 6 if (key.isValid() \u0026amp;\u0026amp; key.isWritable()) { 7 doAsyncWrite(key); 8 } //在doAsyncWrite方法中，从key中获得Call，并对每个call执行processResponse方法。\n1private void doAsyncWrite(SelectionKey key) 2 Call call = (Call)key.attachment(); 3 processResponse(call.connection.responseQueue, false)) 至此观察到服务端的工作的主要过程是：\nServer启动的时候，启动一个listener线程，一个Responder线程，若干个Handler线程。\nListener线程接受客户端发起的请求（在doAccept中接收请求，并且每个请求构建一个Connection，绑定到一个SelectionKey上），读取请求数据，根据请求数据构造call对象，将Call加入队列。\nHandler线程从请求队列（callQueue）中获取每个Call，进行处理，把处理结果放到对应的connection的应答队列中）（responseQueue，通过调用responder.doRespond）。Responder线程检查负责把结果返回给客户端。（processResponse，把responseQueue队列的结果数据返回）\n有一点需要继续关注一下，就是Handler中处理了客户端发起的请求，并且将结果通过Responder返回。但是并没有发现Handler是调用到了Jobtracker的方法。需要继续向下多看一点即可。 从代码看Handler的call方法调用的是org.apache.hadoop.ipc.Server.的抽象方法\n1public abstract Writable call(Class\u0026lt; \u0026gt; protocol, Writable param, long receiveTime) 实际调用是org.apache.hadoop.ipc.Server的子类org.apache.hadoop.ipc.RPC.Server的call方法从org.apache.hadoop.ipc.RPC.Server的call方法入手，该类在是RPC类的一个静态内部类。\n1//传入的param其实是一个Invocation对象。根据该对象的方面明，参数声明构造Method，调用Method，得到执行结果，根据返回值得类型，构造一个Writable的对象。 2Writable call(Class\u0026lt; \u0026gt; protocol, Writable param, long receivedTime) 3{ 4Invocation call = (Invocation)param; 5 Method method = 6 protocol.getMethod(call.getMethodName(), 7 call.getParameterClasses()); 8 method.setAccessible(true); 9 Object value = method.invoke(instance, call.getParameters()); 10 return new ObjectWritable(method.getReturnType(), value); 11} 重点看这一句\n1 Object value = method.invoke(instance, call.getParameters()); 即最终是调用该instance上的对应名称的方法。而instance是那个实例呢？而从Server的构造方法中得到答案。\n1this.interTrackerServer = RPC.getServer(this, addr.getHostName(), addr.getPort(), handlerCount, false, conf);\u0026lt;/pre\u0026gt; 即最终调用到JobTracker的对应方法。\n3 主要流程总结 整个调用过程总结如下：根据接口JobSubmissionProtocol动态代理生成一个代理对象jobSubmitClient，调用这个代理对象的时候;用户的调用请求被RPC的Invoker捕捉到，然后包装成调用请求，序列化成数据流发送到服务端Jobtracker的interTrackerServer实例；服务端interTrackerServer从数据流中解析出调用请求，然后根据用户所希望调用的接口JobSubmissionProtocol，通过反射调用接口真正的实现对象Jobtracker，再把调用结果返回给客户端的jobSubmitClient。\n4 主要类功能描述 至此根据Jobclient通过RPC方式向JobTracker请求服务的过程就描述完毕，到此主要内容应该介绍完毕。但是看到cover的代码，发现RPC的主要功能在里面了。\nrpc package\n为了功能完整期间，在动态的串联这些类以为，把涉及到主要类的功能做个描述，其实大部分在前面代码中也有提到。\nRPC类是对Server、Client的具体化。在RPC类中规定，客户程序发出请求调用时，参数类型必须是Invocation；从服务器返回的值类型必须是ObjectWritable。RPC类是对Server、Client的包装，简化用户的使用。如果一个类需充当服务器，只需通过RPC类的静态方法getServer获得Server实例，然后start。同时此类提供协议接口的实现。如果一个类充当客户端，可以通过getProxy或者waitForProxy获得一个实现了协议接口的proxy object，与服务器端交互。\norg.apache.hadoop.ipc.Server\nServer.Listener： RPC Server的监听者，用来接收RPC Client的连接请求和数据，其中数据封装成Call后PUSH到Call队列。 Server.Handler： RPC Server的Call处理者，和Server.Listener通过Call队列交互。 Server.Responder： RPC Server的响应者。Server.Handler按照异步非阻塞的方式向RPC Client发送响应，如果有未发送出的数据，交由Server.Responder来完成。 Server.Connection： RPC Server数据接收者。提供接收数据，解析数据包的功能。 Server.Call： 持有客户端的Call信息。 org.apache.hadoop.ipc.Client Client.ConnectionId：到RPC Server对象连接的标识 Client.Call： Call调用信息。 Client.ParallelResults： Call响应。 org.apache.hadoop.ipc.RPC RPC.Invoker 对InvocationHandler的实现，提供invoke方法，实现RPC Client对RPC Server对象的调用。 RPC.Invocation 用来序列化和反序列化RPC Client的调用信息。（主要应用JAVA的反射机制和InputStream/OutputStream） 5 VersionedProtocol的其他子接口 除了JobClient和Jobtracker之间通信的JobSubmissionProtocol外，最后查看下VersionedProtocol 的继承树 versionprotocol继承结构\nHadoop中主要服务进程分别实现了各种接口，进而向外提供各种服务，其客户端通过RPC调用对应的服务。当然此处的客户端只是指调用上的客户端。 VersionedProtocol ：它是所有RPC协议接口的父接口，只有一个方法：getProtocolVersion（）。其子类接口的功能分别如下。 HDFS相关：\nClientDatanodeProtocol ：一个客户端和datanode之间的协议接口，用于数据块恢复 ClientProtocol ：client与Namenode交互的接口，所有控制流的请求均在这里，如：创建文件、删除文件等； DatanodeProtocol : Datanode与Namenode交互的接口，如心跳、blockreport等； NamenodeProtocol ：SecondaryNode与Namenode交互的接口。 Mapreduce相关 InterDatanodeProtocol ：Datanode内部交互的接口，用来更新block的元数据； InnerTrackerProtocol ：TaskTracker与JobTracker交互的接口，功能与DatanodeProtocol相似； JobSubmissionProtocol ：JobClient与JobTracker交互的接口，用来提交Job、获得Job等与Job相关的操作； TaskUmbilicalProtocol ：Task中子进程与母进程交互的接口，子进程即map、reduce等操作，母进程即TaskTracker，该接口可以回报子进程的运行状态。 ","link":"https://idouba.com/haddoop_rpc_jobclient_jobtracker/","section":"posts","tags":["mapreduce","RPC"],"title":"【hadoop代码笔记】通过JobClient对Jobtracker的调用详细了解Hadoop RPC"},{"body":"","link":"https://idouba.com/tags/rpc/","section":"tags","tags":null,"title":"RPC"},{"body":"看到wikipedia中文关于数据库相关的几个经典条目有点老旧，尤其和英文条目相比。确定开始翻译其中几篇，先从事务隔离等级开始。格式采用维基Sandbox发布后的格式。翻译完后自己校对过几遍，质量还可以。:-)\n已经在中文维基发布。\n翻译的中文条目地址：事务隔离等级；\n对应的英文条目地址：Isolation (database systems)\n欢迎大家指正，可以直接在维基上对应条目更新的！。\n事务隔离（isolation）定义了数据库系统中一个操作产生的影响什么时候以哪种方式可以对其他并发操作可见。隔离是事务ACID (原子性、一致性性、隔离性、持久性)四大属性中的一个重要属性。\n并发控制(Concurrency control) 并发控制描述了数据库处理隔离以保证数据正确性的机制。为了保证并行事务执行的准确执行数据库和存储引擎在设计的时候着重强调了这一点。典型的事务相关机制限制数据的访问顺序(执行调度)以满足可序列化 和可恢复性。限制数据访问意味着降低了执行的性能，并发控制机制就是要保证在满足这些限制的前提下提供尽可能高的性能。经常在不损害正确性的情况下，为了达到更好的性能，可序列化的的要求会减低一些，但是为了避免数据一致性的破坏，可恢复性必须保证。\n两阶段锁是关系数据库中最常见的提供了可序列化 和可恢复性的并发控制机制，为了访问一个数据库对象，事务首先要获得这个对象的 锁。对于不同的访问类型（如对对象的读写操作）和锁的类型，如果另外一个事务正持有这个对象的锁，获得锁的过程会被阻塞或者延迟。\n隔离级别(Isolation levels) 在数据库事务的ACID四个属性中，隔离性是一个最常放松的一个。为了获取更高的隔离等级，数据库系统的 锁机制或者多版本并发控制机制都会影响并发。 应用软件也需要额外的逻辑来使其正常工作。\n很多DBMS定义了不同的“事务隔离等级”来控制锁的程度。在很多数据库系统中，多数的数据库事务都避免高等级的隔离等级（如可序列化）从而减少对系统的锁定开销。程序员需要小心的分析数据库访问部分的代码来保证隔离级别的降低不会造成难以发现的代码bug。相反的，更高的隔离级别会增加死锁发生的几率，同样需要编程过程中去避免。\nANSI/ISO SQL定义的标准隔离级别如下。\n可序列化(Serializable) 最高的隔离级别。\n在基于锁机制并发控制的DBMS实现可序列化要求在选定对象上的读锁和写锁保持直到事务结束后才能释放。在SELECT 的查询中使用一个“WHERE”子句来描述一个范围时应该获得一个“范围锁(range-locks)”。这种机制可以避免“幻影读(phantom reads)”现象。\n当采用不基于锁的并发控制时不用获取锁。但当系统探测到几个并发事务有“写冲突”的时候，只有其中一个是允许提交的。这种机制的详细描述见“’快照隔离”\n可重复读(Repeatable reads) 在可重复读(REPEATABLE READS)隔离级别中，基于锁机制并发控制的DBMS需要对选定对象的读锁(read locks)和写锁(write locks)一直保持到事务结束，但不要求“范围锁(range-locks)”，因此可能会发生“幻影读(phantom reads)”\n授权读(Read committed) 在授权读(READ COMMITTED)级别中，基于锁机制并发控制的DBMS需要对选定对象的写锁(write locks)一直保持到事务结束，但是读锁(read locks)在SELECT操作完成后马上释放（因此“不可重复读”现象可能会发生，见下面描述）。和前一种隔离级别一样，也不要求“范围锁(range-locks)”。\n简而言之，授权读这种隔离级别保证了读到的任何数据都是提交的数据，避免读到中间的未提交的数据，脏读(dirty reads)。但是不保证事务重新读的时候能读到相同的数据，因为在每次数据读完之后其他事务可以修改刚才读到的数据。\n未授权读(Read uncommitted) 未授权读(READ UNCOMMITTED)是最低的隔离级别。允许_脏读(dirty reads)_，事务可以看到其他事务“尚未提交”的修改。\n通过比低一级的隔离级别要求更多的限制，高一级的级别提供更强的隔离性。标准允许事务运行在更强的事务隔离级别上。(如在可重复读(REPEATABLE READS)隔离级别上执行授权读(READ COMMITTED)的事务是没有问题的)\n默认隔离级别 不同的DBMS默认隔离级别也不同。多少数据库允许用户设置隔离级别。有些DBMS在执行一个SELECT语句时使用额外的语法来获取锁(如_SELECT … FOR UPDATE_来获得在访问的数据行上的排他锁)\n读现象(Read phenomena) ANSI/ISO 标准SQL 92涉及三种不同的一个事务读取另外一个事务可能修改的数据的“读现象”。\n下面的例子中，两个事务，事务1执行语句1。接着，事务2执行语句2并且提交，最后事务1再执行语句1. 查询使用如下的数据表。\nid name age 1 Joe 20 2 Jill 25 脏读(Dirty reads (Uncommitted Dependency)) 当一个事务允许读取另外一个事务修改但未提交的数据时，就可能发生脏读(dirty reads)。\n脏读(dirty reads)和不可重复读(non-repeatable reads)类似。事务2没有提交造成事务1的语句1两次执行得到不同的结果集。在未授权读(READ UNCOMMITTED)隔离级别唯一禁止的是更新混乱，即早期的更新可能出现在后来更新之前的结果集中。\n在我们的例子中，事务2修改了一行，但是没有提交，事务1读了这个没有提交的数据。现在如果事务2回滚了刚才的修改或者做了另外的修改的话，事务1中查到的数据就是不正确的了。\n事务 1 事务 2 /* Query 1 */ SELECT age FROM users WHERE id = 1; /* will read 20 */ /* Query 2 */ UPDATE users SET age = 21 WHERE id = 1; /* No commit here */ /* Query 1 */ SELECT age FROM users WHERE id = 1; /* will read 21 */ ROLLBACK; /* lock-based DIRTY READ */ 在这个例子中，事务2回滚后就没有id是1，age是21的数据行了。\n不可重复读(non-repeatable read) 在一次事务中，当一行数据获取两遍得到不同的结果表示发生了“不可重复读(non-repeatable read)”.\n在基于锁的并发控制中“不可重复读(non-repeatable read)”现象发生在当执行SELECT 操作时没有获得读锁(read locks)或者SELECT操作执行完后马上释放了读锁； 多版本并发控制中当没有要求一个提交冲突的事务回滚也会发生“不可重复读(non-repeatable read)”现象。\n事务 1 事务 2 /* Query 1 */ SELECT * FROM users WHERE id = 1; /* Query 2 */ UPDATE users SET age = 21 WHERE id = 1; COMMIT; /* in multiversion concurrency control, or lock-based READ COMMITTED */ /* Query 1 */ SELECT * FROM users WHERE id = 1; COMMIT; /* lock-based REPEATABLE READ */ 在这个例子中，事务2提交成功，因此他对id为1的行的修改就对其他事务可见了。但是事务1在此前已经从这行读到了另外一个“age”的值。在可序列化 (SERIALIZABLE)和可重复读(REPEATABLE READS)的隔离级别，数据库在第二次SELECT请求的时候应该返回事务2更新之前的值。在授权读(READ COMMITTED)和未授权读(READ UNCOMMITTED)，返回的是更新之后的值，这个现象就是不可重复读(non-repeatable read)。\n有两种策略可以避免不可重复读(non-repeatable read)。一个是要求事务2延迟到事务1提交或者回滚之后再执行。这种方式实现了T1, T2 的串行化调度。串行化调度可以支持可重复读(repeatable reads)。\n另一种策略是_多版本并发控制_。为了得到更好的并发性能，允许事务2先提交。但因为事务1在事务2之前开始，事务1必须在其开始执行时间点的数据库的快照上面操作。当事务1最终提交时候，数据库会检查其结果是否等价于T1, T2串行调度。如果等价，则允许事务1提交，如果不等价，事务1需要回滚并抛出个串行化失败的错误。\n使用基于锁的并发控制，在可重复读(REPEATABLE READS)的隔离级别中，ID=1的行会被锁住，在事务1提交或回滚前一直阻塞语句2的执行。在授权读(READ COMMITTED)的级别，语句1第二次执行，age已经被修改了。\n在_多版本并发控制_机制下，可序列化(SERIALIZABLE)级别，两次SELECT语句读到的数据都是事务1开始的快照，因此返回同样的数据。但是，如果事务1试图UPDATE这行数据，事务1会被要求回滚并抛出一个串行化失败的错误。\n在授权读(READ COMMITTED)隔离级别，每个语句读到的是语句执行前的快照，因此读到更新前后不同的值。在这种级别不会有串行化的错误(因为这种级别不要求串行化)，事务1也不要求重试。\n幻影读(phantom read) 在事务执行过程中，当两个完全相同的查询语句执行得到不同的结果集。这种现象称为“幻影读(phantom read)”\n当事务没有获取_范围锁的情况下执行SELECT … WHERE_操作可能会发生“幻影读(phantom read)”。\n“幻影读(phantom read)”是_不可重复读(Non-repeatable reads)_的一种特殊场景：当事务1两次执行_SELECT … WHERE_检索一定范围内数据的操作中间，事务2在这个表中创建了(如INSERT)了一行新数据，这条新数据正好满足事务1的“WHERE”子句。\n事务 1 事务 2 /* Query 1 */ SELECT * FROM users WHERE age BETWEEN 10 AND 30; /* Query 2 */ INSERT INTO users VALUES ( 3, 'Bob', 27 ); COMMIT; /* Query 1 */ SELECT * FROM users WHERE age BETWEEN 10 AND 30; 需要指出的是事务1执行了两遍同样的查询语句。如果设了最高的隔离级别，两次会得到同样的结果集，这也正是可数据库在序列化(SERIALIZABLE)隔离级别上需要满足的。但是在较低的隔离级别上，第二次查询可能会得到不同的结果集。\n在可序列化(SERIALIZABLE)隔离级别，查询语句1在age从10到30的记录上加锁，事务2只能阻塞直至事务1提交。在可重复读(REPEATABLE READ)级别，这个范围不会被锁定，允许记录插入，因此第二次执行语句1的结果中会包括新插入的行。\n隔离级别、读现象和锁(Isolation Levels, Read Phenomena and Locks) 隔离级别vs读现象(Isolation Levels vs Read Phenomena) 隔离级别 脏读 不可重复读 幻影读 未授权读 可能发生 可能发生 可能发生 授权读 - 可能发生 可能发生 可重复读 - - 可能发生 可序列化 - - - 可序列化(Serializable)隔离级别不等同于可串行化(Serializable)。可串行化调度(Serializable)是避免以上三种现象的必要条件，但不是充分条件。\n“可能发生”表示这个隔离级别会发生对应的现象，“-”表示不会发生。\n隔离级别vs 锁持续时间(Isolation Levels vs Lock Duration) 在基于锁的并发控制中，隔离级别决定了锁的持有时间。“C”-表示锁会持续到事务提交。 “S” –表示锁持续到当前语句执行完毕。如果锁在语句执行完毕就释放则另外一个事务就可以在这个事务提交前修改锁定的数据，从而造成混乱。\n隔离级别 写操作 读操作 范围操作 (where) 未授权读 S S S 授权读 C S S 可重复读 C C S 可序列化 C C C 参照\n相关条目 原子性 一致性 持久性 锁 乐观并发控制 关系数据库 快照隔离 外部链接 Oracle? Database Concepts, chapter 13 Data Concurrency and Consistency, Preventable Phenomena and Transaction Isolation Levels Oracle? Database SQL Reference, chapter 19 SQL Statements: SAVEPOINT to UPDATE, SET TRANSACTION in JDBC: Connection constant fields, Connection.getTransactionIsolation(), Connection.setTransactionIsolation(int) in Spring Framework: @Transactional, Isolation Category:Data management Category:Transaction processing\n","link":"https://idouba.com/wikipedia_isolation/","section":"posts","tags":["数据库","事务"],"title":"【译】数据库事务隔离级别"},{"body":"","link":"https://idouba.com/tags/%E4%BA%8B%E5%8A%A1/","section":"tags","tags":null,"title":"事务"},{"body":"","link":"https://idouba.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/","section":"tags","tags":null,"title":"数据库"},{"body":"一、概述 k-means利用簇内点的均值或加权平均值ci（质心）作为类Ci的代表点。对数值属性数据有较好的几何和统计意义。对孤立点是敏感的，如果具有极大值，就可能大幅度地扭曲数据的分布.\nk-medoids(k-中心点)算法是为消除这种敏感性提出的，它选择类中位置最接近类中心的对象(称为中心点)作为类的代表点，目标函数仍然可以采用平方误差准则。\nPAM（Partitioning Around Medoids，围绕中心点的划分）是最早提出的k中心点算法之一。\n二、算法思想： 随机选择k个对象作为初始的k个类的代表点，将其余对象按与代表点对象的距离分配到最近的类；反复用非代表点来代替代表点，以改进聚类质量。 即：算法将判定是否存在一个对象可以取代已存在的一个中心点。\n通过检验所有的中心点与非中心点组成的对，算法将选择最能提高聚类效果的对，其中成员总是被分配到与中心点距离最短的类中。 假设类Ki 的当前中心点是Oi , 希望确定Oi是否应与非中心点Oh交换.如果交换可以改善聚类的效果，则进行交换。 距离代价的变化是指所有对象到其类中心点的距离之和的变化，这里使用Cjih表示中心点Oi与非中心点Oh交换后，对象Oj到中心点距离代价的变化。\n总代价定义如下：\n三、算法描述： 输入： 簇的数目k和包含n个对象的数据库。\n输出： k个簇的集合\n方法： 1任意选择k个对象作为初始的代表对象（簇中心点） 2repeat 3将每个剩余对象指派到最近的代表对象所代表的簇 4随机地选择一个非代表对象Orandom 5计算用Orandom交换代表对象Oi的总代价S 6if S \u0026lt; 0，then用Orandom替换Oi ，形成新的k个代表对象的集合 7UNTIL不发生变化 四、算法实例 样本点 A B C D E A 1 2 2 3 B 1 2 4 3 C 2 2 1 5 D 2 4 1 3 E 3 3 5 3 第一步 建立阶段： 假如从5个对象中随机抽取的2个中心点为{A，B},则样本被划分为{A、C、D}和{B、E}\n第二步 交换阶段： 假定中心点A、B分别被非中心点C、D、E替换，根据PAM算法需要计算下列代价TC(AC)、 TC(AD)、 TC(AE)、TC(BC)、TC(BD)、 TC(BE)。\n我的注解：即要尝试本聚类内部的C替换A、D替换A、E替换B以外；还要尝试用其他簇内的对象来替换现有考察的中心点。即E替换A，C替换B，D替换B。\n计算每个代价的过程也是比较繁琐的，以计算C替换A的代价TC(AC)为例说明计算过程。\n当A被C替换以后，A不再是一个中心点，因为A离B比A离C近，A被分配到B中心点代表的簇，C(AAC)=d(A,B)-d(A,A)=1 B是一个中心点，当A被C替换以后，B不受影响，C（BAC)=0 C原先属于A中心点所在的簇，当A被C替换以后，C是新中心点，符合PAM算法代价函数的第二种情况C(CAC)=d(C,C)-d(C,A)=0-2=-2 D原先属于A中心点所在的簇，当A被C替换以后，离D最近的中心点是C，根据PAM算法代价函数的第二种情况C(DAC)=d(D,C)-d(D,A)=1-2=-1 E原先属于B中心点所在的簇，当A被C替换以后，离E最近的中心仍然是 B，根据PAM算法代价函数的第三种情况C(EAC)=0 因此，TC(AC)=C(AAC)+ C(BAC)+ C(cAC)+ C(DAC)+ C(EAC) =1+0-2-1+0=-2。 即该替换方案下每个点的影响的代价和。\n在上述代价计算完毕后，我们要选取一个最小的代价，显然有多种替换可以选择，我们选择第一个最小代价的替换（也就是C替换A），根据图（a）所示，样本点被划分为{ B、A、E}和{C、D}两个簇。图（b）和图（c）分别表示了D替换A，E替换A的情况和相应的代价，图（d）、（e）、（f）分别表示了用C、D、E替换B的情况和相应的代价。\n替换中心点A\n..\n替换中心点B\n通过上述计算，已经完成了第一次迭代。在下一迭代中，将用其他的非中心点{A、D、E}替换中心点{B、C}，找出具有最小代价的替换。一直重复上述过程，直到代价不再减小为止。\n五、算法总结 优点 对属性类型没有局限性 通过簇内主要点的位置来确定选择中心点，对孤立点和噪声数据的敏感性小 不足 处理时间要比k-mean更长 用户事先指定所需聚类簇个数k 发现的聚类与输入数据的顺序无关 我的注解： 因为考察选择每个新中心点时候，不像k-mean一样，算下均值即可，需要依次考察每当前非中心点，并计算每种置换方案的总代价。算法比k-mean负责，计算时间也长。和k-mean一样。都是需要事先指定簇的个数。\n其实这也是基于划分的共同特征。\n五、基于划分的聚类算法总结 特点 k事先定好 创建一个初始划分，再采用迭代的重定位技术 不必确定距离矩阵 比系统聚类法运算量小，适用于处理庞大的样本数据 适用于发现球状类 缺陷 不同的初始值，结果可能不同 有些k均值算法的结果与数据输入顺序有关，如在线k均值算法 容易陷入局部极小值 参考： ### View [10ClusBasic.ppt][1] and other presentations by [idouba][2]. 完。\n","link":"https://idouba.com/notes-clustering-k-medoids/","section":"posts","tags":["聚类","k-medoids"],"title":"Data Mining 笔记聚类k-medoids"},{"body":"","link":"https://idouba.com/tags/k-medoids/","section":"tags","tags":null,"title":"k-medoids"},{"body":"","link":"https://idouba.com/tags/%E8%81%9A%E7%B1%BB/","section":"tags","tags":null,"title":"聚类"},{"body":"一、概念 监督式学习VS非监督式学习 Supervised learning (classification): The training data (observations, measurements, etc.) are accompanied by labels indicating the class of the observations. New data is classified based on the training set.\nUnsupervised learning (clustering):The class labels of training data is unknown Given a set of measurements, observations, etc. with the aim of establishing the existence of classes or clusters in the data –Jiawei Han\n监督式学习：提供了训练元组的类标号，通过分析已知数据，得到一个分类模型，用来确定其它的对象属于哪个类别。\n非监督式学习：不依赖有类标号的训练实例\n分类Classification predicts categorical class labels (discrete or nominal), classifies data (constructs a model) based on the training set and the values (class labels) in a classifying attribute and uses it in classifying new data。\n预测分类表示，通过分析训练集中数据的属性来进行构建一个模型来确定新的数据属于哪个分类。\n二、步骤 Model construction模型建立。 每个训练集的的元组都假设属于某个定义好的有分类标签的分类；这个模型作表现上可以是类规则，决策树或算术公式。\nModel usage 模型使用 评估模型的准确性。使用测试集(test sample)已知分类标签的样本和模型的分类结果进行比较。准确率是模型测试集合test set samples中正确分类的比率。测试集(test sample)不能和训练集(training set)相关。\n如果评估结果可以接受，则使用模型来对新的数据进行分类。\n三、分类算法类型： Decision Tree Induction Bayes Classification Methods Rule-Based Classification 完\n","link":"https://idouba.com/notes-about-classification/","section":"posts","tags":["分类","机器学习"],"title":"Data Mining 笔记之Classification"},{"body":"","link":"https://idouba.com/tags/%E5%88%86%E7%B1%BB/","section":"tags","tags":null,"title":"分类"},{"body":"","link":"https://idouba.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"tags","tags":null,"title":"机器学习"},{"body":" 无意发现研究生时候数据挖掘课程关于基于Gini Index的一个Classification的实验报告，还算完整。基于尚老师给的数据集完整完成了模型设计、训练和验证。还用Java写了个简单界面，能导入数据集训练，并画出决策树，并能导入数据集验证，评价准确性。\nA Program demonstrating Gini Index Classification Abstract In this document, a small program demonstrating Gini Index Classification is introduced. Users can select specified training data set, build the decision tree, and then select independent testing data set to verify the accuracy of model. Furthermore, by providing the decision tree visualization (Using JTree of Java), the pattern recognition capacities of users can be greatly improved.When estimating classifier accuracy, the known class label is compared with the learned model’s class prediction for that sample, the conflict records will be filtered to show user what the record’s class label is and what the mined model tells you the result is supposed should be. The program is realized by Java under Windows XP.\n1. Introduction In classification, we are given a set of example records, called a training set, where each record consists of several fields or attributes. Attributes are continuous, coming from an ordered domain, or categorical, coming from an unordered domain. One of the attributes, called the classifying attribute, indicates the class to which each example belongs. The objective of classification is to build a model of the classifying attribute based upon the other attributes. Once a model is built, it can be used to determine the class of future unclassified records. Applications of classification arise in diverse fields, such as retail target marketing, customer retention, fraud detection and medical diagnosis\nClassification is often seen as the most useful form of data mining. Although every pattern recognition technique has been used to do classification, decision trees are the most popular. This may be because they form rules which are easy to understand, or perhaps because they can be converted easily into SQL. While not as “robust” as neural nets and not as statistically “tidy” as discriminate analysis, decision trees often show very good generalization capability.\nTwo independent data set:\n• Training set: a set of examples, where each example is a feature vector (i.e., a set of \u0026lt;attribute,value\u0026gt; pairs) with its associated class. The model is built on this set.\n• Test set: a set of examples disjoint from the training set, used for testing the accuracy of a model.\nThe classification models range from easy to understand to incomprehensible are: Decision trees, Rule induction, Regression models, Genetic Algorithms, Bayesian Networks, Neural networks.\nGini index algorithm use Gini index to evaluate the goodness of the split. If a data set T contains examples from n classes, gini index, gini(T) is defined as\nwhere pj is the relative frequency of class j in T.\nIf a data set T is split into two subsets T1 and T2 with sizes N1 and N2 respectively, the gini index of the split data contains examples from n classes, the gini index gini(T) is defined as\nThe attribute provides the smallest ginisplit(T) is chosen to split the node (need to enumerate all possible splitting points for each attribute).\n2. Implementation The main procedure The data class l Record: The data to be mined, each sample of training data and testing data;\nl AttributeImp: the attribute of records, encapsulate the categorical attribute and numerical attribute;\nl DesionTreeNode: the node of decision tree. Include the decition attribute,the split point and the class value if it is a leaf, as well as the other attribute a tree node required such as parent node, children node.\nData format requirement The training data set and the testing data set should share the same data format. We load the data from a text file. Require the first line should be the attribute name of records to be mined, and the class label should be the last attribute, the word or phrase representing attribute name split with space. The attribute numbers are not limited, in theory, you can mine the data their attribute can be as large as you will, usually the record with too much attributes are not recommended. And the types of attributes are not limited. it can be numeric attributes or categorical attributes. In the program, they will be encapsulated as *AttributeImp* and all the operation upon them are identical.\nBuild the decision Tree using training data, the key part of the work! a) Main idea:\nFor each attribute, try to use each value of this attribute as split point split the record into two partition, compute the Gini, the one with the lowest Gini will be choose as the split point of this attribute.\nDo this work for each attribute, compute the Gini, the one with the lowest Gini will be choose as the test attribute. Create a node and labeled as this attribute, the records then partitioned into left records and right records accordingly, for the left records do the same work as before using the rest attributes, and then the right records.\nThe recursion will be end till the records are empty or the attributes all being used, and another case, all the samples are belong to the same class.\nIn the end of the branch, when there are no remaining attributes on which the samples may be further partitioned, convert the given node into a leaf and label it with the class in majority among the samples\nb) Argorithm:BuildTreeNode\nInput: ArrayList records //The collection of records load from the training data set)\nArrayList attributeNames // The collection attribute also load from the training data set\nOutput: DesionTreeNode root // The root of decision tree.\nMethod:\n1BuildTreeNode (ArrayList *records*,ArrayList *attributeNames*) 2splitRecords (*records*) 3 if (all points in records belong to the same class) then 4 return; 5 for each attribute A do 6 evaluate splits on attribute A using attribute list; 7 use best split found to partition records into *leftRecord* and *rightRecord* (Keep split with lowest GINI index); 8 splitRecords (*leftRecord*); 9 splitRecords (*rightRecord*); c) Procedure:\nTest the model, compute the accuracy Using the test data set which are randomly selected and are independent of the training data, Compute the percentage of test set samples that are correctly classified by the mode\n3. User Interface There is no need to say too much of how to use this small program besides pointing out the sequence of operation and the purpose of each part on the panel because it is really too simple.\n4. Testing results Training Data Set Name Training Attribute Count Training Records Count Testing Data Set Name Training Attribute Count Training Records Count Accuracy Iris_training.txt 5 119 Iris_testing.txt 5 31 93.55% heart train3-200.txt 14 200 heart test3-70.txt 14 70 81.43% credit train2.txt 14 300 credit test2.txt 14 107 89.72% We use three different data sets with separate training data set and test data set. The test results show:\nIt will become harder for the records with more attribute to achieve high accuracy compare with the one with less attributes; The more sufficient records in the training data set can help construct a more accurate model. 5. Something can be improved The built decision tree is a binary branch tree, split values of attribute into two partitions, using Gini index, in some cases it may not work well, need consider more split point be extended to handle multi-way splits or using the cluster to split the values of each attribute. No preprocess. Construct the decision tree, draw the tree, save the model in the memory, then test the model, not save the rule on persitent media such as disk file for future use. 6. Environment requirement: Java Runtime environment 1.5\n7. Conclusion The motivation for this program is an exercise of Data Ming class 2006.With the processing of develop work, deepen the understanding of the algorithm, and found some inspiration in the work.\nThe program can interpret the dataset in a text file with unlimited attributes because of using extensible data class design.\nThe framework can easily be used to other decision tree mining. The only thing need to do is to modify the built tree algorithm using the specified mining algorithm such as ID3, C4.5. The data class (Record,AttributeImp,DesionTreeNode) can be used without change. And all the procedure such as loading the training data, drawing the decision tree, loading the testing data, testing model, computing the accuracy of the model can be used without any modification.\nIn the future work, I will try to add other decision tree classification algorithm into the frame work, add the function of store the learned model(decisiont tree here)into persitent file.\nREFERENCES Jiawei Han , Micheline Kamber, Data mining: concepts and techniques, Morgan Kaufmann Publishers Inc., San Francisco, CA, 2000 Author: zhangchaomeng/ID:066100593\n源代码归档：https://github.com/idouba/classfication.giniindex\n","link":"https://idouba.com/classfication-giniindex-program/","section":"posts","tags":["机器学习","classfication"],"title":"A Program demonstrating Gini Index Classification"},{"body":"","link":"https://idouba.com/tags/classfication/","section":"tags","tags":null,"title":"classfication"},{"body":"","link":"https://idouba.com/tags/%E4%BF%A1/","section":"tags","tags":null,"title":"信"},{"body":"","link":"https://idouba.com/tags/%E5%85%B3%E4%BA%8E/","section":"tags","tags":null,"title":"关于"},{"body":"整理之前常用的网易邮箱的时候，发现几篇比较长的邮件，感受到了当年的年轻气盛，被自己小激励了一把。当年觉得很私密的邮件，现在倒愿意在自己的空间归档下,不然可能就真的扔掉了。是研究生入学前给当年向某位知名人生导师写的邮件，并未得到回复:-(。看到邮件里描述自己的还挺可爱的，虽然使劲回忆起来有些太心酸太囧的经历没有好意思写进去。但是有的啰嗦，怪不得人家不回呢，呵呵。好像更像是自己和自己说话。有热情，有干劲，但是似乎有一点偏执。现在回头看。豆妈说一定不要让我们豆豆也这么辛苦。很感激当年的这些经历，也不太愿意掖着藏着。倒是现在的自己有些地方需要重新拾起来一点当年的热情。\nXX 老师您好：\n问候您，希望不会很冒昧。\n“要做就做到最好，做到山穷水尽”，我是在一个免费的考研讲座前的一个宣传片里听到这句话的，其实应该也是因此而认识您的，准确的讲应该说 是愿意去认识您的，想着去了解和认识这个人（那个人是您吗？我身边的人讲是***，要是错了的话，就很尴尬啦）。然后居然搜到了，这样可以找到您。\n我想，您讲的我也是完全这样认同的，一直也是要求自己这样去做的。可能作的不够好。但我有一个疑问，这样做就够了吗？\n还没有自报家门，太不礼貌了。我叫***。**陕西人，**年生，应该是很大了。有些事还总是问别人，应该是比较惭愧了。\n“我没有上过大学，但我从没有放弃过学习。我现在不会，但我可以很快学会。”这是我找到最后一份工作时，在一页纸的简历上自我介绍的一部分。也许是我有胆量递上简历的时候是人才市场要散场，招聘的老师急着去吃饭，或许是他对我这个最简陋的简历好奇，在口头考了我几个问题后，给了我后面的复试和面试的机会，我也 得到了当时作为门外汉认为的很有意义也很有挑战的软件这份工作。后来这个老师也就成了我的上司，在后面我辞职时他给我的辞职报告书的回信中说，在他应聘桌 前第一眼看到我时，就认为我是一个执着刻苦的人。谢谢李老师。后面的工作我应该没有太令他失望，他也一直给我支持和鼓励。\n好了，本来只是介绍我没有上过大学，怎么扯到找工作上了，不好意思。开始吧，我应该是属于那种比较爱学习的吧。现在的我已经不再像三年前那样觉得小时侯被人 这样承认是一件很讽刺很丢人的事情，当然也不认为这就是一件完全很好的事情。是的，从村里的学前班开始到乡里的初中，不管是一级学生是村里的十几个人，还 是乡里初中的四五百人，考试时，我很少接受把第一给被别人的情况。就像现在的穆里尼奥的切尔西（那时候是穆里尼奥的切尔西一代，比现在要强盛、性感！）一 样，我觉得自己做的很稳健。但却仅仅是在考试上，其他的，我感到自己就像个傻子一样，小时这样认为，现在还是这样认为。\n所以，当初中毕业时考上了中专，我就很光荣的去了，我们初中只有第一二名才过线可以上的。包里还装着我的奥数的书和很多的憧憬。也很有意思，去另外一个城市报名的路上，半夜，临时停车，包被人从车窗外拽走了。一堆精攒的好书就这样没了，爸爸说可惜是个新包，不过幸好通知书不在里面，我却心痛极了。\n这可能也是我后面四年的基调，失落，空虚。我学的是机械维修，就是修车床，一些机加工之类的。本应该是可以学到一些东西的，可能是因为发现这儿讲的英语比初中的还简单，好多课程与自己想象中的完全是两回事，还有与想象中完全不一样的氛围，尽管很容易可以考个好成绩，拿到奖学金，但我从来没有用心投入其中。现在想想，当时确实可以做一些事情。\n就这样，我的15-19岁就这样过来了，毕业后，我们省的都被分配到铁一局，在工地管工程机械和仓库。工程单位是没有周末的，下大雨等天气影响工地不能干活的时候是我们休息的时候。其实我挺喜欢那样的工作的。有一次，替一个大学生的同事搬床时，他的一本要扔的大学英语单词书被我捡了，当时也没想为什么，后来的大半年里，在我用的帐本上，把那书抄了一遍。我给自己开了个头，我居然对这些东西有感情了。过年回家，把弟弟高中的语法书，就是为了高考编的那样的较综合的那种，分单元的，看了一遍，居然都可以看的懂。\n在东北呆了两年多，那个桥修完了，我们也各自回家了，有活再召集。这也是我们的工作方式。一起长大的村里的好朋友，这时候已经上大学了。一番谈话，建议我把学习捡起来，他促使我下 决心了。（应该比较准确这样说，因为我也曾想过）。一个在外面上班的人一直呆在家里是很丢人的。过年后一个月，我就到县城的阿姨家，给她家上学的小学的弟弟和在高中上学的弟弟做饭，然后就是去西安，好朋友那里，了解能不能找一个学校。从五月中旬到六月中旬，记得很清楚，我开始读好朋友俊卫给我买的大学英语1-4册和一本牛津词典（4 元4+25元1，都是旧书）。第一天，我看了第一课A，B只看了一点，但是在晚上睡觉前，我还是认为都看过一遍，也都看懂了，可以安心的睡了。后面的 每一天，把单词记在小本上，后面的练习，会做不会做的都填上了，看图作文也蒙人似的写几句。我坚持下来了，三十天，不间断的，大学英语精度三本书就这样囫囵吞枣地硬灌进去了，或许这是我后面英语的基础，或许是全部。因为第四册，我只是在一年才翻了一遍，而后来买的五六册，现在只是当枕头用着了。我很怀念那一个月，也很感激那一个月，专注地只做一件事，原来屁股真的 可以坐破，坐的流血，可能也是阿姨家的木凳子太硬了吧。\n在那个暑假，就是好朋友俊卫他们学校放假的时候，他骑自行 车带者我在西安各个学校里打听哪个成教之类的地方可以要我。当时对西安不太熟悉，也不知道方位，但像体育场这样路过的大点的地方还是记住了，也成 为记忆中很有感情的一部分，俊卫在一路上鼓励我的话也随着那个地方而在我的记忆中占据一定的位置。\n最终在西安开始了直到现在为止的另一种生活。租最便宜的房子，月租79元。白天找个事情做，晚上回来看书。但不得不羞愧的讲，开始的半年，我是没有能够实 现自己的那种半工半读的生活，因为我找不到工作。好在，靠着当时修铁路工地上班时有点积蓄和最低的生活成本，我活下来了。\n02 年4月，我报了《计算机通信工程》本科的自学考试，算上要求加考的共19门课，03年10月，我完成了。其中四门以上都是纯数学的东西，我想我是啃过来 的,第一二遍是一点不懂的。02 年中和末我完成了CET4和CET6，可能对于普通学生来说这是再正常不过的事情了，可是还是让我很兴奋，用俊卫的话讲，是收获了应有的自信。在这段时间结束了自学的本科课程，也拿到了学位。计算机和通讯的课程虽然高分完成了考试和毕业，但是基础东西理解还是有限，就开始了另外一种方式的学习。\n当时工作是跑业务给饭店美容院这些地方推销卡片，经常报完到，上午出门，在解放路的图书大厦泡大半天，从这本书了解到这个，看不懂，再去看另外的书，这本书讲到这个，就去这方面的详细介绍的书中去找，后来发现，这方面的那一片的书都被翻过了。 但是还一样没有太多实际的操作，只是停留在知道，或是一点的深入了解。后面的工作中还是从这段积蓄中受益非浅，也很感激自己那样不懂却硬着头皮纸上谈兵的 精神。到现在，有了实践和一点经验了，却没有热情去把理论再升华一下了。在书店里不让抄书，没法用笔做笔记，有些东西就用手机存成短信，回来再整理到纸 上。有时无意中看到电话里的已存短信，还挺佩服这些小聪明。尽管可以被算做不认真上班，可我的业绩还是几个同事中最好的，因为我愿意多跑，而且每天在书城 呆的时间总是在一个限度内。后来辞职走时，领导给我的电话里多交了100块钱的话费，说是给我工作的一个奖励吧，当时的工资是500块。\n应该是一个很偶然的机会，我撞进了软件公司。其实很正常，和从前修桥比没有什么不一样。都是干工程，好多理论好像也很像。过程的、质量的、进度的、组织的。但是作为梦想中追逐的东西还是很激动的。我租房子的隔壁是个在校外住的大学生，他毕业找工作，说参加西安软件园的一个招聘会，临时就拉我做伴。几分钟写了一个自我介绍放在U盘里，没有打印，因为也没想到会用上。在中午散场人都走的差不多的时候，磨磨唧唧羞羞答答终于有勇气把刚打印的还热着的一份递给了李老师，他给了我机会。\n工作很忙，正好那一年，我们组平均都要加班到10点，工作氛围，习惯与以前的单位都很不一样，我们组是在日本的公司里办公，做事的方式和思路还是有些新的体会和收获，实实在在的有收获的。尽管一直或者说永远我不会对日本人有好印象。\n七月进公司，第二年的四月，我给李老师的邮箱递交了辞职报告，我想考研。领导们说，像我的工作热情和学习能力，不用考研一样可做的很好，同时也告诉我以我的基础，难度应该很大，鼓励我做一个长远的计划和大打算。其实，我心里也很没有底，可我还是有一点认识，就是开始一件事情而中途放弃比撞的头破血流后失败更失败。我不允许有这样一个先例，然后养成这样一个习惯。因为任何事情，退一步，退回来，总是很容易，很舒服，至少当时会是。我其实也开始考虑了，不用交违约金，不用改变好不容易得来的还算舒适体面的工作现状。\n但我还是决定要走，不想有个这样一个退缩的记录，即使定位为战略撤退也不行。但我的心里真的没底，其中的几门数学课，有一门线性代数从来没有学过，另外两门高数和概率统计简单看过，但是很浅的，只是应付自学考试这样的简单考试。专业课，我还不清楚报哪个学校，当然也不知道，大部分是没有学过的，政治，好几门，哲学这些连内容是什么也不知道。英语，我的那些土办法学 出来的能应付吗？而且，还荒芜了很长时间了。总之，所有的东西，很多还没有概念。不过最要紧的恐怕是时间了，我的安排是最晚五月开始，可八月了我还在公 司，别人是复习，我是全新学习，我的定位只能是今年为明年做一些准备了，把数学学完，巩固到要求的程度。考完再找工作，到明年这时再辞职，重新来过应该比较有把握。\n05年9月1日，我的辞职批准了。一个人的学习，安排进度，对效果的考察。我认为数量是比质量更好考察的一个指标。要质量，可能会在一个点上，一直停滞，反复，有最终不能完成的风险。从整体上再看质量不完整，当然质量不好了。而数量，按时间安排进度，只要数量作到了，或者是作到了压倒性的数量，质量应该是不用太担心的。而且每天只是简单的要求我要做作业1、作业2、作业3总比空洞的说我要作到 程度1、程度2、程度3来的容易。我以前各个阶段自学的过程都是这样做的，有个详尽的可量化的计划，每天作完即可，剩下的就关心不到了。\n开始几本书， 第一章一点也看不懂，根本找不大能看懂的切入点。就尝试从中间部分个别能看懂的地方看，总之是把一本书看完为止，因为并不是每本书都是严格的循序渐进的、由易到难的。对于自学者，我认为很重要，只要的是先围歼，可以多看几遍，后面就相对很容易了。有种自信，发现对那些书开始有感情了，剩下的事就很容易了。每本书，读过的就记笔记，尤其是看不懂的书，可能是读的时候不够细，就一点一点记下来，越不懂，笔记可以记的越详细。先混个脸熟，培养一点感情，也是蚕食战略吧。而且复习时，看着自己整理出来的笔记总比去翻书要有效的多，自己的写的字总比印刷的字亲切。\n每 天都有一个计划，复习更可操作一些，复习也更从容，只关心把当天的事情做完，而不用考虑其他，使操作更简单，不用一个人发楞，没有概念的瞎想，吓自己。9 月有一个计划到10月16号，是一个很细的计划，作的很有效率，很忙，当然都很忙，但不乱。10月下旬到11月，只有两行字的很粗计划使我的复习失控了，一个人容易发愣，时间过得很快。 要求没有达到，每天还是很辛苦，心里总是很惶恐，复习几乎成了问题。每天都会简单记下完成情况和一点心得，那卷笔记本现在找不到了，后面看过一次，体会当时的疯狂，很过瘾。 四个月的考研课程自学，用了七八十个多个油笔芯和近70作业本，时间利用的应该很充分。但对自己的程度不满意，总是重复出现前面犯的错，尽管将所有数学出错的地方都有一个checklist对照。\n考试完后的感觉，比想象的最好的要差，比最差的要好。\n三月，俊卫从日本给我打来过电话，我们早就约定，我只负责完成任务，剩下的他负责。409，政治82、英语82、数学116、专业课129，他告诉我，在那边狂喜，而我却在怀疑。是真的，他把成绩的那个结果页面发到我的邮箱里了，还有一堆鼓励的话，就像这些年他一直做的一样，很让人振奋的话，总结了我们一起商量的道路的每一步前进。谢谢你，我最好的朋友，我把我的这点小成绩的一半要分给你。\n然后是我想到的其他的，随之想到的。（也是我最迷茫的，想到请教您的)我的成绩，我只有成绩，就像小时侯一样，好的成绩总是让人兴奋，也会有一丝成就感，但那就是全部吗？这就是我全部追求的吗？我有些迷茫，其实这个问题以前也考虑过。\n很讽刺，我已经XX岁了，还没有想好自己要做什么？我在追求什么呢？我可以在一个较短的时间内把一个明确的任务作的尽可能的好，尽管可能不能想您说的作到极至。无论其中困难有多大，总会使之有个结果。\n总可以不错的完成一些短期的目标，制定和实施一个短期的计划。但却我没有长远的计划，这应该是很危险的，就像一个曲线一样，在时间的坐标上总是在前进，每个短的阶段不错，但总的趋势呢？没有一个明确的目标，绕弯路，可能会走上岔路。\n我是想好好利用这样一个学习的机遇，但我做好准备了吗？有些心虚。下个月就开学了。还会和我当时上中专时一样吗？我已经习惯了自己学习，也不知道学校里的学习能否适应。 看着很能学，只有初中文化，靠自学在铁路工地的账本上抄一本捡来的单词书，在亲戚家的木凳子硬坐一个月硬灌了大学英语课程，混过了英语、英语四级、六级。靠跑业务的间隙蹭图书馆，入门初步贯通了计算机相关基础。现在（算高分）公费考到了西工大计算机学院。但不知收获了什么， 难道就是那一堆毕业证破纸吗？单位要求学日语，我们就学，我挺爱学，可是我的中国鹰派情结，当然反日了，我又想学德语或法语。但是每当心里有这个“我想 学”几个字时，总是条件放射的有另一种声音，这么大了，学学，还会点别的吗？是，不应这样过了，可我挺喜欢的，我喜欢这种极致投入并有所成的感觉，其实我不是那种聪明 的人，但我愿意投入的去做一件事情，做出一个结果来。但做什么呢，总是很迷惑。没有事做会空虚，有事作完了，一点成就感，然后就失落，空虚。总是做一些看上去很美的事，也就是朋友，父母认为很对很上进的事情，这个真的就对吗？今年三月，我回到了原来的软件公司，平时上班，挺好，周六周日是我最难过的。\n希望能得到您的一些指点。我相信您能给我一些帮助。算是感觉吧。我说的有些多了，只是想让您更多的了解这个人，如果耽误了您太多时间，我先表示歉意了。我已经尽可能讲的详细了，包括自己的一些想法，向您暴露思想中的一些问题。当然如果 还有些问题，我很愿意回答您。我会很珍惜这次学习的机会的。对我很重要，应该是很关键。谢谢您，谢谢您和您的同事们的工作，很佩服你们！ 好了，期盼回音。\n最近有九天的假，可以拿出半天的闲暇来总结一下自己。 20060808\n我的电话：*****\n我的邮箱：*****\n工作邮箱(即时收信的)：*****\n注：我希望不要公开我的信，因为这不完整。我是不会向您写这封信让您认识我的，尽管从去年冬天就有这个冲动要给您写信的。若您的规定必须要，就请忽略这封信吧。\n完。\n","link":"https://idouba.com/history-letter-to-someone-mentor-so-called/","section":"posts","tags":["信","关于"],"title":"给某某导师的一封邮件"},{"body":"收拾之前的东西，不小心看到了一张纸条，是自己写给自己的。有点小感慨，记录下。\n那时候刚开始工作吧，在沈阳的辽中县修桥，好像是从东北回家的路上，在北京的西单图书大厦泡了一天，买了这样一本书。当时看了，技巧倒没有得到什么，但是好像很受激励（年轻时更容易被激励吧），就下决心，真的下决心拿张破纸记下了目标，有点狂妄，当时不知道四级六级具体是啥，只知道六级比四级难，目标就写个大的。\n说实话，读的那本书里面的内容已经完全不记得了，书好像也早都不在了。只是从那里面第一次听说了VOA Special English。后来就从一个还在念大学的好朋友的同学那儿拿到一个破收音机（同时还四块钱一本的从他们学校后门的旧书摊上买了四本大学英语的课本），居然收到那个台了，每天早上七点好像（那时候一直听，里面每周的bill white这些人的声音和风格都能分辨出来，到后来的standard english）现在已经时间都不记得了。唉。\n和其他一些年少时候自己写的日记一样，这些纸条，一直这样收拾着。但是当时却应该是起到作用了。没有上过高中，也没有上过大学，但是确实做到了在一年里过了四级又过了六级。记得六级报名的时候是在西电的大礼堂，大清早从南郊坐教育专线过去（那时候没有google地图，只知道好像在那趟公交站的附近），队伍弯弯曲曲的把礼堂前都占满了。好像是03年吧，或者是02年，那些证现在是在抽屉最下面了，懒得去再翻一下（现在怎么这么懒呢！）。如果是02年就按时完成目标，如果是03年就delay一年。回想起来了，后来读研究生的时候NWPU里规定过了六级可以免修英语，我的因为过得太早，被拒绝免修。唉，忘了当时的政策是几年之内可以免修了。得，也推算不出来。\n本来只是想拿之前的一点小回忆激励现在懒惰的自己一把，激励没有达到，只是更衬托自己的懒惰了！\n豆豆已经一岁了，妈妈有时候会给你讲爸爸原来多么厉害，没有上过大学，十几岁就去工地上班修桥。后来自己为了学习辞了工作在西安跑业务卖东西，一个人考那些自学考试，考了专科又考了本科，后来混到了软件公司里，工作了一年又去考研，线性代数，高数啥也不懂，从九月辞职到元月考试的四个半月里上了个辅导班加上自己上自习还被他四百零九考了个公费，比妈妈和她的同学正经大学毕业的还要高。可爱学习了，可爱工作了。妈妈就是爸爸的研究生同学。\n在这几年忙忙碌碌混混沌沌的的过每一天的时候，这样偶尔翻起点东西，让自己还是小鼓舞一下。“哦，我还行！”，尤其是豆豆最近总是崇拜的把遥控器递到爸爸手里，让爸爸帮你做些你做不到，并且你认为妈妈也做不到的事情时。爸爸必须行！和我们豆豆一起加油，一起还要成长！\n爸爸不喜欢豆豆过得和爸爸之前一样辛苦。但是爸爸必须再努力一点。爸爸喜欢并且享受努力的感觉，不喜欢懒散（妈妈推崇的悠哉）的感觉。同时回想起来之前总是愿意把目标写在小纸片里，后来发现居然都做到了，有些现在自己是不太敢相信的。爸爸希望自己可以有点像之前的自己，虽然妈妈说爸爸已经很努力了，在现在这个年龄，哈哈，听出来没，不是夸奖，是讽刺呀。呵呵，爸爸喜欢这样。照顾好豆豆和妈妈，花时间陪豆豆和妈妈，同时努力做事。爸爸应该更努力些。和之前一样敢想敢做！\ndoudou：baba baba，ni yao qu na li ya？\nbaba： fighting, fighting, whereever you want，with you an mum！\n就写这些吧。\n","link":"https://idouba.com/doudou_history_letter_self_motivated/","section":"posts","tags":["信"],"title":"一张纸勾起的回忆"},{"body":" idouba，爱豆吧！\n爱着豆豆的爸爸，一个叫豆豆的小男孩的爸爸。\n生在关中塬上，\n居于钱江南岸。\n这些十年一直在菊厂搬砖。\n曾经那些年在铁一局修桥筑路，搬真的砖。\n在豆哥出生时，觅得这样一处空间。\n想帮豆哥记录点东西，\n想记录豆哥点东西。\n豆哥一天天长大，\n豆爸一天天忙东（忙）西，\n也没有记录下啥东西。\n","link":"https://idouba.com/about/","section":"posts","tags":null,"title":"关于idouba"},{"body":"","link":"https://idouba.com/tags/english/","section":"tags","tags":null,"title":"English"},{"body":"","link":"https://idouba.com/tags/gre/","section":"tags","tags":null,"title":"GRE"},{"body":"[在寄托的GRE作文练习 档下。\n第二次作业：（20070727） Issue 88: http://bbs.gter.net/bbs/viewthread.php?tid=710682\u0026amp;extra=page%3D1 Argument 26: http://bbs.gter.net/bbs/viewthread.php?tid=710693\u0026amp;extra=page%3D1\n第三次作业：（20070728） Issue 159: http://bbs.gter.net/bbs/viewthread.php?tid=711072\u0026amp;extra=page%3D1 Argument 147: http://bbs.gter.net/bbs/viewthread.php?tid=711321\u0026amp;extra=page%3D1\n第四次作业:（20070729） Issue 177:http://bbs.gter.net/bbs/viewthread.php?tid=711906\u0026amp;extra=page%3D1 Argument 17: http://bbs.gter.net/bbs/viewthread.php?tid=711910\u0026amp;extra=page%3D1\n第五次作业:（20070730） Issue 59 :http://bbs.gter.net/bbs/viewthread.php?tid=712537\u0026amp;extra=page%3D2 Argument 51:http://bbs.gter.net/bbs/viewthread.php?tid=712541\u0026amp;extra=page%3D1\n第六次作业:（20070731） Issue 143 : http://bbs.gter.net/bbs/viewthread.php?tid=713063\u0026amp;extra=page%3D1 Issue 43 :http://bbs.gter.net/bbs/viewthread.php?tid=713593\u0026amp;extra=page%3D1 Argument 2: http://bbs.gter.net/bbs/viewthread.php?tid=713213\u0026amp;extra=page%3D1\n第七次作业:（20070801） Issue 56 : http://bbs.gter.net/bbs/thread-713667-1-1.html Argument 140: http://bbs.gter.net/bbs/thread-713668-1-1.html\n第八次作业:（20070802） Issue 147 :http://bbs.gter.net/bbs/viewthread.php?tid=714625\u0026amp;extra=page%3D1 Issue 51 : http://bbs.gter.net/bbs/viewthread.php?tid=714563\u0026amp;extra=page%3D1 Argument 65: http://bbs.gter.net/bbs/viewthread.php?tid=714619\u0026amp;extra=page%3D1\n第九次作业:（20070803） Issue 207 :http://bbs.gter.net/bbs/viewthread.php?tid=715128\u0026amp;extra=page%3D1 Argument 117 :http://bbs.gter.net/bbs/viewthread.php?tid=715261\u0026amp;extra=page%3D1\n第十次作业:（20070804） Issue 144:http://bbs.gter.net/bbs/viewthread.php?tid=716645\u0026amp;extra=page%3D1 Argument 50：http://bbs.gter.net/bbs/viewthread.php?tid=716918\u0026amp;extra=page%3D1\n第十一次作业:（20070805） Issue 48: http://bbs.gter.net/bbs/viewthread.php?tid=716588\u0026amp;extra=page%3D1 Argument 167：http://bbs.gter.net/bbs/viewthread.php?tid=716479\u0026amp;extra=page%3D1\n第十二次作业:（20070806） Issue 36:http://bbs.gter.net/bbs/viewthread.php?tid=717141\u0026amp;extra=page%3D1 Argument 142 http://bbs.gter.net/bbs/viewthread.php?tid=717606\u0026amp;extra=page%3D1\n第十三次作业:（20070807） Issue 130:http://bbs.gter.net/bbs/viewthread.php?tid=718191\u0026amp;page=1\u0026amp;extra=page%3D1#pid1770680380 Argument 174：http://bbs.gter.net/bbs/viewthread.php?tid=718192\u0026amp;extra=page%3D1\n第十四次作业:（20070808） Issue 93:http://bbs.gter.net/bbs/viewthread.php?tid=719484\u0026amp;extra=page%3D1\n第十五次作业:（20070809） Issue 41:http://bbs.gter.net/bbs/viewthread.php?tid=719481\u0026amp;extra=page%3D1\n第十六次作业:（20070810） Argument 150：http://bbs.gter.net/bbs/viewthread.php?tid=720162\u0026amp;extra=page%3D1\n第十七次作业:（20070811） Issue 185：http://bbs.gter.net/bbs/viewthread.php?tid=720720\u0026amp;extra=page%3D1 Argument 15:http://bbs.gter.net/bbs/viewthread.php?tid=720794\u0026amp;extra=page%3D1\n第十八次作业:（20070812） Issue 120：http://bbs.gter.net/bbs/thread-721968-1-1.html\n第十九次作业:（20070813） Argument 144:http://bbs.gter.net/bbs/thread-721966-1-1.html\n第二十次作业:（20070814） Issue 50：http://bbs.gter.net/bbs/viewthread.php?tid=722589\u0026amp;extra=page%3D1\n第二十一次作业:（20070816） Issue 38：http://bbs.gter.net/bbs/viewthread.php?tid=723575\u0026amp;page=1\u0026amp;extra=page%3D1#pid1770732625\n第二十二次作业:（20070818） Issue 83：http://bbs.gter.net/bbs/thread-725332-1-1.html\n第二十三次作业:（20070820） Issue 17：http://bbs.gter.net/bbs/viewthread.php?tid=726045\u0026amp;extra=page%3D1\n第二十四次作业:（20070821） Issue 54：http://bbs.gter.net/bbs/viewthread.php?tid=726406\u0026amp;extra=page%3D1\n第二十五次作业:（20070823） Argument 128：http://bbs.gter.net/bbs/viewthread.php?tid=727753\u0026amp;extra=page%3D1 Argument 108：http://bbs.gter.net/bbs/viewthread.php?tid=727768\u0026amp;extra=page%3D1\n第二十六次作业:（20070824） Argument 133：http://bbs.gter.net/bbs/thread-728236-1-1.html Argument 134：http://bbs.gter.net/bbs/thread-728230-1-1.html\n第二十七次作业:（20070825） Issue 208：http://bbs.gter.net/bbs/thread-728774-1-1.html\n第二十八次作业:（20070826） Issue 47：http://bbs.gter.net/bbs/thread-729240-1-1.html\n第二十九次作业:（20070827） Issue 11：http://bbs.gter.net/bbs/thread-729724-1-1.html Issue 141：http://bbs.gter.net/bbs/thread-729754-1-1.html\n模考开始： 第三十次作业:（20070828） Issue 70：http://bbs.gter.net/bbs/thread-729935-1-1.html Argument 45：http://bbs.gter.net/bbs/thread-729936-1-1.html\n第三十一次作业:（20070829） Issue 4：http://bbs.gter.net/bbs/thread-730492-1-1.html Argument 73：http://bbs.gter.net/bbs/thread-730496-1-1.html\n第三十二次作业:（20070830）Issue有些点没有写上，头重脚轻 Issue 180：http://bbs.gter.net/bbs/thread-730920-1-1.html Argument 47：http://bbs.gter.net/bbs/thread-730921-1-1.html\n第三十三次作业:（20070831） Issue 196：http://bbs.gter.net/bbs/thread-731618-1-1.html Argument 212：http://bbs.gter.net/bbs/thread-731625-1-1.html\n第三十四次作业:（20070831） Issue 95：http://bbs.gter.net/bbs/thread-731632-1-1.html Argument 164：http://bbs.gter.net/bbs/thread-731633-1-1.html\n第三十五次作业:（20070901） Issue 154: http://bbs.gter.net/bbs/thread-731963-1-1.html： Argument 145：http://bbs.gter.net/bbs/thread-731967-1-1.html\n第三十六次作业:（20070901） Issue 74：http://bbs.gter.net/bbs/thread-731970-1-1.html Argument 175：http://bbs.gter.net/bbs/thread-731972-1-1.html\n第三十七次作业:（20070902） Issue 138：http://bbs.gter.net/bbs/thread-732231-1-1.html Argument 194：http://bbs.gter.net/bbs/thread-732232-1-1.html\n第三十八次作业:（20070902） Issue 30：http://bbs.gter.net/bbs/thread-732441-1-1.html Argument 77：http://bbs.gter.net/bbs/thread-732444-1-1.html\n第三十九次作业:（20070903） Issue 112：http://bbs.gter.net/bbs/thread-732690-1-1.html Argument 203：http://bbs.gter.net/bbs/thread-732693-1-1.html\n第四十次作业:（20070903） Issue 25：http://bbs.gter.net/bbs/thread-733167-1-1.html Argument 5：http://bbs.gter.net/bbs/thread-733170-1-1.html\n第四十一次作业:（20070904） Issue 13：http://bbs.gter.net/bbs/thread-733168-1-1.html Argument 51：http://bbs.gter.net/bbs/thread-733171-1-1.html\n第四十二次作业:（20070905） Issue 69：http://bbs.gter.net/bbs/thread-733796-1-1.html Argument 199：http://bbs.gter.net/bbs/thread-733798-1-1.html\n第四十三次作业:（20070906） Issue 99：http://bbs.gter.net/bbs/thread-734186-1-1.html Argument 203：http://bbs.gter.net/bbs/thread-734188-1-1.html\n第四十四次作业:（20070906） Issue 121：http://bbs.gter.net/bbs/thread-734190-1-1.html Argument 70：http://bbs.gter.net/bbs/thread-734192-1-1.html\n","link":"https://idouba.com/my-gre-index-issue-argument/","section":"posts","tags":["English","GRE"],"title":"GRE作文目录"},{"body":"在国米百年诞辰的时候写给俱乐部的一封信。\n从1998年开始关注追随国际米兰，注册了俱乐部会员。每个生日的前夕都会受到一个合成背后印着名字，号码是当时年龄的蓝黑间条衫。\nHi Inter, HAPPY BIRTHDAY !!!\nI am an Inter fan of China, a postgraduate student of a University in the Northwest of China. I have been the fan of Inter for 12 years. Football was not as popular as it is now in China 12 years ago. I have to admit that one critical reason why I have deep feeling of Inter rather than AC Milan which was highly regarded by lots of my friends is because of Ronaldo. But I find my love did not pale when he move to Real Madrid, maybe I love Inter more.\nI always told my friends that I can share some special feelings, some character with Inter. We are persistent, working hard, and have a dream of great success, but I have to say we are less confident in a long time.\nThe most impressive moment in my memory is the game Inter lose to Juventus in 1998, also it is the worst moment I can remember. I felt being greatly hurt when the referee refused to give penalty when Ronaldo was obviously offended in penalty zone, which eventually lead to the losing of the champion of that season.\nI will never hesitate to say Zanetti if I was asked to tell my favorite plays in Inter. He is the captain of Inter, and has been and will always be a captain in my heart. He is a real man, a nice man no matter on the pitch or off pitch. His performance on the pitch and his charity things all make him a model for the professional players, for the common people like me. I feel closer with him when hearing his regards to the Chinese Inter fans in this Chinese Spring Festival, it just like a conversion with him, so warm. I like him and his Argentina national team just like I love Inter. My favorite national teams are Argentina, Dutch and Italy. It has always been my dream that Zanite captain the Argentina to win the World Cup in 2010. I cried when they had a draw with Sweden and stopped entering the next round in 2002. I was made angry when he was refused to German in 2006 because of some ridiculous reason. I hope I can see him in Beijing Olympic game.\nAnd another one is Roberto Baggio, he is an artist on the pitch, though he didn’t stay a long time in Inter, the picture that he is in Inter shirt is always set as my desktop of my computer and the icon of MSN.\nOne interesting thing is that my birthday is recorded as January 22 when I registered as a member in the official website of Inter, it was the day calculated by Chinese traditional calendar, and it should be March 8 when calculated in Gregorian calendar. I strongly apply to change my birthday to March 8 and then I can receive the present from Inter the day before the birthday of Inter. Is there anything greater than this?\nThe following is my birthday letters from Inter in the past several years. Now let me say HAPPY BIRTHDAY!!! Inter!!\nHi PINUO,\nhttp://www.inter.it/cgi-bin/service/jump.cgi?name=\u0026amp;#038;eta=24th\nHi PINUO,\nhttp://www.inter.it/cgi-bin/service/jump.cgi?name=\u0026amp;#038;eta=25th\nHAPPY BIRTHDAY!!!\nHi pinuo,\nHAPPY BIRTHDAY!!!\nHi pinuo,\nHAPPY BIRTHDAY!!!\nHi pinuo,\nTo receive the birthday letter has become part of my life, which told me I am one years older, and it is time to review what I have done last year and to make a plan for the next year.\nI am always image what I will look like if I receive the shirt from Inter with Veri’s number 32. What my life will be, my work, my family. If I have a kid, I will show him(her or them) my shirt.\nWe have to work hard and show our courage especially when we are in adversity. My heart will be with you in the game against Liverpool at the Stadio Meazza. Beat them as we do in 1965. I have faith in you!! ","link":"https://idouba.com/happy-birthday-inter/","section":"posts","tags":["足球","国际米兰"],"title":"HAPPY BIRTHDAY, INTER"},{"body":"","link":"https://idouba.com/tags/%E5%9B%BD%E9%99%85%E7%B1%B3%E5%85%B0/","section":"tags","tags":null,"title":"国际米兰"},{"body":"","link":"https://idouba.com/tags/%E8%B6%B3%E7%90%83/","section":"tags","tags":null,"title":"足球"},{"body":"","link":"https://idouba.com/tags/japanese/","section":"tags","tags":null,"title":"japanese"},{"body":"","link":"https://idouba.com/tags/%E3%81%AB%E3%81%BB%E3%82%93%E3%81%94/","section":"tags","tags":null,"title":"にほんご"},{"body":"那时候每天早上都要在办公室用日语朗读一篇文章(后来我们小组要求背诵了)，对学习日语确实有很大帮助。平时也就和合作的日本team那边的人写邮件讨论功能，开始大部分句式也都是从senior的人那儿拷贝来，替换中间的词汇来表达。包括入职才一个多月就写的几十页的功能文档，都是这么干的。每天轮流一个人来朗读一篇文章倒确实对大家的口语锻炼有挺多好处的，尽管我们这些当时年龄资历都很不够的人，一般在两周多的项目总结会上，一般也就只是听，没有说的份。做多每次会前，逐个自我介绍，用日语自我介绍下，我叫什么，负责那个模块的，请多指教云云。大组十几个人，有二十个吗？好像有二十个人。一般也就两三周轮到一次。\n在第三次，还是第二次轮到我的时候，发表了这篇文章。原文来自大组的一个同事从日本出差回来组内群发的一份报告（这个邮件组没有日本人）。\n发表后当天就被课长约去谈话了。规劝，警告。\nTO: 李課長様（りかちょうさま）及び（および）皆様（みなさま）\n先日（せんじつ）仕事（しごと）のチャンスで、有難く（ありがたく）友邦（ゆうほう）の日本国（にほんこく）へ出張（しゅっちょう）に足（あし）を運（はこ）びました。\n滞在期間（たいざいきかん）がすごく短（みじ）かったですが、初めて海外出張（かいがいしゅっちょう）なので、色々と良い（いい）勉強になりました。\nもちろん今は言いたい事も一杯（いっぱい）です。帰ったら、友達同士（ともだちどうし）に誘（さそ）われて、BBSに自分（じぶん）の感想（かんそう）をアップしました。\n大変恐れ（たいへんおそれ）を入（い）りますが、一応（いちおう）この場（ば）を借（か）りて、日本で見たり感（かん）じたりした事を言わせて頂（いただ）きます。\n一番（いちばん）気（き）になるのはやはり我々（われわれ）中国人は軽蔑（けいべつ）されるということです。\n身の回り（みのまわり）の日本人は礼儀正しく（れいぎただしく）見えましたが、強国（きょうこく）なりの自慢根性（じまんこんじょう）が明（あき）らかに感（かん）じました。\n彼ら（かれら）たちの目では、中国人のプログラマーはコードの編集能力（へんしゅうのうりょく）やソフトウェアの設計（せっけい）などいずれも日本人と比（くら）べて、桁（けた）が違う（ちがう）です。\nでも、優（すぐ）れた中国人のスタッフが会えば、全く（まったく）見下（みさ）げるというわけはないです。\nご周知（しゅうち）の通り（とおり）、ソフトウェアの開発（かいはつ）はチームワークを強調（きょうちょう）するものですが、時にはこのチームワーク精神（せいしん）はずいぶん弱いと思われます。\n特（とく）に自己中心主義（じこちゅうしんしゅぎ）と集団主義（しゅうだんしゅぎ）がぶつかってしまう時は、後者（こうしゃ）は手を上げて（あげて）悲嘆（ひたん）したケースも少（すく）なくないです。\nそれこそ日本のソフトウェア開発の弱（よわ）みだと認識（にんしき）されました。\nなお、一ヶ月（いっかげつ）の聞いたり見たりしたことでは、我々（われわれ）中国人は先進国（せんしんこく）を学ぶ（まなぶ）べきことは山積み（やまづみ）です。\n技術能力（ぎじゅつのうりょく）でも仕事（しごと）のやる気（き）でも、友邦（ゆうほう）と大きな差（さ）があります。\n次回（じかい）皆様が日本で出張（しゅっちょう）に堂々（どうどう）と出（で）かけるために、頑張りましょう！\n最後（さいご）に、67年前（まえ）の本日（ほんじつ）を記念（きねん）致（いた）します。\nTO: 李科长 ，各位\n前些天因测试工作的需要到府中出差，待了一段时间。因为是第一次出国，俨然一沟里娃进城，见了一下世面，自然也小发了些感慨。回来后便被揪住，说是为了资源共享，便于经历继承，挤上一篇感想，先。当然，也是要借此机会与大家唠一下遇到的问题。\n**感受最深的是日本人其实看不起中国人。**他们表面的谦卑掩饰不住自己强国的自豪，在他们看来，中国的程序员不论是写代码的水平还是软件结构的设计能力都不如日本人，不过，他们的看不起还算不上是傲慢，只要比他们强，自然不会被小觑。\n其次，软件开发是团队合作的过程，某些时候团队精神是脆弱的。当自我主义与团队精神发生摩擦时，后者自然会发出既生瑜何生亮的悲叹。这也可能就是日本软件的致命缺陷。\n再次，通过一个月的观察发现，发达国家是有很多东西值得我们学习和借鉴。平均来讲，不论是技术实力还是工作精神，我们都与之有一定的差距。为了下次出差的同志挺直腰板 — 加油。\n最后，纪念67年前的今天。\n注：今天指九一八。\n","link":"https://idouba.com/daily-japanese-speak-in-office-in-chinese/","section":"posts","tags":["japanese","にほんご"],"title":"一次日语朝礼"},{"body":"","link":"https://idouba.com/series/","section":"series","tags":null,"title":"Series"}]