<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 爱豆吧！</title>
    <link>https://idouba.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 爱豆吧！</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>浙ICP备18050493号-1 浙公网安备 33010802006262号</copyright>
    <lastBuildDate>Wed, 02 Oct 2024 15:32:08 +0000</lastBuildDate><atom:link href="https://idouba.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>云原生工程师初识深度学习系列（一）：从线性回归入门神经网络</title>
      <link>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</link>
      <pubDate>Wed, 02 Oct 2024 15:32:08 +0000</pubDate>
      
      <guid>https://idouba.com/cloud-native-engineer-learn-deeplearning-session1-from-linear-regression-to-neural-network/</guid>
      <description>
        
          
            背景 最近团队的业务除了面向通用计算外，越来越多的需求要处理面向AI场景的软硬件资源的供给、分发、调度等。虽然还是在熟悉的云原生领域，折腾的还是哪些对象哪些事儿，适配到一种新场景。但为了避免新瓶装老酒，能有机会做的更扎实，做出价值，对这个要服务领域内的一些东西也想花点时间和精力稍微了解下。
国庆长假环太湖一圈回来，假期最后这两天豆哥被要求上课，正好有难得的集中时间可以稍微看些东西。能对这部分有个比较完整的入个门，暂时没有精力系统地构建。作为一个云原生领域的从业者，目标是知道容器里跑的是什么，怎么跑的。
习惯稍微正式学点东西不只是把别人的东西快速过一遍，而是愿意用自己的文字，尽可能简单易懂地总结记录贯通下，做不到严谨、全面、深入、专业，开始前定个小目标只要做到基本的通、透、够用即可。
说干就干，先从深度学习基础技术神经网络开始。Google一把，内容可真叫个多。确实现在身边不管曾经做什么的，摇身一变都能与AI扯上关系。这么多信息对我们这些局外人非常不友好。很多年前自学的数学等相关基础课程时，习惯从稍微了解点的东西入手，有点脸熟的东西看着不怵。于是决定重拾十来年前尚老师Data Mining那门课程的部分内容，看看哪些老概念和当前这些新的技术能产生哪些联系。
切入点线性回归 线性回归可能是一个比较适当的切入点，模型简单好理解。线性回归时通过一组数据点来拟合线性模型，找出一个或者多个特征变量和目标结果之间的关系，有了这个关系就可以带入条件预测结果。一个非常经典的线性回归例子就是二手房价预测。
影响房价的因素很多，记得当时书上形式化表达是用了一个向量乘法y = wx + b。x向量是（x1,x2,x3,x4）组成，表示若干个属性。这里简单示意下假设只有两个因素x1、x2，分别表示屋子的房间数和面积，也不用向量乘了，就直接写成 y = w1 * x1 + w2 * x2 + b，y就房子价格。其中w1、w2和b称为线性回归模型的参数，w1、w2称为权重weight，b称为偏差bias。
可以看到，作为一种最简单的回归模型，线性回归使用这种线性回归方程对一个或者多个自变量和因变量之间的关系进行建模。有了这个假设的模型，就可以根据已有的二手房成交记录求解出模型上的参数w1、w2和b，这就是老听说的模型训练。完成模型训练求解出线性回归模型的参数，就可以把其他的房子信息房间数、面积x1、x2带入表达式，得到这个房子的预测价格，这就是一个推理过程。
这样通过一个简单例子把模型的的表达、训练和推理过程顺了一遍。其中省略了太多的信息和步骤，迭代着加上去应该就是关注的神经网络的关键内容。
首先是模型的表达，通过最基础的数学知识，这个线性回归的输入、输出和运算过程可以大致画出这样。
即使完全不了解神经网络，基于最朴素的概念理解，瞅着这个图上这些点的关系好像也已经和神经网络有点神似了。
认识神经网络 神经网络这个典型术语对象标准定义很多，总结下简单理解神经网络就是一种模拟人脑处理信息的方式。从数据中获取关联，在输入和输出中建立关系，特别是复杂的输入和输出之间。类似我们人脑中神经元构成一个复杂、高度互联的网络，互相发送电信号处理信息。神经网络由人工神经元组成，在这些神经元上运行算法，求解各种复杂的模型，所以我们说的神经网络说完整点描述其实是人工神经网络。
只是从外形简单比较，前面线性回归那个图看上去像一种单层或者单个神经元组成的神经网络，大致可以认为是神经网络的简单特例。
神经网络结构 经典的神经网络包括输入层、输出层和隐藏层。
**输入层：**接受外部输入的数据，将数据输入给神经网络，简单处理后发给下一层。在预测房价这个线性模型中，输入层就是影响房价的两个属性。在经常说的图片识别分类的应用中输入层就是像素，如100*100像素的图片就又10000个输入。 **隐藏层：**神经网络中大量的隐藏层从上一层，如输入层或者上一个隐藏层获取输入，进行数据处理，然后传递给下一层。神经网络的关键处理都集中在隐藏层，越复杂的模型、表达能力越强的模型，隐藏层层数越多，隐藏层上的节点也越多。 **输出层：**输出神经网络对数据的最终处理结果。因为模型固定，输出层的节点数一般也是固定的，如上一个图表示，房价预测这个线性回归，作为简单特例的神经网络只有一个输出。如果是分类的模型，有几个分类，就对应输出层有几个节点。 从我们程序员的语言看，一个神经网络可以简单看成我们编程的一个方法。输入层对应这个方法定义的入参，输出层对应方法定义的返回值，隐藏层可以见到类别我们的方法实现逻辑。模型训练好后，去做推理时就是传入入参调用这个方法，得到返回值的过程。
从数学的视角看，一个神经网络就好像一个映射函数，函数的自变量x1、x2对应输入层，输出层对应函数的因变量y。训练过程就是找到函数表达式中的各个参数。有了这个求解的函数表达式，其他任意的x1、x2带进去也能得到对应基本正确的y。
作为映射函数，只要有一组输入就能映射到一组输出。除了前面根据房子大小、房间数映射出房价外，其他更强大的神经网络可以拟合更复杂的函数。对于大多数深度学习的应用，虽然我们没有办法向这个预测房价的例子这么直观地写出一个具体表达式来，但还是理解存在这样一个函数映射，或者说通过训练有办法逼近一个理想的函数映射。
记得从哪里看到黄教主说过“AI深度学习，也是一种解决难以指定的问题的算法和一种开发软件的新方法。想象我们有一个任意维度的通用函数逼近器”。如果设计的神经网络足够深、参数足够多，足够复杂就可以逼近任意复杂的函数映射。不只是预测房价这个简单的线性回归，也不只是当年机器学习课本上的分类、聚类、啤酒尿布频繁项这些业务固定的应用。
为了便于理解把这种不太严谨的类比总结成这个表。
神经网络 程序视角 数学视角 模型 代码方法 映射函数 输入层 入参定义 自变量 隐藏层 方法体实现 函数表达式 输出层 返回值定义 因变量 训练 构造实现并UT验证修正 求解函数参数 推理 实际方法调用 带入新的自变量求解因变量 样板特征 Feature UT测试输入 已知的函数参数 样板标签 Label UT预期输出 对应的函数取值 以上两个临时起意的比喻，前一个更像神经网络的物理存在，不管多复杂的神经网络，最终都是一个方法调用。Restful或者其他协议调到推理服务上，获得一个输出。而数学的这个类比更像神经网络的逻辑定义，模型的定义和训练、推理过程等。
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Mining 笔记聚类k-medoids</title>
      <link>https://idouba.com/notes-clustering-k-medoids/</link>
      <pubDate>Wed, 18 Sep 2013 13:28:36 +0000</pubDate>
      
      <guid>https://idouba.com/notes-clustering-k-medoids/</guid>
      <description>
        
          
            一、概述 k-means利用簇内点的均值或加权平均值ci（质心）作为类Ci的代表点。对数值属性数据有较好的几何和统计意义。对孤立点是敏感的，如果具有极大值，就可能大幅度地扭曲数据的分布.
k-medoids(k-中心点)算法是为消除这种敏感性提出的，它选择类中位置最接近类中心的对象(称为中心点)作为类的代表点，目标函数仍然可以采用平方误差准则。
PAM（Partitioning Around Medoids，围绕中心点的划分）是最早提出的k中心点算法之一。
二、算法思想： 随机选择k个对象作为初始的k个类的代表点，将其余对象按与代表点对象的距离分配到最近的类；反复用非代表点来代替代表点，以改进聚类质量。 即：算法将判定是否存在一个对象可以取代已存在的一个中心点。
通过检验所有的中心点与非中心点组成的对，算法将选择最能提高聚类效果的对，其中成员总是被分配到与中心点距离最短的类中。 假设类Ki 的当前中心点是Oi , 希望确定Oi是否应与非中心点Oh交换.如果交换可以改善聚类的效果，则进行交换。 距离代价的变化是指所有对象到其类中心点的距离之和的变化，这里使用Cjih表示中心点Oi与非中心点Oh交换后，对象Oj到中心点距离代价的变化。
总代价定义如下：
三、算法描述： 输入： 簇的数目k和包含n个对象的数据库。
输出： k个簇的集合
方法： 1任意选择k个对象作为初始的代表对象（簇中心点） 2repeat 3将每个剩余对象指派到最近的代表对象所代表的簇 4随机地选择一个非代表对象Orandom 5计算用Orandom交换代表对象Oi的总代价S 6if S &amp;lt; 0，then用Orandom替换Oi ，形成新的k个代表对象的集合 7UNTIL不发生变化 四、算法实例 样本点 A B C D E A 1 2 2 3 B 1 2 4 3 C 2 2 1 5 D 2 4 1 3 E 3 3 5 3 第一步 建立阶段： 假如从5个对象中随机抽取的2个中心点为{A，B},则样本被划分为{A、C、D}和{B、E}
第二步 交换阶段： 假定中心点A、B分别被非中心点C、D、E替换，根据PAM算法需要计算下列代价TC(AC)、 TC(AD)、 TC(AE)、TC(BC)、TC(BD)、 TC(BE)。
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Mining 笔记之Classification</title>
      <link>https://idouba.com/notes-about-classification/</link>
      <pubDate>Wed, 18 Sep 2013 11:12:40 +0000</pubDate>
      
      <guid>https://idouba.com/notes-about-classification/</guid>
      <description>
        
          
            一、概念 监督式学习VS非监督式学习 Supervised learning (classification): The training data (observations, measurements, etc.) are accompanied by labels indicating the class of the observations. New data is classified based on the training set.
Unsupervised learning (clustering):The class labels of training data is unknown Given a set of measurements, observations, etc. with the aim of establishing the existence of classes or clusters in the data –Jiawei Han
监督式学习：提供了训练元组的类标号，通过分析已知数据，得到一个分类模型，用来确定其它的对象属于哪个类别。
非监督式学习：不依赖有类标号的训练实例
分类Classification predicts categorical class labels (discrete or nominal), classifies data (constructs a model) based on the training set and the values (class labels) in a classifying attribute and uses it in classifying new data。
          
          
        
      </description>
    </item>
    
    <item>
      <title>A Program demonstrating Gini Index Classification</title>
      <link>https://idouba.com/classfication-giniindex-program/</link>
      <pubDate>Wed, 03 Jul 2013 15:50:08 +0000</pubDate>
      
      <guid>https://idouba.com/classfication-giniindex-program/</guid>
      <description>
        
          
            无意发现研究生时候数据挖掘课程关于基于Gini Index的一个Classification的实验报告，还算完整。基于尚老师给的数据集完整完成了模型设计、训练和验证。还用Java写了个简单界面，能导入数据集训练，并画出决策树，并能导入数据集验证，评价准确性。
A Program demonstrating Gini Index Classification Abstract In this document, a small program demonstrating Gini Index Classification is introduced. Users can select specified training data set, build the decision tree, and then select independent testing data set to verify the accuracy of model. Furthermore, by providing the decision tree visualization (Using JTree of Java), the pattern recognition capacities of users can be greatly improved.When estimating classifier accuracy, the known class label is compared with the learned model’s class prediction for that sample, the conflict records will be filtered to show user what the record’s class label is and what the mined model tells you the result is supposed should be.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
