<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 爱豆吧！</title>
    <link>https://idouba.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 爱豆吧！</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>浙ICP备18050493号-1 浙公网安备 33010802006262号</copyright>
    <lastBuildDate>Wed, 18 Sep 2013 13:28:36 +0000</lastBuildDate><atom:link href="https://idouba.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Mining 笔记聚类k-medoids</title>
      <link>https://idouba.com/notes-clustering-k-medoids/</link>
      <pubDate>Wed, 18 Sep 2013 13:28:36 +0000</pubDate>
      
      <guid>https://idouba.com/notes-clustering-k-medoids/</guid>
      <description>
        
          
            一、概述 k-means利用簇内点的均值或加权平均值ci（质心）作为类Ci的代表点。对数值属性数据有较好的几何和统计意义。对孤立点是敏感的，如果具有极大值，就可能大幅度地扭曲数据的分布.
k-medoids(k-中心点)算法是为消除这种敏感性提出的，它选择类中位置最接近类中心的对象(称为中心点)作为类的代表点，目标函数仍然可以采用平方误差准则。
PAM（Partitioning Around Medoids，围绕中心点的划分）是最早提出的k中心点算法之一。
二、算法思想： 随机选择k个对象作为初始的k个类的代表点，将其余对象按与代表点对象的距离分配到最近的类；反复用非代表点来代替代表点，以改进聚类质量。 即：算法将判定是否存在一个对象可以取代已存在的一个中心点。
通过检验所有的中心点与非中心点组成的对，算法将选择最能提高聚类效果的对，其中成员总是被分配到与中心点距离最短的类中。 假设类Ki 的当前中心点是Oi , 希望确定Oi是否应与非中心点Oh交换.如果交换可以改善聚类的效果，则进行交换。 距离代价的变化是指所有对象到其类中心点的距离之和的变化，这里使用Cjih表示中心点Oi与非中心点Oh交换后，对象Oj到中心点距离代价的变化。
总代价定义如下：
三、算法描述： 输入： 簇的数目k和包含n个对象的数据库。
输出： k个簇的集合
方法： 1任意选择k个对象作为初始的代表对象（簇中心点） 2repeat 3将每个剩余对象指派到最近的代表对象所代表的簇 4随机地选择一个非代表对象Orandom 5计算用Orandom交换代表对象Oi的总代价S 6if S &amp;lt; 0，then用Orandom替换Oi ，形成新的k个代表对象的集合 7UNTIL不发生变化 四、算法实例 样本点 A B C D E A 1 2 2 3 B 1 2 4 3 C 2 2 1 5 D 2 4 1 3 E 3 3 5 3 第一步 建立阶段： 假如从5个对象中随机抽取的2个中心点为{A，B},则样本被划分为{A、C、D}和{B、E}
第二步 交换阶段： 假定中心点A、B分别被非中心点C、D、E替换，根据PAM算法需要计算下列代价TC(AC)、 TC(AD)、 TC(AE)、TC(BC)、TC(BD)、 TC(BE)。
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Mining 笔记之Classification</title>
      <link>https://idouba.com/notes-about-classification/</link>
      <pubDate>Wed, 18 Sep 2013 11:12:40 +0000</pubDate>
      
      <guid>https://idouba.com/notes-about-classification/</guid>
      <description>
        
          
            一、概念 监督式学习VS非监督式学习 Supervised learning (classification): The training data (observations, measurements, etc.) are accompanied by labels indicating the class of the observations. New data is classified based on the training set.
Unsupervised learning (clustering):The class labels of training data is unknown Given a set of measurements, observations, etc. with the aim of establishing the existence of classes or clusters in the data –Jiawei Han
监督式学习：提供了训练元组的类标号，通过分析已知数据，得到一个分类模型，用来确定其它的对象属于哪个类别。
非监督式学习：不依赖有类标号的训练实例
分类Classification predicts categorical class labels (discrete or nominal), classifies data (constructs a model) based on the training set and the values (class labels) in a classifying attribute and uses it in classifying new data。
          
          
        
      </description>
    </item>
    
    <item>
      <title>A Program demonstrating Gini Index Classification</title>
      <link>https://idouba.com/classfication-giniindex-program/</link>
      <pubDate>Wed, 03 Jul 2013 15:50:08 +0000</pubDate>
      
      <guid>https://idouba.com/classfication-giniindex-program/</guid>
      <description>
        
          
            无意发现研究生时候数据挖掘课程关于基于Gini Index的一个Classification的实验报告，还算完整。基于尚老师给的数据集完整完成了模型设计、训练和验证。还用Java写了个简单界面，能导入数据集训练，并画出决策树，并能导入数据集验证，评价准确性。
A Program demonstrating Gini Index Classification Abstract In this document, a small program demonstrating Gini Index Classification is introduced. Users can select specified training data set, build the decision tree, and then select independent testing data set to verify the accuracy of model. Furthermore, by providing the decision tree visualization (Using JTree of Java), the pattern recognition capacities of users can be greatly improved.When estimating classifier accuracy, the known class label is compared with the learned model’s class prediction for that sample, the conflict records will be filtered to show user what the record’s class label is and what the mined model tells you the result is supposed should be.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
